\subsection{RQ3: To which degree does generalization affect the size and runtime of the target test suites?}
\label{sec:ancillary-effects-eval}

\ToolTeralizer{} transforms conventional JUnit tests into property-based \ToolJqwik{} tests.
While this transformation improves mutation detection (Section~\ref{sec:primary-effects-eval}),
it also affects test suite characteristics in several ways:

\begin{enumerate}
  \item the number of tests in the test suite (Section~\ref{sec:test-suite-test-count}),
  \item the number of lines of code in the test suite (Section~\ref{sec:test-suite-line-count}),
  \item the execution time of the test suite (Section~\ref{sec:test-suite-execution-time}).
\end{enumerate}

Section~\ref{sec:test-suite-test-count} reveals how test architecture determines
whether added generalized tests can be compensated by original test removals.
Section~\ref{sec:test-suite-line-count} documents
how \ToolTeralizer{}'s constraint encoding and test isolation increase lines of code in the test suite.
Section~\ref{sec:test-suite-execution-time} analyzes runtime patterns,
showing that costs stem primarily from property-based testing overhead and \tries{} repetition.
We focus on the results
of \VariantNaiveC{} and \VariantImprovedC{}
as they best represent the current capabilities of \ToolTeralizer{}.
The results for variants with fewer \tries{} follow similar trends,
albeit with overall smaller effects
on the measured metrics.
Full results for all variants
are available in our replication package~\cite{replicationpackage}.

\subsubsection{Number of Tests in the Test Suite}
\label{sec:test-suite-test-count}

As described in Section~\ref{sec:test-suite-reduction},
\ToolTeralizer{} aims to remove any original and generalized tests
that do not contribute unique fault detection capability
during its test suite reduction stage.
In an ideal scenario, all added property-based tests
are compensated by removed original tests.
However, the effectiveness of test suite reduction
depends strongly on overall test architecture
and mutation detection capabilities of the \VariantOriginal{} test suite.
Table~\ref{tab:tests-per-project} quantifies the observed changes
for the \VariantNaiveC{} and \VariantImprovedC{} generalization variants.
The number of added tests is between 174--211
(3.5--4.4\% of \VariantOriginal{} test suite size)
for the \DatasetsEqBenchEs{} projects,
between 60--75 (2.2--2.8\%)
for the \DatasetsCommonsEs{} projects,
and 3 (0.4\%) for the \DatasetCommonsDev{} project.

\VariantImprovedC{} generally adds a larger number of tests than \VariantNaiveC{}.
This is because more of the tests that are created by \VariantNaiveC{}
are excluded due to \texttt{Too\-Many\-Filter\-Misses\-Exception}s and, therefore,
not retained in the final test suite
(as evaluated in Section~\ref{sec:limitations-eval}).
Furthermore, the number of added tests is much smaller
than the total number of generalized tests that are created and evaluated by \ToolTeralizer{}
for both \VariantNaiveC{} as well as \VariantImprovedC{}
because only tests that measurably increase the mutation score of the test suite are retained.
In total, \ToolTeralizer{} generates 21,478 candidate generalizations across the listed project and generalization variants.
However, test suite reduction retains only 1,555~(7.2\%) generalized tests that demonstrably improve mutation detection results.

\input{tables/tab-tests-per-project}

Even though \ToolTeralizer{} adds hundreds of tests to the generalized test suites,
net test count changes remain minimal.
Added tests are largely compensated by removed tests
in the \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects
which use \ToolEvoSuite{}-generated test suites.
As a result, total test suite size only increases by 0--1 test cases
(0--0.04\% of \VariantOriginal{} test suite size)
for these projects.
The \DatasetCommonsDev{} project sees less compensation success:
none of the tests for which generalizations are added can be removed.
This is because an \VariantOriginal{} test can only be removed
if generalized tests are created for all assertions in the test
(Section~\ref{sec:test-suite-reduction}).
In projects with \ToolEvoSuite{}-generated tests,
this requirement is generally satisfied because most tests only contain a single assertion.
However, this requirement is more difficult to satisfy for \DatasetCommonsDev{} 
because the developer-written tests often contain multiple assertions.

\subsubsection{Lines of Code in the Test Suite}
\label{sec:test-suite-line-count}

While test counts remain relatively stable due to test suite reduction,
lines of code (LOC) increase across all projects and generalization variants.
Table~\ref{tab:lines-per-project} shows increases of
31.5--58.7\% for \DatasetsEqBenchEs{},
18.8--29.9\% for \DatasetsCommonsEs{},
and 4.9--5.3\% for \DatasetCommonsDev{}.
These increases correlate with the number of added tests
rather than net test count changes.
For example, \VariantNaiveC{} adds 177 generalized tests to \DatasetEqBenchA{}
and removes all 177 corresponding original tests.
However, the added tests increase test suite size by 11,780 LOC while the removed tests reduce LOC by only 1,127.
This asymmetry stems from
two characteristics of \ToolTeralizer{}'s current implementation.
First, \ToolTeralizer{} encodes constraints explicitly in the source code of the created property-based tests
(Section~\ref{sec:three-variant-design}).
As a result, the LOC impact scales with parameter count and constraint complexity.
\VariantImproved{} variants also show higher per-test LOC
because of their more sophisticated input generation logic.
For example, \DatasetCommonsDev{}'s three generalized tests require
36 additional LOC with \VariantImprovedC{} compared to \VariantNaiveC{}
(9,018 vs.\ 8,982 total LOC).
Second, test isolation creates duplication.
\ToolTeralizer{} creates new test classes for each generalized method,
copying imports, setup/teardown methods, helper functions, class fields, etc.
from original tests (Section~\ref{sec:transformation-pipeline}).
This prioritizes safety over LOC efficiency,
avoiding unintended interactions between generalized and original tests.

\input{tables/tab-lines-per-project}


The duplication overhead differs significantly between \ToolEvoSuite{}-generated and developer-written tests.
\DatasetsEqBenchEs{} and \DatasetsCommonsEs{}  projects show similar overhead
(66-67 vs 62-63 LOC per added test, respectively),
reflecting the uniformity of \ToolEvoSuite{}-generated tests.
In contrast, \DatasetCommonsDev{} shows substantially higher overhead
(140 LOC per added test).
This is because developer-written tests contain more shared setup code
as well as multi-assertion architectures that hinder compensation through test removals.
The higher increases observed in \DatasetsEqBenchEs{} projects (31-59\%) compared to \DatasetsCommonsEs{} projects (19-30\%)
stem primarily from the retained generalized tests representing a larger fraction of the original test suite
in \DatasetsEqBenchEs{} (3.5-4.4\%) compared to \DatasetsCommonsEs{} (2.2-2.8\%).
The smaller difference between \VariantNaive{} and \VariantImproved{}
in \DatasetsCommonsEs{} compared to \DatasetsEqBenchEs{}
likely reflects \VariantImproved{}'s lower constraint utilization 
(28.5\% for \DatasetsCommonsEs{} vs.\ 54.7\% for \DatasetsEqBenchEs{}).

Both overhead sources represent implementation choices rather than inherent limitations.
The primary reason \ToolTeralizer{} avoids in-place transformation of tests
is to keep changes isolated,
preventing adverse effects on developer-written code
and mutation testing (Section~\ref{sec:transformation-pipeline}).
Similarly, constraint encoding logic could be extracted to a library
that abstracts implementation details.
Only minor changes would remain then in the generalized test code:
modified test annotations, parameterized inputs, and generalized assertions.
The overall impact on test suite LOC would, therefore, be reduced
while preserving the mutation detection benefits.
We leave these improvements for future work.

\subsubsection{Execution Time of the Test Suite}
\label{sec:test-suite-execution-time}

As shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{} and \VariantImprovedC{}
increases overall test suite runtimes
for all \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
More specifically, test suite runtimes
show increases of 574.5--1210.0\% for the \DatasetsEqBenchEs{} projects,
444.1--2651.5\% for the \DatasetsCommonsEs{} projects,
and 9.4--57.1\% for the \DatasetCommonsDev{} project.
Runtime increases are primarily affected by the following factors:

\begin{enumerate}
  \item the number of tests added by the generalization,
  \item the number of tests removed by the test suite reduction,
  \item the number of \tries{} used during property-based testing,
  \item the used generalization approach, i.e., \VariantNaive{} vs. \VariantImproved{} generalization,
  \item the complexity of input partition constraints.
\end{enumerate}

\input{tables/tab-runtime-per-project}

\paragraph{Added and Removed Tests}

Execution of conventional JUnit tests has a lower runtime cost
than execution of corresponding jqwik tests.
More specifically, property-based tests
created with the \VariantBaseline{} generalization strategy 
take, on average, 149.56 milliseconds (ms) longer to execute
than the corresponding \VariantOriginal{} tests
(as shown in Figure~\ref{fig:test-runtime-differences})
which have a mean execution time of only 3.6~ms. % 0.0035570506476290337
% Exact number = 0.0035570506476290337 via:
% SELECT AVG(runtime)
% FROM junit_test_report jtr
% JOIN generalization g ON g.test_id = jtr.test_id
% WHERE stage = 'COLLECT_JUNIT_REPORTS_ORIGINAL'
% AND g.is_included;
Therefore, overall test suite execution time increases,
on average, by at least 149.56~ms per jqwik test
that is added during generalization,
even if no new test inputs are exercised
and the added generalized tests are compensated by removed original tests.
Since these runtime increases are inherent to the use of jqwik,
they are orthogonal to our specific generalization approach.
Any manual or automated transformation of JUnit tests
to jqwik tests incurs the same runtime overhead,
and this overhead can only be reduced
if fewer jqwik tests are created
(e.g., through test suite reduction, as discussed in Section~\ref{sec:test-suite-reduction})
or if the performance of jqwik is improved.

\paragraph{Number of \tries{}}

Increasing the number of \tries{}
directly increases the number of test inputs
that need to be generated and exercised
during property-based test execution.
For example, as shown in Figure~\ref{fig:test-runtime-differences},
execution time of \VariantNaive{} tests
is, on average, 286.13 milliseconds (ms) longer per test
than for \VariantOriginal{} tests when using 10 \tries{}
(a +91.3\% increase compared to the \VariantBaseline{} overhead of 149.56~ms),
348.56~ms (+133.0\%) longer with 50 \tries{},
and 1136.21~ms (+659.7\%) longer with 200 \tries{}.
Similarly, execution time of \VariantImproved{} tests
is 189.17~ms (+26.5\%) longer with 10 \tries{},
246.85~ms (+65.1\%) longer with 50 \tries{},
and 395.26~ms (+164.3\%) longer with 200 \tries{}.
While overall runtime increases as \texttt{tries} increase,
the runtime cost per \texttt{try}
decreases as \texttt{tries} increase.
More specifically, \VariantNaive{} variants
show per-\texttt{try} increases
of 28.61~ms / 6.97~ms / 5.68~ms at 10 / 50 / 200 \tries{}.
\VariantImproved{} variants show
per-\texttt{try} increases 
of 18.92~ms / 4.94~ms / 1.98~ms at 10 / 50 / 200 \tries{}.
However, to attribute these observed
improvements in per-\texttt{try} efficiency to any specific causes
would require a more thorough microbenchmarking setup
that properly accounts for confounding factors such as JVM warmup,
which is beyond the scope of this evaluation.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_test_runtime_differences}
  \caption{
    Runtime comparison between original and generalized tests.
    The runtime differences measure how much longer generalized tests take to execute, on average, compared to corresponding original tests.
    We show the difference per test (left) and per try (right).
  }
  \Description{Two bar charts comparing runtime overhead of generalized tests.
  The left chart shows the mean runtime difference in milliseconds between original and generalized tests.
  BASELINE adds 149.56ms overhead.
  NAIVE variants show increasing overhead with more tries:
  286.13ms (10 tries), 348.56ms (50 tries), 1136.21ms (200 tries).
  IMPROVED variants show lower overhead:
  189.17ms (10 tries), 246.85ms (50 tries), 395.26ms (200 tries).
  The right chart shows the mean runtime difference per try in milliseconds.
  BASELINE shows 149.56ms per try.
  NAIVE variants show decreasing per-try cost:
  28.61ms (10 tries), 6.97ms (50 tries), 5.68ms (200 tries).
  IMPROVED variants show better per-try efficiency:
  18.92ms (10 tries), 4.94ms (50 tries), 1.98ms (200 tries).}
  \label{fig:test-runtime-differences}
\end{figure}

\paragraph{\VariantNaive{} vs.\ \VariantImproved{} Generalization}

Runtime increases are generally larger
for \VariantNaive{} than for \VariantImproved{}.
For example, \VariantNaiveC{} and \VariantImprovedC{}
both generalize the same three tests of \DatasetCommonsDev{}
(see Table~\ref{tab:tests-per-project}).
Nevertheless, as shown in Table~\ref{tab:runtime-per-project},
\VariantNaiveC{} increases test suite runtime by 4.54 seconds
(+57.1\% compared to \VariantOriginal{})
whereas \VariantImprovedC{} only increases runtime by 0.74 seconds (+9.4\%).
% The larger increase of \VariantNaive{} variants is caused by 
% their input value generation approach.
As described in Section~\ref{sec:three-variant-design},
\VariantNaive{} generalization selects inputs for test execution
by first randomly generating inputs
that match the parameter types of the tested method,
and then filtering any inputs that do not satisfy the input specification.
Especially for cases with restrictive constraints (e.g., \texttt{a~==~b~\&\&~b~==~c}),
this causes a large runtime overhead
because many filter-and-regenerate cycles are required
until a valid set of inputs is identified
(or \ToolJqwik{} aborts the process due to \texttt{Too\-Many\-Filter\-Misses\-Exceptions}).
\VariantImproved{} variants are less affected by this
because they already consider (some) input constraints
during initial input value generation
(as described in Section~\ref{sec:three-variant-design}).
As a result, fewer inputs need to be filtered and regenerated,
which lessens the overall runtime impact of \VariantImproved{} generalization
despite the more involved input value generation process.

\paragraph{Complexity of Constraints}

As complexity of input specifications increases,
required runtime also increases.
This is because more complex constraints
cause more filter-and-regenerate cycles to occur
during execution of property-based tests.
While \VariantNaive{}
is more strongly affected by this than \VariantImproved{}
because it does not consider any constraints during value generation
(as discussed in the previous paragraph),
the issue also affects \VariantImproved{} generalization
in cases where less than 100\% of constraints can be used
(for further details on constraint use,
see Sections~\ref{sec:three-variant-design} and \ref{sec:constraint-complexity-eval}).
For example, as shown in Table~\ref{tab:tests-per-project},
\VariantImprovedC{} adds 3 times as many tests (206 vs.\ 69) to \DatasetEqBenchA{}
as it adds to \DatasetCommonsA{},
yet runtime increases are similar: +578.7\% vs.\ +680.5\%.
This is because input specifications in \DatasetEqBenchA{}
have fewer operations (mean: 159.1 vs.\ 208.0, median: 10 vs.\ 15)
and constraints (mean: 6.5 vs.\ 7.5, median: 2 vs.\ 5)
than in \DatasetCommonsA{}, 
and fewer of these constraints
can be used during input value generation
(mean: 42.9\% vs.\ 61.5\%, median: 80\% vs.\ 100\%).
This suggests that improving support for complex input constraints
would not only increase detection rates
(as suggested in Section~\ref{sec:constraint-complexity-eval})
but could also reduce the runtime cost of generalized tests.

\rqanswerbox{3}{
  Test generalization consistently increases test suite LOC and runtime,
  whereas test count changes are highly dependent on test architecture.
  \ToolEvoSuite{}-generated test suites see near-complete compensation via test suite reduction (1,549 tests added, 1,541 removed).
  Developer-written tests resist compensation due to multi-assertion architectures (6 added, 0 removed).
  LOC increases by 4.9--58.7\% due to explicit constraint encoding and test isolation.
  \VariantImproved{} shows larger increases than \VariantNaive{} because of its more sophisticated input generation logic.
  Runtime increases by 9.4--2,651.5\% across projects and \tries{} settings,
  reflecting property-based testing overhead, i.e., jqwik framework cost plus increased number of tested inputs.
  \VariantNaive{} shows larger runtime increases than \VariantImproved{}
  because random generation requires more filter-and-regenerate cycles to satisfy constraints.
}
