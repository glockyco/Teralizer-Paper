\section{Introduction}
\label{sec:introduction}

Conventional unit tests validate software behavior
by checking specific input-output pairs~\cite{orso_2014_software,ammann_2016_intro,myers_2011_art},
but leave most inputs along the same execution path untested.
Property-based testing~\cite{claessen_2000_quickcheck,hughes_2007_quickcheck} instead generates many inputs
and checks whether specified properties hold across executions.
For example, given the $abs$ method in Figure~\ref{fig:regression-detection},
a unit test which asserts that $abs(0)$ returns $0$ would 
still pass after changing \texttt{x >= 0} to \texttt{x == 0}, 
whereas a property-based test which asserts $abs(x) = x$ for $x \geq 0$ 
would expose this regression.
Industrial experience reports suggest that property-based testing 
often uncovers edge cases and boundary conditions missed by unit tests~\cite{hughes_2016_experiences,goldstein_2024_pbt_practice}.
Adoption, however, remains limited because writing property-based tests 
requires manual effort to define both input constraints and suitable properties,
a task practitioners find challenging~\cite{goldstein_2024_pbt_practice}.
This motivates research into transformation approaches
that automatically generalize existing unit tests by deriving properties from program semantics.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test misses a regression that the property-based test detects.}
  \Description{Regression detection comparison between unit test and property-based test.
  Left shows buggy abs() implementation with incorrect condition (x == 0 instead of x >= 0),
  highlighted with red background for removed line and green for added line.
  Middle shows original @Test that still passes (1 test run, 0 failures)
  because it only tests input 0.
  Right shows @Property test that fails (1 test run, 1 failure)
  detecting the regression with inputs like 5 and 42,
  showing "expected <5> but was <-5>" error message.}
  \label{fig:regression-detection}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  \Description{Transformation workflow showing Teralizer converting a unit test to a property-based test.
  Left side shows the original abs() implementation and a JUnit test with @Test annotation
  that asserts abs(0) equals 0.
  Center shows the Teralizer transformation arrow.
  Right side shows the generated jqwik property-based test with @Property annotation,
  parameterized input (min=0) int x,
  and generalized assertion assertEquals(x, abs(x)).
  Annotations indicate replaced annotations, added constraints, added parameters,
  replaced expected values, and replaced arguments.}
  \label{fig:generalization}
\end{figure}

We propose a semantics-based approach for automated test generalization
that analyzes both test code and implementation code
to derive path-exact specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic}.
Our method determines which inputs follow the same execution path as existing tests
and transforms unit tests into property-based tests
that validate the same assertions across entire input partitions.
Because specifications are extracted directly from program semantics,
the resulting properties are exact for each execution path
and preserve the developer-provided oracles encoded in assertions.
This enables thorough testing within those validated behaviors.
To our knowledge, JARVIS~\cite{peleg_2018_jarvis} is the only prior work
that automatically generalizes unit tests into property-based tests.
However, JARVIS infers properties from input-output examples based solely on test code,
relying on predefined abstraction templates that yield overapproximations.
In contrast, our white-box approach leverages both static and dynamic program analysis
to extract exact specifications for the execution paths exercised by the original tests.

We implemented this approach in \ToolTeralizer{}, a prototype tool for Java
that transforms JUnit tests into property-based \ToolJqwik{}~\cite{link_2022_jqwik} tests.
\ToolTeralizer{} employs a five-stage pipeline:
(1)~analyzing tests and their assertions regarding suitability for generalization,
(2)~identifying tested methods through data flow analysis,
(3)~extracting specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
(4)~creating generalized property-based tests, and
(5)~filtering generalized tests to retain only those that improve fault detection capability.
Figure~\ref{fig:generalization} illustrates the effects of this transformation,
showing how a simple equality assertion $abs(0) = 0$
becomes the property $abs(x) = x$,
valid for all non-negative values of $x$.

To evaluate the potential and limitations of semantics-based test generalization,
we applied \ToolTeralizer{} to three complementary datasets.
First, the \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench} provides controlled settings
with numeric-focused programs well-suited for symbolic analysis.
Because \DatasetEqBench{} lacks test suites, we generated tests using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}.
Second, utility methods extracted from Apache Commons projects
offer a middle ground between controlled and real-world scenarios.
Here, we directly compared \ToolEvoSuite{}-generated and developer-written tests
on the same codebase, partially isolating the influence of test architecture and quality on generalization outcomes.
Finally, we applied \ToolTeralizer{} to 632 real-world Java projects with developer-written tests
from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
to expose the full complexity of practical application scenarios.
This progression from controlled to real-world conditions highlights
the achievable fault-detection improvements
as well as the theoretical and practical limitations of the proposed approach.

Our evaluation shows modest yet consistent improvements under controlled conditions.
On \ToolEvoSuite{}-generated tests, mutation scores increased by 1--4 percentage points:
from 48--52\% to 52--55\% on \DatasetEqBench{},
and from 57--58\% to 58--59\% on Apache Commons utilities.
In contrast, generalization of developer-written tests for Apache Commons utilities
showed minimal improvement of 0.05--0.07 percentage points from a baseline of 80.35\%.
This shows that the tests of these projects already detected most of the faults targeted by automated generalization.
Beyond mutation detection improvements,
the results from the RepoReapers projects revealed practical applicability barriers:
% our analysis of 632 real-world projects reveals practical applicability barriers:
only 1.7\% of projects successfully completed the full generalization pipeline.
Failures primarily occurred due to 
type support limitations of symbolic analysis
as well as static analysis limitations
of our current implementation of \ToolTeralizer{}.
To provide a roadmap for future work,
we classify these failures into
those that can be resolved through additional engineering effort
and those that represent deeper research challenges in specification extraction and encoding.

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Semantics-based test generalization:}
We introduce an automated approach for transforming unit tests into property-based tests
through the use of path-exact specifications extracted via symbolic analysis.
Our prototype implementation called \ToolTeralizer{}
currently supports test generalization for Java projects
and achieves mutation score improvements of 1--4 percentage points
under conditions suitable for symbolic analysis.

% \item \textbf{Empirical dataset:}
% We provide a dataset that characterizes test generalizability across 630+ Java projects,
% covering assertion patterns (types, frequencies, per-test distributions),
% specification complexity metrics (operation counts, constraint utilization),
% and filtering outcomes that explain why 98.3\% of real-world generalizations fail.

\item \textbf{Analysis of applicability challenges:}
We analyze barriers to practical test generalization,
distinguishing addressable engineering limitations
(e.g., identifying tested methods in complex control flow, detecting assertions in helper methods)
from fundamental research challenges (e.g., constraint encoding for non-numeric types),
and provide concrete guidance for advancing the field.

\item \textbf{Replication package:}
Our replication package~\cite{replicationpackage} contains the source code of \ToolTeralizer{},
all datasets and scripts used for the evaluation, and the results,
thus enabling reproduction and extension of our work.
\end{enumerate}

The paper is organized as follows.
Section~\ref{sec:background} introduces the technical foundations of test generalization.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s five-stage pipeline.
Section~\ref{sec:evaluation} evaluates our approach through six research questions,
covering mutation score improvements, impact on test suite size and execution time,
runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} discusses the results, directions for future work, and threats to validity.
Section~\ref{sec:related-work} positions our work within the broader testing literature,
and Section~\ref{sec:conclusions} concludes the paper.
