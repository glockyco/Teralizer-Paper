\section{Introduction}
\label{sec:introduction}

Unit tests validate software behavior by checking specific input-output pairs,
leaving most inputs along the same execution path untested~\cite{orso_2014_software}.
Property-based testing~\cite{claessen_2000_quickcheck,hughes_2007_quickcheck} takes a different approach,
generating multiple test inputs and checking whether specified properties 
hold for the resulting executions.
This difference has practical implications:
a unit test verifying that \texttt{abs(0)} returns \texttt{0} would 
pass even when the implementation incorrectly changes \texttt{x >= 0} to \texttt{x == 0}, 
whereas a property-based test checking that \texttt{abs(x) = x} holds for 
non-negative values of \texttt{x} would detect this regression (Figure~\ref{fig:regression-detection}).
Industrial experience reports suggest that property-based testing can 
uncover edge cases and boundary conditions that are commonly missed
by conventional unit tests~\cite{hughes_2016_experiences,goldstein_2024_pbt_practice}.
However, creating property-based tests requires manual effort to define 
both input generators and the properties that should hold~\cite{goldstein_2024_pbt_practice}.
Practitioners report challenges in formulating appropriate properties 
and designing generators that produce relevant test inputs~\cite{goldstein_2024_pbt_practice}.
This manual effort requirement limits adoption,
motivating research into automated transformation approaches.

This paper presents \ToolTeralizer{}, a semantics-based test generalization approach
that automatically transforms conventional unit tests into property-based tests.
\ToolTeralizer{} implements generalization through a three-phase pipeline:
first analyzing tests to identify tested methods and their assertions,
then extracting specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
and finally using these specifications to transform existing unit tests into property-based tests.
Unlike prior work that generalizes from input-output examples 
by instantiating predefined abstraction templates that overapproximate program behaviors~\cite{peleg_2018_jarvis},
we extract specifications directly from the program semantics
to obtain path-exact properties for the specific execution paths covered by original tests.
Our approach builds on the observation that existing assertions encode oracles
that should hold for all inputs following the same execution path
(i.e., all inputs that are part of the same input partition),
not just the single tested input.
Figure~\ref{fig:generalization} illustrates the described transformation,
showing how a simple equality assertion \texttt{abs(0) = 0}
is transformed into a property \texttt{abs(x) = x}
that holds for all non-negative values of \texttt{x}.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test detects fewer regressions than a corresponding property-based test.}
  \Description{Regression detection comparison between unit test and property-based test.
  Left shows buggy abs() implementation with incorrect condition (x == 0 instead of x >= 0),
  highlighted with red background for removed line and green for added line.
  Middle shows original @Test that still passes (1 test run, 0 failures)
  because it only tests input 0.
  Right shows @Property test that fails (1 test run, 1 failure)
  detecting the regression with inputs like 5 and 42,
  showing "expected <5> but was <-5>" error message.}
  \label{fig:regression-detection}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  \Description{Transformation workflow showing Teralizer converting a unit test to a property-based test.
  Left side shows the original abs() implementation and a JUnit test with @Test annotation
  that asserts abs(0) equals 0.
  Center shows the Teralizer transformation arrow.
  Right side shows the generated jqwik property-based test with @Property annotation,
  parameterized input (min=0) int x,
  and generalized assertion assertEquals(x, abs(x)).
  Annotations indicate replaced annotations, added constraints, added parameters,
  replaced expected values, and replaced arguments.}
  \label{fig:generalization}
\end{figure}

To understand both the potential and limitations of semantics-based test generalization,
we evaluate our prototype implementation of \ToolTeralizer{}
across three complementary datasets that progressively
reveal the gap between ideal and real-world conditions.
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench} provides controlled conditions
with numeric-focused programs ideal for symbolic analysis;
since it lacks test suites, we generate tests using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}.
Utility methods extracted from Apache Commons projects
offer a middle ground between ideal and real-world conditions,
enabling direct comparison between \ToolEvoSuite{}-generated and developer-written tests
on the same codebase to partially isolate
the effects of test architecture and quality on generalization outcomes.
Finally, 1,160 projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
with their existing developer tests expose the full complexity of practical application conditions.
This systematic progression from favorable to challenging conditions helps us identify
both achievable fault detection improvements as well as current theoretical and practical limitations of the proposed approach.

Our evaluation results show consistent but modest improvements under favorable conditions.
On \ToolEvoSuite{}-generated tests, mutation scores improve by 1--4 percentage points:
\DatasetEqBench{} increases from 48--52\% to 52--55\%
across different baseline test suites and generalization strategies,
while Apache Commons utilities improve from 57--58\% to 58--59\%.
In contrast, generalization of developer-written tests from Apache Commons utilities
shows minimal improvement (0.05--0.07 percentage points from a baseline of 80.35\%)
as these mature tests have already caught the majority
of detectable faults that automated test generalization targets.
Beyond mutation detection results,
our analysis of 1,160 real-world projects identifies practical barriers:
only 0.9\% of projects successfully pass the full generalization pipeline,
with generalization failures being primarily caused by 
assertion-to-method mapping issues, type support limitations, and constraint encoding challenges.
Our overall classification of failures distinguishes between
issues that can be resolved through additional engineering effort
and more fundamental research challenges in specification extraction and encoding.

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Semantics-based test generalization:}
We demonstrate automated transformation from unit tests to property-based tests
through semantics-based specification extraction,
achieving mutation score improvements of 1--4 percentage points
under conditions suitable for current symbolic analysis.

\item \textbf{Empirical dataset:}
We provide a dataset characterizing test structure
and generalizability across a diverse set of projects,
including metrics on assertion patterns, specification complexity,
and filtering outcomes to support future testing research.

\item \textbf{Analysis of applicability challenges:}
We identify and quantify barriers to practical applicability
of semantics-based test generalization,
categorizing identified issues into engineering limitations
and more fundamental research challenges
requiring advances in specification extraction.

\item \textbf{Open implementation and replication package:}
We provide a complete implementation of the proposed generalization approach
and comprehensive replication package including all experimental data,
enabling reproduction and extension of our work.
\end{enumerate}

The remainder of this paper is organized as follows.
Section~\ref{sec:background} introduces the technical foundations for test generalization.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s three-phase pipeline
for automated test generalization.
Section~\ref{sec:evaluation} evaluates our approach through four research questions
examining mutation score improvements, test suite size and execution time impacts,
generalization runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} interprets the results, implications, and threats to validity.
Section~\ref{sec:related-work} positions our work within the broader testing literature.
Section~\ref{sec:conclusions} summarizes contributions and outlines future research directions.
Our implementation and complete replication package, including all experimental data and analysis scripts, are publicly available~\cite{replicationpackage}.
