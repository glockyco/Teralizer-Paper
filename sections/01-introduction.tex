\section{Introduction}
\label{sec:introduction}

Unit tests validate software behavior by checking specific input-output pairs,
testing only individual points within the vast space of possible program inputs~\cite{orso_2014_software}.
For example, a test might verify that \texttt{Math.abs(0)} returns \texttt{0},
checking only this single combination while leaving countless other inputs untested
within the same execution path.
While this approach effectively documents expected behavior and catches regressions~\cite{rothermel_1996_analyzing,yoo_2012_regression},
each test explores only a single execution through its targeted code path.
Property-based testing~\cite{claessen_2000_quickcheck,hughes_2007_quickcheck} addresses this limitation 
by automatically generating multiple test inputs that satisfy specified properties,
enabling more thorough exploration of program behavior within tested execution paths.
However, creating property-based tests requires significant expertise
in both the testing framework and the domain under test,
along with manual effort to define appropriate input generators and property specifications~\cite{barr_2015_oracle}.

This paper presents \ToolTeralizer{}, an automated approach
that transforms conventional unit tests into property-based tests.
Our work represents a specific form of test amplification~\cite{danglot_2019_snowballing}
that generalizes from single input-output pairs to properties holding across entire input partitions.
Test amplification exploits knowledge embedded in existing tests to enhance them
for engineering goals such as improved fault detection or better coverage~\cite{danglot_2019_snowballing,baudry_2015_dspot,xie_2006_augmenting}.
\ToolTeralizer{} achieves this enhancement through test generalization:
transforming point-specific tests into property-based tests
that validate behavior across input classes.
For instance, our approach automatically transforms the test \texttt{assertEquals(0, Math.abs(0))}
into a property-based test that verifies \texttt{Math.abs(x)} returns \texttt{x}
for hundreds of generated inputs where \texttt{x >= 0}, as illustrated in Figure~\ref{fig:generalization}.
This transformation enables detection of regressions missed by the original test.
Figure~\ref{fig:regression-detection} shows how a bug introduced in the conditional logic
(changing \texttt{x >= 0} to \texttt{x == 0}) is caught by the generalized test but not the original.
Unlike prior work such as JARVIS that requires manually curated constraint templates~\cite{peleg_2018_jarvis},
\ToolTeralizer{} extracts exact path specifications directly from the implementation
through single-path symbolic analysis guided by existing tests.
By using \ToolSPFLong{}'s (\ToolSPF{}) constraint collection mode~\cite{pasareanu_2013_symbolic},
our approach obtains precise path conditions for execution paths already covered by tests,
then uses these specifications to generate property-based tests~\cite{link_2022_jqwik}
that explore additional inputs within the same logical partitions.
This automation reduces the manual effort traditionally required
for property-based test creation while preserving the domain knowledge
and test intent encoded in existing test suites~\cite{barr_2015_oracle}.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  \Description{Transformation workflow showing Teralizer converting a unit test to a property-based test.
  Left side shows the original Math.abs() implementation and a JUnit test with @Test annotation
  that asserts abs(0) equals 0.
  Center shows the Teralizer transformation arrow.
  Right side shows the generated jqwik property-based test with @Property annotation,
  parameterized input (min=0) int x,
  and generalized assertion assertEquals(x, abs(x)).
  Annotations indicate replaced annotations, added constraints, added parameters,
  replaced expected values, and replaced arguments.}
  \label{fig:generalization}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test detects fewer regressions than the generalized property-based test.}
  \Description{Regression detection comparison between unit test and property-based test.
  Left shows buggy Math.abs() implementation with incorrect condition (x == 0 instead of x >= 0),
  highlighted with red background for removed line and green for added line.
  Middle shows original @Test that still passes (1 test run, 0 failures)
  because it only tests input 0.
  Right shows @Property test that fails (1 test run, 1 failure)
  detecting the regression with inputs like 5 and 42,
  showing "expected <5> but was <-5>" error message.}
  \label{fig:regression-detection}
\end{figure}

To understand both the potential and limitations of automated test generalization,
we evaluate \ToolTeralizer{} across three complementary datasets that progressively
reveal the gap between ideal and real-world conditions.
The EqBench benchmark~\cite{badihi_2021_eqbench} provides controlled conditions
with numeric-focused programs; since it lacks test suites, we generate tests using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}.
For Apache Commons utility methods, we extract production code and create two evaluation scenarios:
one using the original developer-written tests and another using \ToolEvoSuite{}-generated tests,
enabling direct comparison of generalization effectiveness across test creation approaches.
Finally, 1,160 projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
with their existing developer-written tests expose the full complexity of real-world deployment.
Beyond measuring effectiveness, our implementation captures comprehensive metrics
throughout the transformation pipeline, revealing which test characteristics
predict successful generalization and which technical barriers require attention.
Our current implementation leverages \ToolSPF{} for specification extraction,
which limits us to Java 5--8 projects and methods with numeric and boolean parameters
(where \ToolSPF{} produces accurate constraints).
While the test generalization approach is conceptually independent of the specification extraction technique,
\ToolSPF{} represents the current state of the art for extracting precise path conditions
suitable for encoding as input generators.
% Other symbolic execution tools (JBSE~\cite{braione_2016_jbse}, JDart~\cite{luckow_2016_jdart}, 
% GDart~\cite{mues_2022_gdart}, SWAT~\cite{mues_2024_swat}) share similar limitations.
% Alternative specification extraction approaches like dynamic invariant detection (Daikon~\cite{ernst_2007_daikon}),
% abstract interpretation (Infer~\cite{calcagno_2015_infer}), or property discovery (QuickSpec~\cite{smallbone_2017_quickspec})
% produce different types of specifications unsuitable for generating precise test inputs.
Extending support to other types and newer Java versions remains an open research challenge
in specification extraction.

Our evaluation reveals the current capabilities and boundaries of automated test generalization.
We measure effectiveness through mutation testing~\cite{jia_2011_analysis,papadakis_2019_mutation},
which systematically injects faults to assess test suite quality~\cite{coles_2016_pit}.
On \ToolEvoSuite{}-generated tests, \ToolTeralizer{} achieves consistent improvements:
mutation scores increase by 3--4 percentage points on \DatasetEqBench{} (from 48--52\% to 52--55\%)
and 1--2 percentage points on Apache Commons utilities (from 57--58\% to 58--59\%).
These improvements persist across different \ToolEvoSuite{} search budgets (1s, 10s, 60s)
and three generalization strategies that trade off between generation cost and effectiveness.
Developer-written Apache Commons tests, already achieving 80.4\% mutation score,
show minimal improvement (0.05--0.07 percentage points),
suggesting diminishing returns for mature test suites.
Beyond controlled evaluation, we systematically analyze 1,160 RepoReapers projects
to quantify the gap between current capabilities and broad applicability.
Among the 53\% of projects that build successfully (a prerequisite for any testing approach),
our pipeline completes for 1.6\% of projects.
This systematic analysis identifies which technical advances would enable broader deployment:
improved static analysis for method resolution (24.7\% of assertion-level failures),
support for non-numeric types (15.4\% of assertion exclusions),
and handling of methods with side effects beyond pure numeric computations.

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{First fully automated semantics-based test generalization:}
We demonstrate that automated transformation from unit tests to property-based tests is feasible,
achieving mutation score improvements of 1--4 percentage points
without requiring manual intervention when conditions align with current capabilities.
Unlike prior work such as JARVIS that automates test transformation
but relies on manually designed constraint templates and produces over-approximations,
\ToolTeralizer{} extracts exact path specifications through single-path symbolic analysis.

\item \textbf{Comprehensive empirical dataset:}
Our analysis produces a dataset characterizing test structure
and generalizability across diverse projects,
providing detailed metrics on assertion patterns, specification complexity,
and filtering outcomes that inform future testing research.
This data enables the community to understand what makes tests amenable to generalization.

\item \textbf{Empirical mapping of deployment challenges:}
We systematically identify and quantify barriers to real-world deployment,
distinguishing between engineering limitations addressable through implementation effort
and fundamental challenges requiring research advances.
This mapping guides future work by quantifying which improvements would have the greatest impact.

\item \textbf{Open implementation and replication package:}
We provide a complete implementation of the approach
and comprehensive replication package including all experimental data,
enabling reproduction and extension of our work.
\end{enumerate}

The remainder of this paper is organized as follows.
Section~\ref{sec:background} introduces the technical foundations for automated test generalization.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s three-phase pipeline
for automated test transformation.
Section~\ref{sec:evaluation} evaluates our approach through four research questions
examining mutation score improvements, test suite size and execution time impacts,
generalization runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} interprets the results and their implications.
Section~\ref{sec:related-work} positions our work within the broader testing literature.
Section~\ref{sec:conclusions} summarizes contributions and outlines future research directions.
Our implementation and complete replication package, including all experimental data and analysis scripts, are publicly available~\cite{replicationpackage}.
