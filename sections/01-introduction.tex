\section{Introduction}
\label{sec:introduction}

Conventional unit tests validate software behavior
by checking specific input-output pairs~\cite{orso_2014_software,ammann_2016_intro,myers_2011_art},
but leave most inputs along the same execution path untested.
Property-based testing~\cite{claessen_2000_quickcheck,hughes_2007_quickcheck} instead generates many inputs
and checks whether specified properties hold across executions.
For example, given the $abs$ method in Figure~\ref{fig:regression-detection},
a unit test which asserts that $abs(0)$ returns $0$ would 
still pass after changing \texttt{x >= 0} to \texttt{x == 0}, 
whereas a property-based test which asserts $abs(x) = x$ for $x \geq 0$ 
would expose this regression.
Industrial experience reports suggest that property-based testing 
often uncovers edge cases and boundary conditions missed by unit tests~\cite{hughes_2016_experiences,goldstein_2024_pbt_practice}.
Adoption, however, remains limited because writing property-based tests 
requires manual effort to define both input constraints and suitable properties,
a task practitioners find challenging~\cite{goldstein_2024_pbt_practice}.
This motivates research into transformation approaches
that automatically generalize existing unit tests by deriving properties from program semantics.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test misses a regression that the property-based test detects.}
  \Description{Regression detection comparison between unit test and property-based test.
  Left shows buggy abs() implementation with incorrect condition (x == 0 instead of x >= 0),
  highlighted with red background for removed line and green for added line.
  Middle shows original @Test that still passes (1 test run, 0 failures)
  because it only tests input 0.
  Right shows @Property test that fails (1 test run, 1 failure)
  detecting the regression with input 1,
  showing "expected <1> but was <-1>" error message.}
  \label{fig:regression-detection}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  \Description{Transformation workflow showing Teralizer converting a unit test to a property-based test.
  Left side shows the original abs() implementation and a JUnit test with @Test annotation
  that asserts abs(0) equals 0.
  Center shows the Teralizer transformation arrow.
  Right side shows the generated jqwik property-based test with @Property annotation,
  parameterized input (min=0) int x,
  and generalized assertion assertEquals(x, abs(x)).
  Annotations indicate replaced annotations, added constraints, added parameters,
  replaced expected values, and replaced arguments.}
  \label{fig:generalization}
\end{figure}

We propose a semantics-based approach for automated test generalization
that analyzes both test and implementation code
to derive path-exact specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic}.
Our method determines which inputs follow the same execution path as existing tests
and transforms unit tests into property-based tests
that validate the same assertions across entire input partitions.
Because specifications are extracted directly from program semantics,
the resulting properties are exact for each execution path
and preserve the developer-provided oracles encoded in assertions.
%This enables thorough testing within those validated behaviors.
To our knowledge, JARVIS~\cite{peleg_2018_jarvis} is the only prior work
that automatically generalizes unit tests into property-based tests.
However, JARVIS infers properties from input-output examples based solely on test code,
relying on predefined abstraction templates that yield overapproximations.
In contrast, our white-box approach leverages both static and dynamic program analysis
to extract exact specifications for the execution paths exercised by the original tests.

We implemented this approach in \ToolTeralizer{}, a prototype tool for Java
that transforms JUnit tests into property-based \ToolJqwik{}~\cite{link_2022_jqwik} tests.
\ToolTeralizer{} employs a five-stage pipeline:
(1)~analyzing tests and their assertions regarding suitability for generalization,
(2)~identifying tested methods through data flow analysis,
(3)~extracting specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
(4)~creating generalized property-based tests, and
(5)~filtering generalized tests to retain only those that improve fault detection capability.
Figure~\ref{fig:generalization} illustrates the effects of this transformation,
showing how a simple equality assertion $abs(0) = 0$
becomes the property $abs(x) = x$,
valid for all non-negative values of $x$.

To evaluate our approach,
we applied \ToolTeralizer{} to three complementary datasets.
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench} provides controlled settings
with numeric-focused programs well-suited for symbolic analysis.
Because \DatasetEqBench{} lacks test suites, we generated tests using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}.
Utility methods extracted from Apache Commons projects
offer a middle ground between controlled and real-world scenarios.
Here, we directly compared \ToolEvoSuite{}-generated and developer-written tests
on the same codebase, partially isolating the influence of test architecture on generalization outcomes.
Finally, we applied \ToolTeralizer{} to 632 real-world Java projects with developer-written tests
from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
to expose the full complexity of practical application scenarios.
This progression from controlled to real-world conditions highlights
both the potential and limitations of semantics-based test generalization.

Our evaluation shows modest yet consistent improvements under controlled conditions.
On \ToolEvoSuite{}-generated tests, mutation scores increased by 1--4 percentage points:
from 48--52\% to 52--55\% on \DatasetEqBench{},
and from 57--58\% to 58--59\% on Apache Commons utilities.
In contrast, generalization of developer-written tests for Apache Commons utilities
showed only 0.05--0.07 percentage points improvement from a baseline of 80.35\%.
% This shows that the tests of these projects already detected most of the faults targeted by automated generalization.
%Beyond mutation detection improvements,
Results from the RepoReapers projects reveal practical applicability barriers:
% our analysis of 632 real-world projects reveals practical applicability barriers:
only 1.7\% of projects successfully completed the generalization pipeline.
Failures primarily occurred due to 
type support limitations of symbolic analysis
as well as static analysis limitations
of our prototype.
To provide a roadmap for future work,
we classified these failures into
those that can be resolved through engineering effort
and those that represent deeper research challenges in specification extraction and encoding.

This paper makes the following contributions:

\begin{enumerate}
\item A \textbf{semantics-based test generalization approach} that extracts specifications via symbolic analysis to transform conventional unit tests into property-based tests.
\item A comprehensive \textbf{empirical evaluation} across three complementary datasets, demonstrating 1--4 percentage point mutation score improvements under controlled conditions.
\item A systematic \textbf{analysis of applicability barriers}, distinguishing addressable engineering limitations from fundamental research challenges in specification extraction and encoding.
\item An \textbf{open implementation and replication package} \cite{replicationpackage}, enabling reproduction and extension of our results.
\end{enumerate}

The paper is organized as follows.
Section~\ref{sec:background} introduces the technical foundations of test generalization.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s five-stage pipeline.
Section~\ref{sec:evaluation} evaluates our approach through six research questions,
covering mutation score improvements, impact on test suite size and execution time,
runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} discusses the results, directions for future work, and threats to validity.
Section~\ref{sec:related-work} positions our work within the broader testing literature,
and Section~\ref{sec:conclusions} concludes the paper.
