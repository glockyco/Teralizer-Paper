\section{Introduction}
\label{sec:introduction}

Unit tests validate software behavior by checking specific input-output pairs,
testing only individual points within the vast space of possible program inputs~\cite{orso2014software}.
For example, a test might verify that \texttt{Math.abs(0)} returns \texttt{0},
checking only this single combination while leaving countless other inputs untested
within the same execution path.
While this approach effectively documents expected behavior and catches regressions~\cite{rothermel1996analyzing,yoo2012regression},
each test explores only a single execution through its targeted code path.
Property-based testing~\cite{claessen2000quickcheck,hughes2007quickcheck} addresses this limitation 
by automatically generating multiple test inputs that satisfy specified properties,
enabling more thorough exploration of program behavior within tested execution paths.
However, creating property-based tests requires significant expertise
in both the testing framework and the domain under test,
along with manual effort to define appropriate input generators and property specifications~\cite{barr2015oracle}.

This paper presents \ToolTeralizer{}, an automated approach
that transforms conventional unit tests into property-based tests.
Our work represents a specific form of test amplification~\cite{danglot2019snowballing}
that generalizes from single input-output pairs to properties holding across entire input partitions.
Test amplification exploits knowledge embedded in existing tests to enhance them
for engineering goals such as improved fault detection or better coverage~\cite{danglot2019snowballing,baudry2015dspot,xie2006augmenting}.
\ToolTeralizer{} achieves this enhancement through test generalization:
transforming point-specific tests into property-based tests
that validate behavior across input classes.
For instance, our approach automatically transforms the test \texttt{assertEquals(0, Math.abs(0))}
into a property-based test that verifies \texttt{Math.abs(x)} returns \texttt{x}
for hundreds of generated inputs where \texttt{x >= 0} (Figures~\ref{fig:generalization} and \ref{fig:regression-detection}).
Unlike prior work that requires manual constraint templates~\cite{peleg_2018_jarvis},
\ToolTeralizer{} automatically extracts exact specifications from the implementation
through symbolic analysis guided by existing tests.
By leveraging \ToolSPFLong{}'s (\ToolSPF{}) constraint collection mode~\cite{pasareanu2013symbolic},
our approach obtains precise path conditions for execution paths already covered by tests,
then uses these specifications to generate property-based tests~\cite{link2022jqwik}
that explore additional inputs within the same logical partitions.
This automation eliminates the manual effort traditionally required
for property-based test creation while preserving the domain knowledge
and test intent encoded in existing test suites~\cite{barr2015oracle}.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  %\Description{@TODO}
  \label{fig:generalization}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test detects fewer regressions than the generalized property-based test.}
  %\Description{@TODO}
  \label{fig:regression-detection}
\end{figure}

To understand both the potential and limitations of automated test generalization,
we evaluate \ToolTeralizer{} across three complementary datasets that progressively
reveal the gap between ideal and real-world conditions.
The EqBench benchmark~\cite{badihi_2021_eqbench} provides controlled conditions
with numeric-focused programs; since it lacks test suites, we generate tests using \ToolEvoSuite{}~\cite{fraser2011evosuite}.
For Apache Commons utility methods, we extract production code and create two evaluation scenarios:
one using the original developer-written tests and another using \ToolEvoSuite{}-generated tests,
enabling direct comparison of generalization effectiveness across test creation approaches.
Finally, 1,160 projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
with their existing developer-written tests expose the full complexity of real-world deployment.
Beyond measuring effectiveness, our implementation captures comprehensive metrics
throughout the transformation pipeline, revealing which test characteristics
predict successful generalization and which technical barriers require attention.
Our current implementation leverages \ToolSPF{} for specification extraction,
which limits us to Java 5--8 projects and methods with numeric and boolean parameters
(where \ToolSPF{} produces accurate constraints).
While the test generalization approach is conceptually independent of the specification extraction technique,
\ToolSPF{} represents the current state of the art for extracting precise path conditions
suitable for encoding as input generators.
% Other symbolic execution tools (JBSE~\cite{braione2016jbse}, JDart~\cite{luckow2016jdart}, 
% GDart~\cite{mues2022gdart}, SWAT~\cite{TODO_swat2024}) share similar limitations.
% Alternative specification extraction approaches like dynamic invariant detection (Daikon~\cite{ernst2007daikon}),
% abstract interpretation (Infer~\cite{calcagno2015infer}), or property discovery (QuickSpec~\cite{smallbone2017quickspec})
% produce different types of specifications unsuitable for generating precise test inputs.
Extending support to other types and newer Java versions remains an open research challenge
in specification extraction.

Our evaluation reveals the current capabilities and boundaries of automated test generalization.
We measure effectiveness through mutation testing~\cite{jia2011analysis,papadakis2019mutation},
which systematically injects faults to assess test suite quality~\cite{coles2016pit}.
On \ToolEvoSuite{}-generated tests, \ToolTeralizer{} achieves consistent improvements:
mutation scores increase by 2--3 percentage points on EqBench (from 48--52\% to 50--55\%)
and 1--2 percentage points on Apache Commons utilities (from 57--58\% to 58--59\%).
These improvements persist across different \ToolEvoSuite{} search budgets (1s, 10s, 60s)
and three generalization strategies that trade off between generation cost and effectiveness.
Developer-written Apache Commons tests, already achieving 80.4\% mutation score,
show minimal improvement (0.05--0.07 percentage points),
suggesting diminishing returns for mature test suites.
Real-world deployment faces significant challenges:
only 0.9\% of 1,160 RepoReapers projects complete the full pipeline successfully.
Build errors and dependency issues account for over half of failures,
while the remainder reveal technical barriers in static analysis for method resolution,
assertion detection in helper methods,
and our current focus on pure methods with numeric parameters.

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{First fully automated test generalization approach:}
We demonstrate that automated transformation from unit tests to property-based tests is feasible,
achieving mutation score improvements of 1--4 percentage points
without requiring manual intervention when conditions align with current capabilities.
Unlike prior work that requires manual constraint templates,
\ToolTeralizer{} automatically extracts exact specifications through symbolic analysis.

\item \textbf{Comprehensive empirical dataset:}
Our analysis produces a dataset characterizing test structure
and generalizability across diverse projects,
providing detailed metrics on assertion patterns, specification complexity,
and filtering outcomes that inform future testing research.
This data enables the community to understand what makes tests amenable to generalization.

\item \textbf{Empirical mapping of deployment challenges:}
We systematically identify and quantify barriers to real-world deployment,
distinguishing between engineering limitations addressable through implementation effort
and fundamental challenges requiring research advances.
This mapping guides future work by quantifying which improvements would have the greatest impact.

\item \textbf{Open implementation and replication package:}
We provide a complete implementation of the approach
and comprehensive replication package including all experimental data,
enabling reproduction and extension of our work.
\end{enumerate}

The remainder of this paper is organized as follows.
Section~\ref{sec:background} introduces property-based testing and related test generalization work.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s three-phase pipeline
for automated test transformation.
Section~\ref{sec:evaluation} evaluates our approach through four research questions
examining mutation score improvements, test suite size and execution time impacts,
generalization runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} interprets the results and their implications.
Section~\ref{sec:related-work} positions our work within the broader testing literature.
Section~\ref{sec:conclusions} summarizes contributions and outlines future research directions.
Our implementation and complete replication package, including all experimental data
and analysis scripts, are publicly available at \url{https://github.com/XXX}~\cite{replicationpackage}.
