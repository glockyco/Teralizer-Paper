\section{Introduction}
\label{sec:introduction}

Conventional unit tests validate software behavior
by checking specific input-output pairs~\cite{orso_2014_software,ammann_2016_intro,myers_2011_art},
but leave most inputs along the same execution path untested.
Property-based testing~\cite{claessen_2000_quickcheck,hughes_2007_quickcheck} instead generates many inputs
and checks whether specified properties hold across executions.
For example, a unit test verifying that \texttt{abs(0)} returns \texttt{0} would 
still pass after an erroneous change from \texttt{x >= 0} to \texttt{x == 0}, 
whereas a property-based test asserting \texttt{abs(x) = x} for non-negative \texttt{x} 
would expose this regression (Figure~\ref{fig:regression-detection}).
Industrial experience reports suggest that property-based testing 
often uncovers edge cases and boundary conditions missed by unit tests~\cite{hughes_2016_experiences,goldstein_2024_pbt_practice}.
Adoption, however, remains limited because writing property-based tests 
requires manual effort to define both input constraints and suitable properties,
a task practitioners find challenging~\cite{goldstein_2024_pbt_practice}.
This motivates research into automated transformation approaches
that automatically generalize existing unit tests by deriving properties from program semantics.

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_regression_detection}
  \caption{The conventional unit test misses a regression that the property-based test detects.}
  \Description{Regression detection comparison between unit test and property-based test.
  Left shows buggy abs() implementation with incorrect condition (x == 0 instead of x >= 0),
  highlighted with red background for removed line and green for added line.
  Middle shows original @Test that still passes (1 test run, 0 failures)
  because it only tests input 0.
  Right shows @Property test that fails (1 test run, 1 failure)
  detecting the regression with inputs like 5 and 42,
  showing "expected <5> but was <-5>" error message.}
  \label{fig:regression-detection}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics[width=.95\linewidth]{figures/fig_generalization}
  \caption{\ToolTeralizer{} takes implementation and test code as input, and produces property-based tests as output.}
  \Description{Transformation workflow showing Teralizer converting a unit test to a property-based test.
  Left side shows the original abs() implementation and a JUnit test with @Test annotation
  that asserts abs(0) equals 0.
  Center shows the Teralizer transformation arrow.
  Right side shows the generated jqwik property-based test with @Property annotation,
  parameterized input (min=0) int x,
  and generalized assertion assertEquals(x, abs(x)).
  Annotations indicate replaced annotations, added constraints, added parameters,
  replaced expected values, and replaced arguments.}
  \label{fig:generalization}
\end{figure}

We propose a semantics-based approach for automated test generalization
that analyzes both test code and implementation code
to derive path-exact specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic}.
Our method determines which inputs follow the same execution path as existing tests
and transforms unit tests into property-based tests
that validate the same assertions across entire input partitions.
Because specifications are extracted directly from program semantics,
the resulting properties are exact for each execution path
and preserve the developer-provided oracles encoded in assertions.
This enables thorough testing within those validated behaviors.
To our knowledge, JARVIS~\cite{peleg_2018_jarvis} is the only prior work
that automatically generalizes unit tests into property-based tests.
However, JARVIS infers properties from input-output examples based solely on test code,
relying on predefined abstraction templates that yield overapproximations.
In contrast, our white-box approach leverages dynamic program analysis
to extract exact specifications for the execution paths exercised by the original tests.

We implement this approach in \ToolTeralizer{}, a prototype tool for Java
that transforms JUnit tests into property-based \ToolJqwik{} \cite{link_2022_jqwik} tests.
\ToolTeralizer{} follows a five-stage pipeline:
(1)~analyzing tests and their assertions regarding suitability for generalization,
(2)~identifying tested methods through data flow analysis,
(3)~extracting specifications through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
(4)~creating generalized property-based tests, and
(5)~filtering generalized tests to retain only those that improve fault detection capability.
Figure~\ref{fig:generalization} illustrates the effects of this transformation,
showing how a simple equality assertion \texttt{abs(0) = 0}
becomes the property \texttt{abs(x) = x},
valid for all non-negative values of \texttt{x}.

To evaluate the potential and limitations of semantics-based test generalization,
we apply \ToolTeralizer{} to three complementary datasets.
First, the \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench} provides controlled settings
with numeric-focused programs well-suited for symbolic analysis.
Because \DatasetEqBench{} lacks test suites, we generate tests using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}.
Second, utility methods extracted from Apache Commons projects
offer a middle ground between controlled and real-world scenarios.
Here, we can directly compare \ToolEvoSuite{}-generated and developer-written tests
on the same codebase, partially isolating the influence of test architecture and quality on generalization outcomes.
Finally, 632 real-world Java projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers},
with their existing developer-written tests, expose the full complexity of practical application conditions.
This progression from controlled to real-world conditions highlights
the achievable fault-detection improvements
as well as the theoretical and practical limitations of the proposed approach.

Our evaluation shows modest yet consistent improvements under controlled conditions.
On \ToolEvoSuite{}-generated tests, mutation scores increase by 1--4 percentage points:
from 48--52\% to 52--55\% on \DatasetEqBench{},
and from 57--58\% to 58--59\% on Apache Commons utilities.
In contrast, generalization of developer-written tests for Apache Commons utilities
shows minimal improvement (0.05--0.07 percentage points from a baseline of 80.35\%).
These mature tests have already detected most of the faults targeted by automated generalization.
Beyond mutation detection results,
our analysis of 632 real-world projects reveals practical applicability barriers:
only 1.7\% of projects successfully complete the full generalization pipeline.
Failures are primarily caused by 
fundamental type support limitations of symbolic analysis
as well as static analysis and constraint encoding limitations
of our current implementation of \ToolTeralizer{}.
To provide a clear roadmap for future work,
we classify these failures into
those that can be resolved through additional engineering effort
and those that represent deeper research challenges in specification extraction and encoding.

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Semantics-based test generalization:}
We introduce an automated approach for transforming unit tests into property-based tests
via semantics-based specification extraction,
analyzing both test code and implementation code to derive path-exact specifications.
Our prototype implementation called \ToolTeralizer{} demonstrates feasibility for Java projects,
achieving mutation score improvements of 1--4 percentage points
under conditions suitable for current symbolic analysis.

\item \textbf{Empirical dataset:}
We provide a dataset characterizing test structure
and generalizability across 630+ Java projects,
covering assertion patterns (types, frequencies, per-test distributions),
specification complexity metrics (operation counts, constraint utilization),
and detailed filtering outcomes explaining why 98.3\% of real-world generalizations fail.

\item \textbf{Analysis of applicability challenges:}
We analyze barriers to practical test generalization,
distinguishing addressable engineering limitations
(e.g., identifying tested methods in complex control flow, detecting assertions in helper methods)
from fundamental research challenges (e.g., constraint solving for non-numeric types),
and provide concrete guidance for advancing the field.

\item \textbf{Open implementation and replication package:}
We release a complete implementation and comprehensive replication package
with all experimental data, enabling reproduction and extension of our work.
\end{enumerate}

The paper is organized as follows.
Section~\ref{sec:background} introduces the technical foundations of test generalization.
Section~\ref{sec:approach} presents \ToolTeralizer{}'s five-stage pipeline.
Section~\ref{sec:evaluation} evaluates our approach through six research questions,
covering mutation score improvements, impact on test suite size and execution time,
runtime requirements, and causes of unsuccessful generalizations.
Section~\ref{sec:discussion} discusses the results, their implications, and threats to validity.
Section~\ref{sec:related-work} positions our work within the broader testing literature.
Finally, Section~\ref{sec:conclusions} summarizes contributions and outlines directions for future research.
Our implementation and full replication package, including all experimental data and analysis scripts, is publicly available~\cite{replicationpackage}.
