\section{Approach}
\label{sec:approach}

Consider the bonus calculation method in Listing~\ref{lst:bonus-method} that returns different rates based on sales performance.
The unit test in Listing~\ref{lst:original-test} verifying \texttt{calculate(2500, 1000)} returns \texttt{250}
passes for the current implementation but would miss regressions that alter behavior within the same partition.
For instance, changing the condition from \texttt{sales/2 >= target} to \texttt{sales/2 > target}
would still pass the original test (since \texttt{2500/2 > 1000}) but breaks the boundary case.
\ToolTeralizer{} automatically transforms this into a property-based test
that explores the partition boundary, catching such regressions
that the single-input test would miss.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_approach.drawio}
  \caption{Overview of Teralizer's test generalization process.}
  \Description{System architecture diagram showing Teralizer's three-phase pipeline.
  Input (left): Java Project box containing Implementation and Test Suite components.
  Center: Teralizer system with three numbered phases:
  (1) Test Analysis, (2) Specification Extraction, (3) Test Transformation.
  Below phases: Intermediate Outputs listing Processing Logs, Test/Assertion/Generalization Data,
  Input/Output Specifications, and External Tool Reports.
  Bottom row shows integrated tools: JUnit, JaCoCo, PIT, jqwik, Spoon, and JPF/SPF.
  Output (right): Generalized Tests box containing three variants: BASELINE, NAIVE, and IMPROVED.
  Thick arrows connect input to Teralizer and Teralizer to output.}
  \label{fig:approach-overview}
\end{figure}

\ToolTeralizer{} achieves this fully automated transformation
through a three-phase pipeline (Figure~\ref{fig:approach-overview})
that leverages existing symbolic analysis capabilities.
Our key design insight is that existing tests already encode
the necessary oracles for generalization:
developer-written assertions validate specific behaviors
that should hold for entire input partitions,
not just the single tested input.
By following the execution paths of existing tests,
we extract exact specifications only for behaviors that developers
have explicitly validated, avoiding the risk of overfitting
to implementation details of untested paths.
The three main phases of \ToolTeralizer{}'s pipeline are:

\begin{itemize}
    \item \textbf{Test Analysis} (Section~\ref{sec:test-analysis}) --
    identifies executable test methods via JUnit reports,
    and detects assertions as well as corresponding methods under test via static analysis.
    \item \textbf{Specification Extraction} (Section~\ref{sec:specification-extraction}) --
    uses \ToolSPFLong{} (\ToolSPF{}) in constraint collection mode
    for single-path symbolic analysis to infer input/output specifications
    of the methods under test along the execution paths exercised by identified tests.
    \item \textbf{Test Transformation} (Section~\ref{sec:test-transformation}) --
    uses the extracted input/output specifications
    to create property-based \ToolJqwik{} tests from original JUnit tests,
    generating three variants (\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{})
    to systematically study the mutation detection effectiveness and runtime efficiency
    of different input generation strategies.
\end{itemize}

Throughout the pipeline, we apply filtering to focus on tests
amenable to automated generalization (detailed analysis in Section~\ref{sec:limitations-eval}).
Current limitations stem from both fundamental challenges and engineering scope:
we require pure functions with numeric parameters (where \ToolSPF{} produces accurate constraints),
clear assertion-method relationships identifiable through static analysis,
and developer-provided assertions that serve as oracles.
These constraints reflect deliberate scope decisions
to demonstrate feasibility with reasonable engineering effort.

Our implementation targets Java 5--8 projects (limited by \ToolSPF{})
with Maven or Gradle build systems and JUnit 4/5 test suites.
To systematically understand the impact of these limitations,
we capture comprehensive metrics throughout the transformation pipeline,
documenting not only successful generalizations
but also the specific reasons tests cannot be generalized.
This data, available in our replication package~\cite{replicationpackage},
provides the research community with insights into
what makes tests amenable to generalization
and which technical advances would enable broader applicability.

Before processing begins, 
\ToolTeralizer{} prepares the target project by detecting the build system (Maven or Gradle)
and injecting necessary dependencies including \ToolJqwik{}, \ToolPit{}, and \ToolJacoco{}.
This instrumentation creates modified build files while preserving originals,
enabling the pipeline to execute tests and collect mutation testing results
required for evaluating generalization effectiveness.

\subsection{Test Analysis}
\label{sec:test-analysis}

The test analysis phase identifies generalizable tests
by establishing clear mappings between test assertions and the methods they validate.
This assertion-method mapping is challenging: tests often invoke multiple methods
and use intermediate variables,
requiring data flow analysis to determine which method an assertion actually tests.
For instance, our BonusCalculator test (Listing~\ref{lst:original-test}) contains multiple calculate invocations
stored in separate variables, followed by multiple assertions on those variables.
Our analysis must trace backwards from each assertion to identify
which specific calculate call produced the asserted value.
This precise mapping is essential because all input generators
must first include the original concrete input values
to guarantee we never miss inputs that developers explicitly encoded in their tests,
with additional inputs generated afterwards (Section~\ref{sec:three-variant-design}).
Section~\ref{sec:limitations-eval} analyzes how often this mapping fails in practice.

\ToolTeralizer{} first executes the original test suite
and parses JUnit XML reports to identify successfully executed test methods.
Using JUnit reports rather than custom static analysis
avoids reimplementing JUnit's test handling logic
for ignored tests and test failures,
ensuring we only process tests that actually pass
and can serve as reliable oracles for generalization.
For each identified test,
we use static analysis based on Spoon~\cite{pawlak_2016_spoon}
to locate assertions (calls to methods in \texttt{org.\-junit.\-Assert}
or \texttt{org.\-junit.\-jupiter.\-api.\-Assertions})
and trace data flow backwards to identify which method call
produced the value being asserted.
We currently support four assertion types:
\texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}, and \texttt{assertThrows}.

Test analysis produces assertion-method pairs that identify
which specific method invocation each assertion validates,
along with the concrete input values used in the original test.
This information enables the specification extraction phase
to obtain precise path conditions and symbolic outputs for each tested behavior.

Our data flow analysis handles different assertion types systematically.
For \texttt{assertThrows} assertions, 
we extract the executable body from the lambda expression
and identify the last method call,
which typically represents the operation expected to throw the exception.
For value assertions (\texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}),
we locate the actual parameter (whose position varies between JUnit versions)
and trace its origin.
If the actual value comes from a direct method invocation,
we use that method immediately.
If it comes from a variable,
we trace back to the variable's assignment statement
and extract the method call from the right-hand side.
When this analysis cannot resolve a clear method invocation,
we return null to indicate that the assertion
cannot be mapped to a testable method.

Our current implementation requires that tested methods
are pure functions: deterministic, side-effect-free,
and dependent only on their input parameters.
This constraint reflects both \ToolSPF{}'s symbolic execution capabilities
and our deliberate scope decisions --
\ToolSPF{} produces accurate constraints for numeric computations
but cannot model complex state changes or external effects,
while our single-path approach requires deterministic behavior
to ensure generalized tests validate the same semantics as originals.
Additionally, we only support numeric parameters
(\texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float}, \texttt{double})
where \ToolSPF{} produces accurate constraints.
Section~\ref{sec:limitations-eval} analyzes how often
these constraints limit applicability in practice,
while Section~\ref{sec:limitations} discusses the fundamental challenges
in extending type support beyond current capabilities.

\subsection{Specification Extraction}
\label{sec:specification-extraction}

The specification extraction phase uses \ToolSPF{}
to extract input/output specifications for each assertion.
These specifications capture two key elements:
the path condition that characterizes which inputs
follow the same execution path as the test,
and the symbolic output expression that computes
the expected result for any input in that partition.
Examples of extracted specifications from the \texttt{BonusCalculator.calculate} method are shown
in Listing~\ref{lst:input-output-specs}.
These specifications enable the test transformation phase
(Section~\ref{sec:test-transformation})
to create input generators and generalized oracles
for property-based tests.

We leverage \ToolSPF{}'s constraint collection mode,
which performs symbolic state tracking along concrete test paths
without the computational overhead of constraint solving
for path exploration.
This mode follows the test's execution path while maintaining
symbolic representations of variables,
collecting path conditions at each branch point.
However, constraint collection was not consistently enforced throughout \ToolSPF{}'s implementation,
requiring minor modifications to bypass constraint solving during collection mode.
Section~\ref{sec:runtime-eval} evaluates the runtime efficiency
of this approach.

\subsubsection{Driver Program Generation}
\label{sec:driver-generation}

To enable \ToolSPF{} analysis of individual assertions,
we generate a driver program for each assertion-method pair.
The driver program serves as an executable entry point
that isolates the targeted behavior within \ToolSPF{}'s execution environment,
ensuring that symbolic analysis focuses precisely on the tested method
without interference from unrelated test logic.

Each driver program consists of a simple class with a \texttt{main} method
that recreates the test execution context.
The driver instantiates the test class,
executes any setup methods annotated with \texttt{@Before}, \texttt{@BeforeClass},
\texttt{@BeforeEach}, or \texttt{@BeforeAll},
and then invokes the target test method.
This approach preserves the original test's execution environment
while providing \ToolSPF{} with a clean entry point for analysis.

Algorithm~\ref{alg:driver-generation} describes this process systematically.
For each assertion, we clone the original test class
and remove all other test methods and assertions,
leaving only the target behavior for focused analysis.
The driver generation process also creates the \ToolSPF{} configuration file
that specifies which method to analyze symbolically,
sets execution limits (60-second timeout, 100,000-character path condition limit, 100-step depth limit),
and configures our custom listener for specification extraction.

\begin{algorithm}[t]
\caption{Driver Program Generation}
\label{alg:driver-generation}
\begin{algorithmic}[1]
\REQUIRE Test class $C$, Test method $M$, Assertion $A$, Tested method $T$
\ENSURE Driver program $D$ and \ToolSPF{} configuration $F$
\STATE $I \gets $ create instrumented class from $C$ containing only $M$ with only $A$
\STATE $I_T \gets $ create instrumented method in $I$ that calls $T$ with symbolic parameters
\STATE Replace call to $T$ in $M$ with call to $I_T$
\STATE $S \gets $ detect setup methods in $C$ with annotations @Before*, @BeforeEach*
\STATE Generate driver class $D$:
\STATE \hspace{1em} Create \texttt{main} method
\STATE \hspace{1em} Instantiate $I$
\STATE \hspace{1em} Execute static setup methods from $S$ on class $I$
\STATE \hspace{1em} Execute instance setup methods from $S$ on instance
\STATE \hspace{1em} Invoke target test method $M$ on instance
\STATE Generate \ToolSPF{} configuration $F$:
\STATE \hspace{1em} Set \texttt{target} to driver class $D$
\STATE \hspace{1em} Set \texttt{symbolic.method} to instrumented method $I_T$
\STATE \hspace{1em} Enable \texttt{symbolic.collect\_constraints}
\STATE \hspace{1em} Configure execution limits and custom listener
\STATE \RETURN $(D, F)$
\end{algorithmic}
\end{algorithm}

The instrumentation process creates unique identifiers for each driver
to avoid conflicts when processing multiple assertions from the same test.
This systematic isolation ensures that each assertion gets its own focused
analysis environment within the \ToolSPF{} execution framework.

The driver program executes within \ToolSPF{}'s modified Java Virtual Machine,
which maintains both concrete and symbolic state throughout execution.
We register a custom \ToolSPF{} listener that monitors method entry and exit events,
starting symbolic state tracking when entering the tested method
and extracting specifications when exiting.
This listener implements several key mechanisms to ensure robust specification collection.

First, the listener tracks execution depth to handle method recursion correctly,
only extracting specifications when exiting the outermost call to the tested method.
Second, it enforces the configured execution limits during symbolic execution:
terminating analysis if the path condition exceeds 100,000 characters,
if execution time exceeds the 60-second timeout,
or if the search depth reaches the 100-step limit.
These safeguards prevent resource exhaustion on complex methods
while allowing successful analysis of the target behaviors.

When the tested method completes execution,
the listener captures both concrete and symbolic execution results.
The concrete results include the actual input parameter values
and the returned output value (or thrown exception),
providing ground truth for validation.
The symbolic results include the path condition
(conjunction of all branch conditions encountered along the execution path)
and the symbolic output expression
(output computed as a function of symbolic input variables).
For methods that throw exceptions,
we record the exception type as a constant symbolic output,
since all inputs satisfying the same path condition
produce the same exception type.

The listener transforms these \ToolSPF{}-specific symbolic representations
into our intermediate model format,
then serializes them as JSON files for later processing.
Once specifications are successfully extracted and persisted,
the listener terminates \ToolSPF{} execution immediately
to minimize runtime overhead --
no further symbolic exploration is needed
since we only analyze the single path exercised by the original test.

\subsection{Test Transformation}
\label{sec:test-transformation}

The test transformation phase converts JUnit tests into property-based \ToolJqwik{} tests
using the extracted specifications.
We use \ToolJqwik{}\footnote{\url{https://github.com/jqwik-team/jqwik}} because it is actively maintained,
well-documented, and integrates seamlessly with JUnit 5,
unlike alternatives such as junit-quickcheck\footnote{\url{https://github.com/pholser/junit-quickcheck}}
and quicktheories\footnote{\url{https://github.com/quicktheories/QuickTheories}}.

\subsubsection{Transformation Pipeline}
\label{sec:transformation-pipeline}

For each assertion-method pair, we generate a new test class containing
a property-based test. The transformation process:
(1) clones the original test class,
(2) removes all test methods except the target,
(3) removes all assertions except the target,
(4) replaces \texttt{@Test} with \texttt{@Property} annotations,
(5) adds a \texttt{TestParameters} class to encapsulate all numeric parameters
(a simple data structure with public fields for each generalizable parameter),
(6) adds a \texttt{TestParametersSupplier} to generate inputs,
and (7) replaces method arguments with values from \texttt{TestParameters}.
We generate one class per assertion rather than combining multiple assertions
because PIT mutation testing operates at class-level granularity --
this isolation ensures that failed generalizations do not prevent
successful ones from being included in mutation testing.
The original test method is preserved alongside the generalized version
to maintain backward compatibility.

\subsubsection{Three-Variant Ablation Design}
\label{sec:three-variant-design}

We generate three variants following standard ablation study methodology
to isolate different aspects of the approach,
enabling systematic analysis of how constraint complexity affects generation effectiveness
(Section~\ref{sec:key-findings}):
%
\textbf{\VariantBaseline{}} uses only the original test inputs,
measuring pure framework overhead without additional input generation.
This variant creates a \texttt{TestParametersSupplier} that returns
a single \texttt{TestParameters} instance with the original values.
%
\textbf{\VariantNaive{}} first includes the original test inputs,
then generates additional random inputs matching parameter types
and filters them against the input specification.
This approach generates additional values independently for each parameter,
then applies a filter function encoding the complete path condition.
For complex constraints, this leads to high rejection rates
and \texttt{TooManyFilterMissesException}s when generation cannot find valid inputs.
%
\textbf{\VariantImproved{}} first includes the original test inputs,
then generates additional inputs by partially encoding constraints
during input generation
to reduce filtering failures.
This variant analyzes the input specification to extract
equality and inequality constraints,
then generates additional values respecting these constraints
before applying the complete filter.
The next section details this constraint encoding strategy.

\subsubsection{Constraint Encoding Strategy}
\label{sec:constraint-encoding}

The \VariantImproved{} variant analyzes input specifications
to identify constraints that can be directly encoded in \ToolJqwik{} arbitraries.
We support five types of simple constraints:
equality (\texttt{x == y}),
strict inequality (\texttt{x < y}, \texttt{x > y}),
and non-strict inequality (\texttt{x <= y}, \texttt{x >= y}),
where \texttt{x} and \texttt{y} can be variables or constants.
Complex expressions involving arithmetic operations
(e.g., \texttt{x + y == z}) or function calls
(e.g., \texttt{sin(x) > 0}) cannot be encoded
and must be handled through filtering.
%
To handle circular dependencies (e.g., \texttt{a >= b \&\& b >= a}),
we rewrite constraints to apply only to the variable with the highest index.
For example, \texttt{a >= b} becomes \texttt{b <= a},
allowing us to first generate \texttt{a} without constraints,
then generate \texttt{b} respecting both \texttt{b <= a} and \texttt{b >= a}
(which together imply \texttt{b == a}).
This constraint rewriting enables us to generate custom jqwik arbitraries
that respect variable dependencies during runtime input generation.
Algorithm~\ref{alg:constraint-encoding} describes this constraint-aware generation process systematically.
We deliberately handle only simple equality and inequality constraints
to keep the approach tractable for demonstrating generalization feasibility,
while more complex constraints are handled through filtering.
%
Despite these optimizations, some valid constraints lead to unsatisfiable
situations during generation.
For example, with \texttt{b > a \&\& b <= 0},
if we generate \texttt{a = 0}, no valid \texttt{b} exists.
In such cases, we return an empty arbitrary,
prompting \ToolJqwik{} to retry with different values.

\begin{algorithm}[t]
\caption{Constraint-Aware Input Generation (\VariantImproved{})}
\label{alg:constraint-encoding}
\begin{algorithmic}[1]
\REQUIRE Input specification $S$, Parameters $P = \{p_1, \ldots, p_n\}$
\ENSURE Generated test inputs satisfying $S$
\STATE Assign indices: $idx(p_i) = i$ for $i \in \{1, \ldots, n\}$
\FORALL{constraints $c \in S$ of form $p_i \odot p_j$ where $\odot \in \{=, <, \leq, >, \geq\}$}
    \IF{$idx(p_i) > idx(p_j)$}
        \STATE Add constraint to $p_i$ based on $p_j$
    \ENDIF
    \IF{$idx(p_j) > idx(p_i)$}
        \STATE Rewrite constraint and add to $p_j$ based on $p_i$
        \STATE E.g., $p_i < p_j$ becomes $p_j > p_i$
    \ENDIF
\ENDFOR
\FOR{each parameter $p_i$ in index order}
    \STATE $E_i \gets$ equality constraints for $p_i$
    \STATE $L_i \gets$ lower bound constraints for $p_i$  
    \STATE $U_i \gets$ upper bound constraints for $p_i$
    \IF{$E_i \neq \emptyset$}
        \STATE Generate $p_i = $ value from first equality constraint
    \ELSIF{$L_i \neq \emptyset$ or $U_i \neq \emptyset$}
        \STATE $lower \gets \max(L_i)$ if exists, else type minimum
        \STATE $upper \gets \min(U_i)$ if exists, else type maximum
        \STATE Generate $p_i \in [lower, upper]$
    \ELSE
        \STATE Generate $p_i$ randomly within type bounds
    \ENDIF
\ENDFOR
\STATE Apply filter for non-encodable constraints
\STATE \RETURN generated inputs if filter passes
\end{algorithmic}
\end{algorithm}

\subsection{Running Example}
\label{sec:running-example}

We illustrate the complete transformation process
using a bonus calculation method as a running example.
Listings~\ref{lst:bonus-method}--\ref{lst:java-spec-encodings}
show the original implementation, test, extracted specifications,
and the three generated variants.
This example demonstrates how \ToolTeralizer{} transforms
a single assertion testing one specific input
into a property-based test that validates the same behavior
across hundreds of inputs within the same partition.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Implementation of the \texttt{calculate} method.}, label=lst:bonus-method]
class BonusCalculator {
  int calculate(int sales, int target) {
    if (sales / 2 >= target) {
      // exceptional performance
      return sales / 10; 
    } else if (sales >= target) {
      // good performance
      return sales / 20; 
    }
    // bad performance
    return 0; 
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Original test for the \texttt{calculate} method.}, label=lst:original-test]
@Test
void testCalculate() {
  BonusCalculator c = new BonusCalculator();
  int b1 = c.calculate(2500, 1000);
  int b2 = c.calculate(1500, 1000);
  int b3 = c.calculate(500, 1000);
  assertEquals(250, b1);
  assertEquals(75, b2);
  assertEquals(0, b3);
}
\end{lstlisting}

\begin{lstlisting}[caption={Input/output specifications of \texttt{calculate}.}, label=lst:input-output-specs]
exceptional performance:
- input:  sales / 2 >= target
- output: sales / 10 
good performance:
- input:  sales / 2 < target && sales >= target
- output: sales / 20
bad performance:
- input:  sales / 2 < target && sales < target
- output: 0
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Generalized test for the \textit{good performance} assertion.}, label=lst:generalized-test]
@Property(supplier = ..., tries = ...)
void testCalculate(TestParams _p_) {
  BonusCalculator c = new BonusCalculator();
  ...
  int b2 = c.calculate(_p_.sales, _p_.target);
  ...
  assertEquals(calculateExpected(_p_), b2);
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantBaseline{} supplier for \texttt{testCalculate(...)} inputs. The supplier uses the same inputs as the original test.}, label=lst:baseline-supplier]
class BaselineSupplier {
  Arbitrary get() {
    return Arbitraries.just(
      new TestParams(1500, 1000));
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantNaive{} supplier for \texttt{testCalculate(...)} inputs. The supplier generates random inputs and then filters them to only test cases that match the input specification.}, label=lst:naive-supplier]
class NaiveSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      sales -> Arbitraries.integers().map(
        target -> new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantImproved{} supplier for \texttt{testCalculate(...)} inputs. The supplier (partially) encodes the input specification, thus reducing \texttt{TooManyFilterMissesExceptions} compared to \VariantNaive{}.}, label=lst:improved-supplier]
class ImprovedSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      target -> Arbitraries.integers()
        // sales >= target is encoded
        // sales / 2 < target is not encoded
        .between(target, Integer.MAX_VALUE)
        .map(sales -> 
          new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Java encodings of input/oputput specifications.}, label=lst:java-spec-encodings]
boolean satisfiesInputSpec(TestParams _p_) {
  return _p_.sales / 2 <= _p_.target
    && _p_.sales >= _p_.target;
}

int calculateExpected(TestParams _p_) {
  return _p_.sales / 20;
}
\end{lstlisting}
\end{minipage}
\end{genericfloat}
}

\subsection{Filtering Strategy}
\label{sec:all-filtering}

Automated test generalization succeeds only when tests meet specific structural and semantic requirements.
\ToolTeralizer{} applies systematic filtering across three levels
that correspond to the pipeline phases,
characterizing the boundaries of current generalization capabilities.
This filtering strategy ensures generated tests execute correctly
while systematically documenting test characteristics that enable or prevent transformation.
The three-level cascade identifies distinct categories of constraints:
structural prerequisites at the test level,
specification extraction requirements at the assertion level,
and property generation feasibility at the generalization level.
Section~\ref{sec:limitations-eval} provides empirical quantification of these filtering boundaries.

\subsubsection{Test-Level Filtering: Structural Prerequisites}

Test-level filtering identifies the structural foundations required for generalization,
applying three primary filters that address distinct requirements.

The \textbf{NonPassingTest} filter excludes tests that fail during execution,
including originally failing tests and tests that fail after disabling \ToolEvoSuite{}'s isolation features
(which must be disabled to prevent \ToolPit{} crashes during property-based test execution).
This requirement stems from both practical constraints (mutation testing requires a green test suite)
and the fundamental need for reliable behavioral oracles.
Only tests that execute successfully provide trustworthy specifications for generalization.

The \textbf{TestType} filter restricts processing to standard \texttt{@Test} annotations,
excluding parameterized tests (\texttt{@ParameterizedTest}) and other specialized test types.
These alternative test patterns require different generalization strategies
beyond the current transformation approach.

The \textbf{NoAssertions} filter excludes tests lacking explicit assertions in the main test method body.
This includes tests that validate behavior solely through successful execution (no exception thrown)
and tests that delegate assertion logic to helper methods.
While both patterns represent valid testing approaches,
they cannot provide the explicit oracles needed for property-based test generation.
Our current implementation uses intraprocedural static analysis,
which cannot trace assertions in helper methods.
While interprocedural analysis could address this limitation,
we chose the simpler approach as sufficient for demonstrating generalization feasibility.


\subsubsection{Assertion-Level Filtering: Specification Extraction Requirements}

Assertion-level filtering addresses the requirements for extracting usable specifications,
revealing where current static analysis and symbolic execution capabilities
create boundaries for automated test generalization.
These filters check both the feasibility of identifying tested behaviors
and the compatibility of those behaviors with \ToolSPF{}'s constraint generation.

The \textbf{MissingValue} filter excludes assertions where static data flow analysis
cannot establish which method invocation produced the asserted value.
Common failure patterns include assertions on values computed through complex control flow
(e.g., conditional assignments across multiple branches),
indirect method calls stored in variables before assertion,
and unresolvable method declarations due to inheritance hierarchies or external dependencies.
These cases represent the practical limits of intraprocedural analysis
in the presence of modern Java programming patterns.

The \textbf{ParameterType} filter restricts generalization to methods with numeric and boolean parameters
(\texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float}, \texttt{double}, \texttt{boolean}).
This constraint reflects fundamental challenges in symbolic constraint generation for complex types~\cite{baldoni_2018_survey}.
While \ToolSPF{} can track symbolic values for strings, arrays, and objects during execution,
the resulting constraints often involve complex heap relationships and aliasing conditions
that cannot be readily encoded as input generators for property-based testing frameworks.
String constraint solving, despite significant advances in SMT solvers like Z3-str~\cite{zheng_2013_z3str},
remains computationally expensive for complex patterns~\cite{amadini_2021_string_survey}.
For example, string constraints may depend on internal character array representations
or involve complex regular expression patterns that exhibit PSPACE-complete satisfiability~\cite{chen_2022_regex_complexity}.

The \textbf{VoidReturnType} filter excludes methods with void return types
because generalization requires output specifications for property validation.
While void methods could theoretically be generalized by modeling side effects,
our current implementation focuses on pure functions with explicit return values.

The \textbf{AssertionType} filter restricts processing to assertion patterns
amenable to symbolic specification extraction:
\texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}, and \texttt{assertThrows}.
Unsupported assertion types include reference equality checks (\texttt{assertSame}),
null checks (\texttt{assertNull}), and array comparisons (\texttt{assertArrayEquals}).
These assertions involve object identity, memory layout, or structural comparisons
that cannot be adequately captured through \ToolSPF{}'s path-condition-based constraint generation,
which focuses on value-based relationships rather than heap topology.

The \textbf{ExcludedTest} filter ensures consistency by excluding assertions
from tests already filtered at the test level.

Beyond these proactive filters, assertion-level exclusions also result from
\ToolSPF{} execution failures during specification extraction.
These failures include exceptions from missing native method models
in \ToolSPF{}'s incomplete standard library implementation,
exceeded analysis limits (60-second execution timeout, 100,000-character path condition size limit, 100-step search depth limit),
and null pointer exceptions in our specification extraction implementation
when handling edge cases in symbolic state management.
These execution failures highlight the practical challenges
of applying symbolic execution tools to diverse real-world codebases,
where implementation completeness and robustness remain ongoing research challenges~\cite{baldoni_2018_survey}.

\subsubsection{Generalization-Level Filtering: Property Generation Feasibility}

Generalization-level filtering validates that extracted specifications can produce
executable property-based tests, revealing the boundaries of practical constraint handling.

The primary failure mode is \texttt{TooManyFilterMissesException}s,
which occur when \ToolJqwik{} cannot generate inputs satisfying path constraints
within its retry limits.
This reflects fundamental limitations in constraint-based input generation
for property-based testing~\cite{claessen_2000_quickcheck,link_2022_jqwik}:
when the discard ratio exceeds framework thresholds (typically 0.5),
generation fails rather than continuing expensive search.
While \VariantImproved{} encodes simple equality and inequality constraints
during input generation (Section~\ref{sec:constraint-encoding}),
complex constraints involving arithmetic expressions or function calls
must be handled through filtering.
When constraints are highly restrictive,
even constraint-aware generation struggles to find valid inputs,
resulting in generation failures.

Additional failures stem from inaccurate specifications
caused by patterns not properly handled by current specification extraction,
such as implicit preconditions, assertions within loops,
or tested methods called within loops.
These patterns can produce specifications that do not accurately capture
the intended behavior, leading to generalized tests that fail execution.

Finally, Java's 64KB method bytecode limit can prevent compilation
of generated tests with extremely large specifications,
necessitating preemptive exclusion of the most complex constraint sets.

These filtering outcomes illustrate the gap between
theoretical constraint extraction and practical input generation,
identifying constraint encoding and input generation as key research challenges
for expanding automated test generalization capabilities.
The boundary between extractable and generateable specifications
depends not only on constraint complexity
but also on the sophisticated interplay between symbolic analysis,
constraint satisfaction, and property-based testing framework capabilities.
