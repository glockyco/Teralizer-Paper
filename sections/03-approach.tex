\section{Approach}
\label{sec:approach}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_approach.drawio}
  \caption{Overview of Teralizer's test generalization process.}
  \Description{System architecture diagram showing Teralizer's three-stage pipeline.
  Input (left): Java Project box containing Implementation and Test Suite components.
  Center: Teralizer system with three numbered stages:
  (1) Test Analysis, (2) Specification Extraction, (3) Test Transformation.
  Below stages: Intermediate Outputs listing Processing Logs, Test/Assertion/Generalization Data,
  Input/Output Specifications, and External Tool Reports.
  Bottom row shows integrated tools: JUnit, JaCoCo, PIT, jqwik, Spoon, and JPF/SPF.
  Output (right): Generalized Tests box containing three variants: BASELINE, NAIVE, and IMPROVED.
  Thick arrows connect input to Teralizer and Teralizer to output.}
  \label{fig:approach-overview}
\end{figure}

\ToolTeralizer{} demonstrates the feasibility of semantics-based test generalization,
transforming conventional unit tests into property-based tests
by analyzing both test and implementation code.
While property-based tests can validate entire input partitions (Section~\ref{sec:property-based-testing}),
they traditionally require manual specification of generators and properties~\cite{goldstein_2024_pbt_practice},
a barrier that limits adoption in practice.
Our approach automates this transformation through a three-phase pipeline (Figure~\ref{fig:approach-overview}):
test analysis identifies assertions and maps them to tested methods,
specification extraction recovers path conditions and symbolic outputs through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
and test transformation generates property-based tests with constraint-aware input generation.

The oracle problem fundamentally shapes this design~\cite{barr_2015_oracle} (Section~\ref{sec:test-amplification}).
We can only generalize execution paths where developers have provided assertions,
as other paths lack validated oracles to distinguish intended behavior from incidental state changes or outputs.
This requirement leads us to follow the concrete execution of existing tests,
collecting conditions along their paths without exploring alternative branches.
Our white-box analysis of implementation code enables extraction of path-exact specifications,
unlike black-box approaches such as JARVIS~\cite{peleg_2018_jarvis}
that infer patterns solely from test input-output examples.
JARVIS applies predefined abstraction templates to these examples,
producing overapproximations that may be too general for the actual execution paths.

Figure~\ref{fig:approach-overview} presents \ToolTeralizer{}'s three-stage pipeline.
Throughout the pipeline, we apply filtering to focus on tests
amenable to semantics-based generalization
(Section~\ref{sec:limitations-eval} quantifies these exclusions),
with comprehensive metrics captured at each stage.
The three stages are:

\begin{itemize}
    \item \textbf{Test Analysis}:
    identifies executable test methods from JUnit reports
    and establishes assertion-to-method mappings through data flow analysis.
    \item \textbf{Specification Extraction}:
    uses \ToolSPFLong{} (\ToolSPF{})~\cite{pasareanu_2013_symbolic} in constraint collection mode
    to derive path conditions and symbolic outputs for each assertion-method pair.
    \item \textbf{Test Transformation}:
    generates property-based \ToolJqwik{}~\cite{link_2022_jqwik} tests from extracted specifications,
    producing three test variants (\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{})
    to evaluate different input generation strategies.
\end{itemize}

Consider the bonus calculation method in Listing~\ref{lst:bonus-method},
which returns different rates based on sales performance.
The unit test in Listing~\ref{lst:original-test} verifies that \texttt{calculate(2500, 1000)} returns \texttt{250}.
This test passes for the specified input but cannot detect regressions within the same partition.
For example, changing the condition from \texttt{sales/2 >= target} to \texttt{sales/2 > target}
preserves test success but breaks the boundary case.
\ToolTeralizer{} transforms this test into a property-based variant
that represents the entire partition, detecting such regressions.

Our implementation targets Java 5--8 projects (limited by \ToolSPF{})
with Maven or Gradle build systems and JUnit 4/5 test suites.
Before processing, \ToolTeralizer{} detects the build system
and injects necessary dependencies including \ToolJqwik{}~\cite{link_2022_jqwik}, \ToolPit{}~\cite{coles_2016_pit}, and \ToolJacoco{}\footnote{\url{https://github.com/jacoco/jacoco}}.
This instrumentation creates modified build files while preserving originals,
enabling the pipeline to execute tests and collect mutation testing results
for evaluating generalization effectiveness.

The following subsections detail each stage:
Section~\ref{sec:test-analysis} explains assertion-to-method mapping,
Section~\ref{sec:specification-extraction} describes path condition and symbolic output extraction,
Section~\ref{sec:test-transformation} presents the three generalization strategies,
and Section~\ref{sec:all-filtering} analyzes barriers preventing generalization.
Comprehensive metrics (available in our replication package~\cite{replicationpackage})
reveal what makes tests amenable to generalization
and highlight technical advances needed for broader applicability.

\subsection{Test Analysis}
\label{sec:test-analysis}

The test analysis stage maps each test assertion to the specific method invocation it validates.
This mapping requires tracing data flow through tests that often invoke multiple methods,
store results in intermediate variables, and assert on those variables.
For example, the BonusCalculator test (Listing~\ref{lst:original-test}) calls
\texttt{calculate} several times and then asserts on separate variables.
\ToolTeralizer{} must trace each assertion backward to the call that produced
the asserted value. This mapping is essential because all input generators
must include the original concrete inputs encoded by developers, with
additional inputs generated afterward (Section~\ref{sec:three-variant-design}).
Section~\ref{sec:limitations-eval} shows that this mapping fails for a notable fraction of assertions,
with substantially higher failure rates in real-world projects.

\ToolTeralizer{} first executes the original test suite and parses JUnit XML
reports to identify tests that successfully executed. Using JUnit reports,
rather than custom static analysis, avoids reimplementing JUnit's handling of
ignored tests and failures. This ensures we only process tests that actually
pass and can serve as validated oracles for generalization.

For each identified test, \ToolTeralizer{} uses static analysis based on
Spoon~\cite{pawlak_2016_spoon} to locate assertions (calls to methods in
\texttt{org.junit.Assert} or \texttt{org.junit.jupiter.api.Assertions})
and trace data flow backward to the producing method call.
The current implementation supports \texttt{assertEquals}, \texttt{assertTrue},
\texttt{assertFalse}, and \texttt{assertThrows}.

Test analysis outputs assertion-method pairs that record which invocation an
assertion validates and the concrete input values used in the original test.
These pairs drive specification extraction, enabling precise path conditions
(constraints on inputs) and symbolic outputs (expressions for expected results)
for each tested behavior.

We handle different assertion types systematically.
For \texttt{assertThrows}, we extract the executable body from the lambda expression
and identify the last method call, which typically represents the operation expected to throw.
For value assertions (\texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}),
we locate the actual parameter and trace its origin.
If the actual value comes from a direct method invocation, we use that method.
If it comes from a variable, we trace back to the variable's assignment
and extract the method call from the right-hand side.
When this analysis cannot resolve a clear method invocation,
we mark the assertion as unmappable.

Our current implementation requires that tested methods are pure functions:
deterministic, side-effect-free, and dependent only on their input parameters.
This purity requirement is common across symbolic execution tools~\cite{cadar_2013_symbolic,baldoni_2018_survey}
and white-box test generation approaches such as Pex~\cite{tillmann_2008_pex},
as non-determinism undermines the fundamental assumption that inputs determine execution paths.
Without determinism, the same inputs could follow different paths on different executions,
invalidating our extracted specifications.
We support only numeric parameters
(\texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float}, \texttt{double})
where \ToolSPF{} produces accurate constraints.
Section~\ref{sec:limitations-eval} shows these type limitations exclude a significant portion of assertions,
particularly in real-world projects,
while Section~\ref{sec:limitations} discusses the challenges
in extending type support beyond current capabilities.

\subsection{Specification Extraction}
\label{sec:specification-extraction}

Having identified assertion-method pairs through test analysis,
the specification extraction stage uses \ToolSPF{}
to extract input/output specifications for each assertion.
These specifications capture two key elements:
the path condition that characterizes which inputs
follow the same execution path as the test,
and the symbolic output expression that computes
the expected result for any input in that partition.
Examples of extracted specifications from the \texttt{BonusCalculator.calculate} method are shown
in Listing~\ref{lst:input-output-specs}.
These specifications enable the test transformation stage
(Section~\ref{sec:test-transformation})
to create input generators and generalized oracles
for property-based tests.

We leverage \ToolSPF{}'s constraint collection mode,
which follows the test's execution path collecting conditions at each branch
without invoking the constraint solver for branch exploration.
As discussed in Section~\ref{sec:symbolic-analysis},
this single-path approach avoids the exponential cost of full symbolic execution
while maintaining symbolic representations of variables
and accumulating path conditions along the concrete execution path.
We modified \ToolSPF{} to consistently bypass constraint solving
during collection mode, as the original implementation
occasionally invoked the solver despite the collection-only setting.
Section~\ref{sec:runtime-eval} shows this approach completes
specification extraction in under 10\% of total processing time.

To enable \ToolSPF{} analysis of individual assertions,
we generate driver programs that isolate each assertion-method pair
for focused symbolic analysis.
The driver program recreates the test execution context
by instantiating the test class,
executing setup methods (\texttt{@Before}, \texttt{@BeforeClass}, etc.),
and invoking the target test method with only the specific assertion of interest.
This isolation ensures that symbolic analysis focuses precisely on the tested method
without interference from unrelated test logic or assertions.

During execution, a custom \ToolSPF{} listener monitors method entry and exit events
to extract specifications when the tested method completes.
The listener captures both concrete execution results
(actual parameter values and return values)
and symbolic results (path conditions and symbolic output expressions).
We apply per-assertion execution limits to keep processing tractable for large test suites:
60-second timeout per specification extraction,
100,000-character path condition limit
(avoiding memory issues and Java's 64KB method bytecode limit in generated tests),
and 100-step depth limit for recursive methods.
Without such limits, test suites with hundreds of assertions could require
days of processing for complex methods.
These thresholds balance completeness with practical runtime constraints.
As shown in Section~\ref{sec:limitations-eval}, they exclude only a small fraction of assertions
while keeping most extractions under a few seconds.
Once specifications are extracted and serialized,
\ToolSPF{} terminates immediately since only the single test path requires analysis.

\subsection{Test Transformation}
\label{sec:test-transformation}

With specifications extracted for each assertion-method pair,
the test transformation stage converts JUnit tests into property-based \ToolJqwik{} tests.
We use \ToolJqwik{}\footnote{\url{https://github.com/jqwik-team/jqwik}}
as our property-based testing framework
because it is actively maintained, well-documented, and integrates seamlessly with JUnit 5,
unlike alternatives such as junit-quickcheck\footnote{\url{https://github.com/pholser/junit-quickcheck}}
and quicktheories\footnote{\url{https://github.com/quicktheories/QuickTheories}}.

\subsubsection{Transformation Pipeline}
\label{sec:transformation-pipeline}

For each assertion-method pair, we generate a new test class containing
a property-based test (as shown in Listing~\ref{lst:generalized-test}).
The transformation process:
(1) clones the original test class,
(2) removes all test methods except the target,
(3) removes all assertions except the target,
(4) replaces \texttt{@Test} with \texttt{@Property} annotations,
(5) adds a \texttt{TestParameters} class to encapsulate generalizable parameters
(a simple data structure with public fields for each numeric/boolean parameter),
(6) adds a \texttt{TestParametersSupplier} to provide input generators,
and (7) replaces generalizable method arguments with values from \texttt{TestParameters}
while non-generalizable arguments retain their original concrete values.
We generate one class per assertion rather than combining multiple assertions
because \ToolPit{} requires a green test suite and only supports class-level exclusions.
This isolation ensures that failed generalizations do not prevent
successful ones from being included in mutation testing.
The original test method is preserved alongside the generalized version
to maintain backward compatibility.

\subsubsection{Three-Variant Ablation Design}
\label{sec:three-variant-design}

We employ systematic ablation study methodology,
generating three variants to isolate the effects of different input generation strategies.
This allows us to determine whether sophisticated constraint handling improves effectiveness
or whether simple random generation suffices.
Section~\ref{sec:primary-effects-eval} reveals that no single variant dominates across all conditions:
\VariantNaive{} excels on projects with simple constraints and high prevalence of arithmetic mutations,
while \VariantImproved{} performs better on complex constraints despite its reduced arithmetic diversity.
The implementation differences between these variants are illustrated
in Listings~\ref{lst:baseline-supplier}--\ref{lst:improved-supplier}:
%
\textbf{\VariantBaseline{}} uses only the original test inputs,
measuring pure framework overhead without additional input generation.
This variant's supplier returns an arbitrary that always produces
a single \texttt{TestParameters} instance with the original values.
%
\textbf{\VariantNaive{}} first includes the original test inputs,
then generates additional random inputs matching parameter types
and filters them against the input specification.
This approach generates additional values independently for each parameter,
then applies a filter function encoding the complete path condition.
For complex constraints, this leads to high rejection rates
and \texttt{TooManyFilterMissesException}s when generation cannot find valid inputs.
%
\textbf{\VariantImproved{}} first includes the original test inputs,
then generates additional inputs by partially encoding constraints
during input generation to reduce filtering failures.
This variant analyzes the input specification to extract
equality and inequality constraints,
then generates additional values respecting these constraints
before applying the complete filter.
The next section details this constraint encoding strategy.

\subsubsection{Constraint Encoding Strategy}
\label{sec:constraint-encoding}

The \VariantImproved{} variant analyzes input specifications
to identify constraints that can be directly encoded in \ToolJqwik{} arbitraries
(jqwik's term for input generators).
We support five types of simple constraints:
equality (\texttt{x == y}),
strict inequality (\texttt{x < y}, \texttt{x > y}),
and non-strict inequality (\texttt{x <= y}, \texttt{x >= y}),
where \texttt{x} and \texttt{y} can be variables or constants.
Complex expressions involving arithmetic operations
(e.g., \texttt{x + y == z}) or function calls
(e.g., \texttt{sin(x) > 0}) cannot be encoded
and must be handled through filtering.
%
To handle circular dependencies (e.g., \texttt{a >= b \&\& b >= a}),
we rewrite constraints to apply only to the variable with the highest index.
Without this rewriting, such constraints create "dead ends" where
no valid assignment is possible.
For example, \texttt{a >= b} becomes \texttt{b <= a},
allowing us to first generate \texttt{a} without constraints,
then generate \texttt{b} respecting both \texttt{b <= a} and \texttt{b >= a}
(which together imply \texttt{b == a}).
This constraint rewriting enables us to generate custom jqwik arbitraries
that respect variable dependencies during runtime input generation.
Algorithm~\ref{alg:constraint-encoding} describes this constraint-aware generation process systematically.
Section~\ref{sec:boundary-detection-effectiveness} demonstrates that
this strategy's effectiveness correlates with constraint complexity:
projects with higher constraint utilization show greater improvements from \VariantImproved{}.
We handle only simple equality and inequality constraints
to keep the approach tractable.
More complex constraints are addressed through filtering rather than encoding.
%
Despite these optimizations, some valid constraints lead to unsatisfiable
situations during generation.
For example, with \texttt{b > a \&\& b <= 0},
if we generate \texttt{a = 0}, no valid \texttt{b} exists.
In such cases, we return an empty arbitrary,
prompting \ToolJqwik{} to retry with different values.

\begin{algorithm}[t]
\caption{Constraint-Aware Input Generation (\VariantImproved{})}
\label{alg:constraint-encoding}
\begin{algorithmic}[1]
\REQUIRE Input specification $S$, Parameters $P = \{p_1, \ldots, p_n\}$
\ENSURE Generated test inputs satisfying $S$
\STATE Assign indices: $idx(p_i) = i$ for $i \in \{1, \ldots, n\}$
\FORALL{constraints $c \in S$ of form $p_i \odot p_j$ where $\odot \in \{=, <, \leq, >, \geq\}$}
    \IF{$idx(p_i) > idx(p_j)$}
        \STATE Add constraint to $p_i$ based on $p_j$
    \ENDIF
    \IF{$idx(p_j) > idx(p_i)$}
        \STATE Rewrite constraint and add to $p_j$ based on $p_i$
        \STATE E.g., $p_i < p_j$ becomes $p_j > p_i$
    \ENDIF
\ENDFOR
\FOR{each parameter $p_i$ in index order}
    \STATE $E_i \gets$ equality constraints for $p_i$
    \STATE $L_i \gets$ lower bound constraints for $p_i$  
    \STATE $U_i \gets$ upper bound constraints for $p_i$
    \IF{$E_i \neq \emptyset$}
        \STATE Generate $p_i = $ value from first equality constraint
    \ELSIF{$L_i \neq \emptyset$ or $U_i \neq \emptyset$}
        \STATE $lower \gets \max(L_i)$ if exists, else type minimum
        \STATE $upper \gets \min(U_i)$ if exists, else type maximum
        \STATE Generate $p_i \in [lower, upper]$
    \ELSE
        \STATE Generate $p_i$ randomly within type bounds
    \ENDIF
\ENDFOR
\STATE Apply filter for non-encodable constraints
\STATE \RETURN generated inputs if filter passes
\end{algorithmic}
\end{algorithm}

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Implementation of the \texttt{calculate} method.}, label=lst:bonus-method]
class BonusCalculator {
  int calculate(int sales, int target) {
    if (sales / 2 >= target) {
      // exceptional performance
      return sales / 10; 
    } else if (sales >= target) {
      // good performance
      return sales / 20; 
    }
    // bad performance
    return 0; 
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Original test for the \texttt{calculate} method.}, label=lst:original-test]
@Test
void testCalculate() {
  BonusCalculator c = new BonusCalculator();
  int b1 = c.calculate(2500, 1000);
  int b2 = c.calculate(1500, 1000);
  int b3 = c.calculate(500, 1000);
  assertEquals(250, b1);
  assertEquals(75, b2);
  assertEquals(0, b3);
}
\end{lstlisting}

\begin{lstlisting}[caption={Input/output specifications of \texttt{calculate}.}, label=lst:input-output-specs]
exceptional performance:
- input:  sales / 2 >= target
- output: sales / 10 
good performance:
- input:  sales / 2 < target && sales >= target
- output: sales / 20
bad performance:
- input:  sales / 2 < target && sales < target
- output: 0
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Generalized test for the \textit{good performance} assertion.}, label=lst:generalized-test]
@Property(supplier = ..., tries = ...)
void testCalculate(TestParams _p_) {
  BonusCalculator c = new BonusCalculator();
  ...
  int b2 = c.calculate(_p_.sales, _p_.target);
  ...
  assertEquals(calculateExpected(_p_), b2);
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantBaseline{} supplier for \texttt{testCalculate(...)} inputs. The supplier uses the same inputs as the original test.}, label=lst:baseline-supplier]
class BaselineSupplier {
  Arbitrary get() {
    return Arbitraries.just(
      new TestParams(1500, 1000));
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantNaive{} supplier for \texttt{testCalculate(...)} inputs. The supplier generates random inputs and then filters them to only test cases that match the input specification.}, label=lst:naive-supplier]
class NaiveSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      sales -> Arbitraries.integers().map(
        target -> new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantImproved{} supplier for \texttt{testCalculate(...)} inputs. The supplier (partially) encodes the input specification, thus reducing \texttt{TooManyFilterMissesExceptions} compared to \VariantNaive{}.}, label=lst:improved-supplier]
class ImprovedSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      target -> Arbitraries.integers()
        // sales >= target is encoded
        // sales / 2 < target is not encoded
        .between(target, Integer.MAX_VALUE)
        .map(sales -> 
          new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Java encodings of input/oputput specifications.}, label=lst:java-spec-encodings]
boolean satisfiesInputSpec(TestParams _p_) {
  return _p_.sales / 2 <= _p_.target
    && _p_.sales >= _p_.target;
}

int calculateExpected(TestParams _p_) {
  return _p_.sales / 20;
}
\end{lstlisting}
\end{minipage}
\end{genericfloat}
}

\subsection{Filtering Strategy}
\label{sec:all-filtering}

Throughout the pipeline stages described above,
\ToolTeralizer{} applies systematic filtering
to identify which tests can be successfully generalized.
The filtering cascade operates at three levels:
test-level structural prerequisites (e.g., passing tests with standard @Test annotations),
assertion-level specification extraction requirements (e.g., identifiable method calls with numeric parameters),
and generalization-level property generation feasibility (e.g., specifications where jqwik can generate satisfying inputs).
Section~\ref{sec:limitations-eval} quantifies these exclusions:
under favorable conditions, roughly half of assertions can be successfully generalized,
while real-world projects face substantially higher exclusion rates.
The filtering patterns reveal both the capabilities of our current implementation
and the technical barriers that limit broader applicability.

\subsubsection{Test-Level Filtering}

Test-level filtering excludes tests that cannot provide reliable foundations for generalization.
The \textbf{NonPassingTest} filter excludes tests that fail during execution,
including originally failing tests and tests that fail after disabling \ToolEvoSuite{}'s isolation features
(required to prevent \ToolPit{} crashes during property-based test execution).
The \textbf{TestType} filter restricts processing to standard \texttt{@Test} annotations.
Parameterized tests (\texttt{@ParameterizedTest}) and other specialized test types
would require different transformation strategies not implemented in \ToolTeralizer{}.
The \textbf{NoAssertions} filter excludes tests lacking explicit assertions in the main test method body.
This includes two common patterns:
tests that validate behavior solely through successful execution
and tests that delegate assertion logic to helper methods.
\ToolTeralizer{}'s intraprocedural static analysis cannot trace assertions in helper methods.
Interprocedural analysis could address this engineering limitation.

\subsubsection{Assertion-Level Filtering}

Assertion-level filtering identifies which assertions \ToolTeralizer{} can successfully process
given its static analysis capabilities and \ToolSPF{}'s constraint generation limitations.
The \textbf{MissingValue} filter excludes assertions where \ToolTeralizer{}'s data flow analysis
cannot identify the tested method,
commonly due to conditional assignments across branches,
indirect method calls through variables,
or unresolved method declarations in inheritance hierarchies.
The \textbf{ParameterType} filter requires at least one numeric or boolean parameter for generalization
(\texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float}, \texttt{double}, \texttt{boolean}).
Methods with mixed parameter types can be partially generalized:
numeric parameters become property-based test inputs while non-numeric parameters
retain their original concrete values from the test.
While \ToolSPF{} can track symbolic values for strings, arrays, and objects,
it cannot extract complete constraints for these types~\cite{baldoni_2018_survey}.
Without accurate constraints for these complex types, we cannot precisely characterize their input partitions.
The \textbf{ReturnType} filter excludes methods with void return types
or unsupported non-numeric/boolean return types,
as \ToolSPF{} cannot produce meaningful symbolic outputs for these cases.
The \textbf{ExcludedTest} filter excludes assertions belonging to tests
already filtered at the test level, ensuring consistency across filtering stages.
The \textbf{AssertionType} filter supports \texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}, and \texttt{assertThrows}.
Unsupported assertions like \texttt{assertSame}, \texttt{assertNull}, and \texttt{assertArrayEquals}
involve object identity or structural comparisons
that \ToolTeralizer{}'s specification extraction approach cannot handle.
Additional exclusions result from \ToolSPF{} execution failures:
missing native method models,
exceeded analysis limits (60-second timeout, 100,000-character path condition limit),
and null pointer exceptions in \ToolTeralizer{}'s specification extraction implementation.

\subsubsection{Generalization-Level Filtering}

Generalization-level filtering validates that extracted specifications produce executable property-based tests.
The primary failure mode is \texttt{TooManyFilterMissesException}s,
which occur when \ToolJqwik{} cannot generate inputs satisfying path constraints
within its configured retry limits.
This reflects the fundamental inefficiency of filtering-based input generation
for complex constraints~\cite{claessen_2000_quickcheck,link_2022_jqwik}.
\VariantImproved{} reduces these failures by encoding simple equality and inequality constraints
during input generation (Section~\ref{sec:constraint-encoding}),
but complex constraints involving arithmetic expressions must still be handled through filtering.
Additional failures stem from inaccurate specifications
when \ToolTeralizer{} encounters patterns it cannot properly model:
implicit preconditions, assertions within loops, or tested methods called within loops.
These filtering outcomes identify specific areas where enhanced static analysis,
extended type support, and more sophisticated constraint handling
could expand \ToolTeralizer{}'s generalization capabilities.

\subsection{Approach Generalizability}
\label{sec:approach-generalizability}

While our implementation targets Java with SPF and jqwik,
the core approach of semantics-based test generalization
generalizes to other languages and tools.
The fundamental principle—extracting path-exact specifications through white-box analysis
of both test and implementation code—applies to any language
with appropriate tool support.

Adapting the approach requires three components:
(i)~a unit test framework with assertion mechanisms,
(ii)~a symbolic execution tool capable of constraint extraction,
and (iii)~a property-based testing framework.
Several language ecosystems provide all three components with varying maturity levels.
JVM-based languages offer the most straightforward adaptation path,
as they can directly leverage SPF:
Scala (ScalaTest + SPF + ScalaCheck),
Kotlin (Kotest + SPF + Kotest Property Testing),
and Groovy (Spock + SPF + jqwik).
Similarly, .NET languages benefit from Microsoft's Pex infrastructure:
C\# and F\# can combine xUnit/FsUnit with IntelliTest and FsCheck.
For native code, C and C++ programs can use KLEE~\cite{cadar_2008_klee}
with frameworks like RapidCheck for property generation,
though KLEE requires compilation to LLVM bitcode.
Python offers PyExZ3~\cite{ball_2015_pyexz3} paired with Hypothesis~\cite{maciver_2019_hypothesis},
though with less mature symbolic execution support than SPF or KLEE.

Beyond tool availability, two fundamental constraints affect generalizability.
First, the oracle problem persists regardless of language:
we can only generalize tests with developer-provided assertions,
as other execution paths lack validated expected behaviors.
Second, the pure function requirement remains universal:
methods with side effects, non-deterministic behavior, or external dependencies
challenge symbolic analysis across all platforms.
These constraints—both the fundamental requirements and varying tool maturity—mean that
while the approach itself is language-agnostic,
its practical realization remains tied to the capabilities of each language's symbolic execution ecosystem.
