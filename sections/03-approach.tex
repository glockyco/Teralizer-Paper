\newpage{}
\section{Approach}
\label{sec:approach}

\ToolTeralizer{} automates the transformation
of traditional unit tests into property-based tests (PBTs)
through a three-phase pipeline (Figure~\ref{fig:approach-overview}).
The approach combines static analysis,
extraction of input and output specifications from existing tests,
and automated test rewriting
to produce generalized PBTs that 
more thoroughly test covered input partitions
while preserving the intent of the original tests.
The three main phases of \ToolTeralizer{}'s generalization pipeline are:

\begin{itemize}
    \item \textbf{Test Analysis} (Section~\ref{sec:test-analysis}) --
    identifies executable test methods via JUnit reports,
    and detects assertions as well as corresponding methods under test via static analysis.
    \item \textbf{Specification Extraction} (Section~\ref{sec:specification-extraction}) --
    uses \ToolSPF{} in constraint collection mode
    to infer input/output specifications of the methods under test
    along the execution paths exercised by identified tests.
    \item \textbf{Test Transformation} (Section~\ref{sec:test-transformation}) --
    uses the extracted input/output specifications to rewrite original JUnit tests
    into property-based \ToolJqwik{} tests,
    generating three variants (\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{})
    to systematically study the mutation detection effectiveness and runtime efficiency
    of different input generation strategies.
\end{itemize}

Filtering (Section~\ref{sec:all-filtering}) is applied throughout the processing pipeline
to exclude tests that cannot be meaningfully generalized.
Test-level filtering removes non-standard tests and tests without assertions.
Assertion-level filtering excludes cases
for which \ToolTeralizer{} is unable to resolve a method under test
or for which the method under test only contains parameters with non-generalizable types.
Generalization-level filtering excludes generalized tests
that fail execution or do not improve fault detection.

Our current prototype implementation of \ToolTeralizer{}
handles projects targeting Java 5--8
(a limitation caused by our \ToolSPF{} dependency),
using Maven or Gradle as a build system, and using JUnit 4 or JUnit 5 for testing.
Generalization supports numeric and boolean parameters,
inherited from \ToolSPF{}'s constraint tracking capabilities,
though the approach could extend to other types given appropriate symbolic representations.
Dependencies that are required for the test generalization process
(i.e., JaCoCo, PIT, and jqwik)
are automatically added to the target project by \ToolTeralizer{}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_approach.drawio}
  \caption{Overview of Teralizer's test generalization process.}
  %\Description{@TODO}
  \label{fig:approach-overview}
\end{figure}

\subsection{Test Analysis}
\label{sec:test-analysis}

\ToolTeralizer{} begins by preparing the target project
and identifying candidates for generalization.
The tool accepts either a Git repository URL or local directory path,
cloning repositories as needed.
To enable evaluation without modifying original build files,
\ToolTeralizer{} creates a separate \texttt{pom.xml} or \texttt{build.gradle} file
that mirrors settings of the original build file
and additionally adds required dependencies:
an up-to-date JUnit version, \ToolJqwik{} for property-based testing,
\ToolJacoco{} for coverage measurement, and \ToolPit{} for mutation testing.
By using a separate build file, 
we ensure that all necessary generalization dependencies are available without
affecting original project builds.

To identify potentially generalizable tests,
\ToolTeralizer{} executes the original test suite of the target project
and parses JUnit XML reports to extract successfully executed test methods.
Using JUnit reports instead of custom static analysis to identify generalizable tests
ensures that any tests that are marked as ignored by developers
or identified as failing during test execution can be properly excluded from generalization
without having to reimplement any of JUnit's test handling logic.
For each identified, potentially generalizable test,
\ToolTeralizer{} uses custom static analysis based on Spoon to locate assertions,
i.e., calls to methods residing inside the \texttt{org.junit.Assert} (JUnit 4)
or \texttt{org.junit.jupiter.api.Assertions} (JUnit 5) package.
Furthermore, for each assertion, \ToolTeralizer{} identifies the method under test via basic data flow analysis.
Specifically, \ToolTeralizer{} uses Spoon
to detect which method produced the output value
that is tested by the assertion.

Assertion analysis assumes that each assertion tests exactly one method call
---
a deliberate simplification
that enables us to demonstrate feasibility of test generalization
with reasonable engineering effort.
For more complex cases that test side effects
such as changes in program state or external program outputs,
\ToolSPF{}'s symbolic representation of program state
does not provide enough information
to enable accurate generalization of test oracles.
However, we plan to extend \ToolTeralizer{}'s implementation in future work 
to support a larger subset of real-world test scenarios
by using alternative tools for specification inference
or by extending \ToolSPF{}'s symbolic state tracking
to include more in-depth descriptions of such side effects,
thus lifting this practical limitation while
maintaining the underlying generalization ideas.

\section{Approach}
\label{sec:approach}

\subsection{Overview}
\label{sec:approach-overview}

processing pipeline:
CLEANUP\_PROJECT(0),
%
DOWNLOAD\_PROJECT(1),
SETUP\_PROJECT(2),
%
ADD\_DEPENDENCIES(3),
BUILD\_PROJECT\_ORIGINAL(4),
%
GENERATE\_EVOSUITE\_TESTS(5),
POSTPROCESS\_EVOSUITE\_TESTS(6),
%
BUILD\_SPOON\_MODEL(7),
%
EXECUTE\_TESTS\_ORIGINAL(8),
COLLECT\_JUNIT\_REPORTS\_ORIGINAL(9),
COLLECT\_JACOCO\_DATA\_ORIGINAL(10),
FILTER\_TESTS\_ORIGINAL(11),
COLLECT\_PIT\_DATA\_ORIGINAL(12),
%
ANALYZE\_TESTS(13),
FILTER\_TESTS(14),
FILTER\_ASSERTIONS(15),
%
ADD\_JPF\_INSTRUMENTATION(16),
BUILD\_PROJECT\_INSTRUMENTED(17),
EXECUTE\_JPF(18),
ANALYZE\_JPF(19),
CLEANUP\_JPF\_INSTRUMENTATION(20),
%
BUILD\_PROJECT\_INITIAL(21),
EXECUTE\_TESTS\_INITIAL(22),
%
COLLECT\_JUNIT\_REPORTS\_INITIAL(23),
COLLECT\_JACOCO\_DATA\_INITIAL(24),
COLLECT\_PIT\_DATA\_INITIAL(25),
%
CLEANUP\_GENERALIZATION(26),
%
GENERALIZE\_TESTS(27),
BUILD\_PROJECT\_GENERALIZED(28),
%
EXECUTE\_TESTS\_GENERALIZED(29),
COLLECT\_JUNIT\_REPORTS\_GENERALIZED(30),
FILTER\_GENERALIZATIONS(31),
%
COLLECT\_JACOCO\_DATA\_GENERALIZED(32),
COLLECT\_PIT\_DATA\_GENERALIZED(33);

basically:
- instrument project
- detect test methods + assertions + tested methods
- for each assertion / tested method, collect input + output specification with SPF
- for each assertion / tested method, create generalization with 3 variants (utilizing extracted specifications): BASELINE, NAIVE, IMPROVED

at various points, apply filtering to avoid processing of tests / assertions / generalizations that are unlikely to successfully pass all processing steps (or even guaranteed to fail)

thoughout the pipeline, track (i) runtime, (ii) (intermediate) results (success? failure? causes? other task outputs / created files?), (iii) mutation / coverage / test data;

offers a cleanup task to revert any changes applied by generalization (we only add files, so cleanup is just removing files; no existing files are modified);

\subsection{Project Instrumentation}
\label{sec:project-instrumentation}

(shorten + merge this with the overview? we have so many subsections...)

accepts either a URL to Git repository or a path to local directory as target;
if target is a URL to a Git repository, the project is cloned automatically (with Git's default settings);
processing then continues the same for both types of projects;

detects JUnit 4 vs. JUnit 5 testing framework, others not supported;
detects Maven vs. Gradle (Groovy DSL) projects, others not supported;
adds required dependencies based on detected project type (update JUnit, add JUnit Vintage, add JaCoCo, add PIT, add jqwik);
creates separate pom.xml / build.gradle (with comments for additions by Teralizer) - original file is left untouched;
to verify successful instrumentation, project is built, tests executed, test / coverage / mutation results collected;

\subsection{Test / Assertion Detection and Analysis}
\label{sec:test-analysis}

execute test suite;
identify executed test methods via junit / surefire XML reports;
for each test method, identify all assertions in the method via Spoon (calls to methods in "org.junit.Assert" (JUnit 4) or "org.junit.jupiter.api.Assertions" (JUnit 5)));
for each (supported) assertion (assertEquals, assertTrue, assertFalse, assertThrows), identify one tested method call via Spoon (for assertThrows: the executed method, for assertEquals/True/False: the method that returned the "actual" value of the assertion);
we assume each assertion "tests" one method, and do not consider side effects => no support for, e.g., method sequences that modify object state (e.g.: list.add(...), assert(..., list.size()));
side effects could be modeled as inputs/outputs of the tested method;

tests and assertions are filtered if they cannot be (correctly) handled by the current implementation. for details, see Section "Test / Assertion / Generalization Filtering".

\subsection{Specification Extraction}
\label{sec:specification-extraction}

\subsubsection{Driver Generation}
\label{sec:driver-generation}

for each assertion / tested method call, create a driver program and a corresponding SPF configuration;
the driver program is a single class with a "public static void main(...)" method;

the main method:
(i) executes any setup methods of the test class (JUnit 4: "@Before", "@BeforeClass", JUnit 5: "@BeforeAll", "@BeforeEach"),
(ii) creates an instance of the test class, and then
(iii) calls the tested method.

the SPF configuration:
(i) sets the main method of the driver program as the entrypoint for SPF execution,
(ii) sets the tested method as SPF's "symbolicMethod" (using symbolic inputs for generalizable input parameters),
(ii) configures SPF to run in constraint-collection mode (=> no constraint solving, just following the concrete execution path),
(iii) registers + configures a custom listener that extracts input-output specifications (see next Section "SPF Execution"),
(iv) configures several execution limits (depth limit, execution time limit, etc.; see Section "Other Limits / Safeguards").

(note: we had to modify (fix?) SPF to disable constraint solving during constraint-collection mode execution.)
(note: we're actually also creating an instrumented version of the test class / method, but that's only there so we can more easily identify the tested method call during SPF execution if the same method is called multiple times.)

\subsubsection{SPF Execution}
\label{sec:spf-execution}

execute SPF once for each driver program;
start tracking symbolic state when entering the tested method;
when exiting the tested method:
(i) write the concrete input values to a file,
(ii) write the concrete output values to a file,
(iii) write the symbolic input values to a file (=> path condition / input specification),
(iv) write the symbolic output values to a file (=> output specification),
then immediately terminate the execution (no need to keep going - we have everything we need);

((show an example of extracted data here))

for tested methods that exit via thrown exception,
use the thrown exception (type) as the concrete output value.
no symbolic output value can be collected in this case
(because the (type of the) thrown exception is not a function of the symbolic input values,
but is instead constant for all sets of inputs in the partition).

\subsection{Test Transformation}
\label{sec:test-transformation}


using jqwik (1.8.5) for property-based testing (\url{https://github.com/jqwik-team/jqwik});
last official release in 2024 (1.9.2);
last commit 2 days ago (checked on: 2005-04-23);
590 GitHub stars;
built for junit 5!;
comprehensive user guide (\url{https://jqwik.net/docs/current/user-guide.html});

competition 1:
junit-quickcheck (\url{https://github.com/pholser/junit-quickcheck});
last official release in 2020 (1.0);
last commit 8 months ago;
590 GitHub stars;
built for junit 4, junit 5 support only via junit-vintage (\url{https://github.com/pholser/junit-quickcheck/issues/189#issuecomment-414706607});
documentation less comprehensive (\url{https://pholser.github.io/junit-quickcheck/site/1.0/index.html});

competition 2:
quicktheories (\url{https://github.com/quicktheories/QuickTheories});
last official release in 2018 (0.25);
last commit 6 years ago;
509 GitHub stars;
built for junit 4;

\subsubsection{"BASELINE" Generalization}
\label{sec:baseline-generalization}

transforms target test into a property-based test;
one test class per generalizable assertion;
(PIT only offers class-level selections / exclusions, so generating classes causes less "collateral damage" for failing generalizations);
uses only the original set of input values via custom arbitrary;

allows us to see how much runtime overhead jqwik introduces even without any generation of input values;

transformation steps:
clone the original test class (all further actions on the cloned class);
delete other test methods in the class (non-test methods need to be preserved because they might be used by the target test);
add a nested class "TestParameters" that can hold values for all generalizable parameters of the tested method (i.e., all parameters of type byte, short, int, long, float, double);
add a nested class "TestParametersSupplier" that can generate "arbitrary" (jqwik term) "TestParameters" instances;
for the BASELINE variant, only one instance of TestParameters is generated by the supplier;
this instance uses the same tested method input values as the original test;
delete all existingTest annotations from the test method (removing @Test is most important, but other annotations are removed as well because they are unlikely to be compatible with @Property);
add jqwik @Property annotation (seed = 0, ShrinkingMode.OFF, EdgeCasesMode.FIRST, tries = 10 / 50 / 200);
add parameter of type "TestParameters \_p\_" with annotation \@ForAll(supplier = TestParametersSupplier) to the test method;
replace tested method arguments with values from TestParameters instance \_p\_ (e.g., foo(a, b, c) -> foo(\_p\_.a, \_p\_.b, \_p\_.c); only for generalizable inputs, the others remain unchanged).
delete other assertions in the test method (unless they have return values that are used in the code, e.g., Exception e = assertThrows(...));
no need to modify assertions (because inputs stay the same, so expected outputs should also stay the same);

the original test method is always preserved in the current implementation;
this would not be necessary for cases where there is only a single assertion in the test method and generalization is successful;
this would also not be necessary for casses where all assertions in a test method are successfully generalized;
statistics on how common these cases are in R1 (or RQ2? or RQ4?).

because most of the test method is copied for each generalized assertion, this creates a lot of duplicate code;
the currently implementation does not optimize for this at all - statistics on test suite size increases in RQ2;
some of this could likely be avoided by putting in more engineering effort, e.g., automatically extracting setup functions that can be reused across all generalizations of a test method;
alternatively, we could add multiple TestParameters parameters (one for each generalized assertion) - but that might not be very maintainable either;

\subsubsection{"NAIVE" Generalization}
\label{sec:naive-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "naive" approach for selecting sets of input values;

basic approach:
step 1: randomly generate sets of values that match the types of input parameters;
step 2: apply filter to keep only value sets that satisfy the input specification;
repeat until desired number of input sets (we use 10, 50, 200 in the evaluation) has been generated (automatically done by jqwik, we just set how many we want);
still preserves original test inputs via a custom arbitrary => no reduction of mutation score due to "bad" random values;
beware that generalization does NOT change coverage - we only test additional inputs of already covered input partitions;

problem: many TooManyFilterMissesExceptions;
reason: depending on the input specification, randomly selecting sets of input values can be (very) unlikely to produce satisfying inputs (e.g., a = b = c => 3 random ints that are equal);

example: a = b = c (all ints);
randomly generate a;
randomly generate b;
randomly generate c;
apply the a = b = c filter (likely not a match => throw away and try again; after too many non-matching attempts => TooManyFilterMissesExceptions)

\subsubsection{"IMPROVED" Generalization}
\label{sec:improved-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "improved" approach for selecting sets of input values to reduce TooManyFilterMissesExceptions;

basic approach:
step 1: generate sets of input values that already take into account "as many constraints as possible";
step 2: apply filter to keep only value sets that satisfy the full input specification;
repeat until the desired number of input sets has been generated;
like NAIVE, IMPROVED also preserves original test inputs via a custom arbitrary;
like NAIVE, IMPROVED also does NOT cover any previously uncovered input partitions;

example: a = b \&\& b = c (all ints);
randomly generate a since we don't have any constraints to consider yet;
generate b such that b = a (i.e., take into account the a = b constraint);
generate c such that b = c (i.e., take into account the b = c constraint);
apply the a = b = c filter (trivial in this case => use a more interesting example);

currently only considers the following constraints:
var1 == (var2 | const);
var1 < (var2 | const);
var1 <= (var2 | const);
var1 > (var2 | const);
var1 >= (var2 | const);

supported types: byte, short, int, long, float, double;
mixed-type constraints are also supported (e.g., int-var < float-var);

more complex terms (e.g., "compound" terms (is this the correct terminology?), (trigonometric) function calls) are not taken into account (e.g., a < b - c, a == cos(b));
constraints that are not equality, upper- or lower-bound constraints are not taken into account either (e.g., inequality constraints);
=> show some statistics about used vs. unused constraints -> further details in RQ4 or the discussion;

actual value selection logic (code that implements this logic for all generalizable inputs is automatically generated):

if at least one equality constraint exists for a variable, all other constraints are ignored
if multiple equality constraints exist, we just take "the first one";
all equality constraints have the same value anyway because we select these at runtime based on whichever concrete values have already been assigned to involved variables;

if multiple upper / lower bounds exist, the strongest bound is used, i.e., the highest lower bound and the lowest upper bound.
as with equality constraints, this is determined at runtime, i.e., based on on whichever values have already been assigned to involved variables;

in practice, naive processing of constraints often leads to "dead ends" where no further assignments are possible;
for example: a >= b \&\& b >= a; here we would have a ">= b" constraint on "a" and a ">= a" constraint on "b";
to resolve such situations, we assign an index to each variable that occurs in the input specification, e.g., idx(a)=1, idx(b)=2;
constraints are then rewritten to apply only to the variable with the highest index, e.g. (i) "a >= b" -> "b <= a", and (ii) "b >= a" -> "b >= a";
thus, the used constraints for variable value selection become: a -> no constraints, b -> {"<= a", ">= a"};
since a is now unconstrained, we can simply select a random value for it; once a is assined, b can be assigned as well; 
similar transformations are applied for all suppored constraints (==, <, <=, >, >=);
(todo: find the correct terminology to describe this; perhaps constraint rewriting / simplification?
other related terms: constraint satisfaction problems, variable elimination, constraint propagation, domain reduction, and arc consistency algorithms)

in some cases, choices of early variables lead to unsatisfiable constraints later on;
for example: b > a \&\& b <= 0 -> no satisfying assignment for b if a a >= 0;
in this cases, we return an empty arbitrary for b, thus prompting jqwik to pick a new value for a;
similar situations occur for over-/underflows, e.g., b > a with a = Integer.MAX\_VALUE;
in this case, b = a + 1 = Integer.MIN\_VALUE, which will be rejected by filtering;

some current limitations could be resolved through more engineering effort (e.g., custom arbitraries);
one way to generate values that satisfy all / more constraints would be through constraint solving (add references);
however, this would have a significant runtime cost and would still suffer various limitations (add references);

\subsection{Test / Assertion / Generalization Filtering}
\label{sec:all-filtering}

describe filtering here or in RQ4?

we need to ensure that:
1: we do not generate any incompilable code;
2: we have a green test suite for mutation testing (PIT only works with green suites - otherwise it throws an error);
also, we would like to avoid spending processing effort on generalization attempts that are unlikely to be successful (=> early excludes);
to achieve this, we apply filtering at multiple stages + levels in the processing pipeline (for filtering data, see RQ4).

(note: large reduction of required filtering would be possible by putting in more engineering effort)

\subsubsection{Test-level Filtering}
\label{sec:test-filtering}

filterTestOriginal:
NonPassingTestFilter: filters failing tests (PIT requires green suite, and generalization of failing tests does not seem useful anyway);
TestTypeFilter: filters tests with unsupported test annotations (we currently only support @Test annotations);

filterTest:
NoAssertionsFilter: filters tests without assertions (with no assertions, choosing a target method is even trickier than it already is, and we have no useful output specification);

\subsubsection{Assertion-level Filtering}
\label{sec:assertion-filtering}

filterAssertion:
ExcludedTestFilter: filters assertions that are part of filtered or otherwise excluded tests (if the tests cannot be handled, assertion-level results cannot override this);
MissingValueFilter: filters assertions for which no tested method could be identified (without a tested method, we don't have specifications, so cannot perform generalization);
VoidReturnTypeFilter: filters assertions with a tested method that has a void return type (no return type -> no output specification -> no generalization);
UnsupportedAssertionFilter: filters unsupported assertions (we currently only support assertEquals, assertTrue, assertFalse, and (with some caveats?) asserThrows);
ParameterTypeFilter: filters assertions with a tested method that has no parameters of supported types (we can currently generalize parameters of types byte, short, int, long, float, double);

\subsubsection{Generalization-level Filtering}
\label{sec:generalization-filtering}

filterGeneralization:
NonPassingTestFilter: filters generalizations that fail during test execution (PIT requires green suite, can be either due to "incorrect" generalization or due to "bad" original tests);

\subsubsection{Other Limits / Safeguards}
\label{sec:other-filtering}

During SPF execution:
maximum PC size limit;
maximum depth limit;
maximum execution time;

During Test Transformation:
maximum Java specification size;

\subsection{Program Output / Collected Data}
\label{sec:collected-data}

primarily, we provide 1 property-based test for each original assertion (excluding ones that are filtered throughout the processing pipeline).
(do we need this section? or are outputs / data already explained well enough in the other section? even then, might still be useful to have a condensed overview here)

additionally, we provide (i) a PostgreSQL database with data about the processed projects and (ii) various intermediate results / log files.
(perhaps move this information to some "Data Availability" section.)

The database contains tables:
- project,
- test,
- assertion,
- generaliztion,
- filter\_result,
- (evosuite\_runtime),
- (evosuite\_report),
- junit\_test\_report,
- jacoco\_coverage\_report,
- pit\_coverage\_report,
- pit\_mutation\_report,
- task.

The intermediate results / log files contain:
- modified pom.xml / build.gradle files,
- command-data (all executed commands + output logs + error logs),
- jacoco-data (JaCoCo coverage CSV reports per project + variant),
- jpf-data (output logs, input-output values + specifications, driver + config + instrumented test files),
- junit-data (JUnit / Surefire XML reports per project + variant),
- pit-data (PIT linecoverage + mutations XML reports per project variant),

using PIT for mutation testing because;
most mutation testing tools
(i) do not provide results in a structured format
that's suitable for automated processing
and / or (ii) do not provide (official) support for Java 8,
and / or (iii) are not actively maintained anymore
(according to the PIT website:
\url{https://pitest.org/java_mutation_testing_systems/#summary-of-mutation-testing-systems})
using DEFAULTS set of mutators (see Table~\ref{tab:pit-mutators})
for further details about mutators, see the PIT website \footnote{\url{https://pitest.org/quickstart/mutators/}}
