\section{Approach}
\label{sec:approach}

\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_approach.drawio}
  \caption{Overview of Teralizer's test generalization process.}
  \Description{System architecture diagram showing Teralizer's five-stage pipeline.
  Input (left): Java Project box containing Implementation and Test Suite components.
  Center: Teralizer system with five numbered stages:
  (1) Test and Assertion Analysis, (2) Tested Method Identification, 
  (3) Specification Extraction, (4) Generalized Test Creation, (5) Test Suite Reduction.
  Below stages: Intermediate Outputs listing Processing Logs, Test/Assertion/Generalization Data,
  Input/Output Specifications, and External Tool Reports.
  Bottom row shows integrated tools: JUnit, JaCoCo, PIT, jqwik, Spoon, and JPF/SPF.
  Output (right): Generalized Tests box containing three variants: BASELINE, NAIVE, and IMPROVED.
  Thick arrows connect input to Teralizer and Teralizer to output.}
  \label{fig:approach-overview}
\end{figure}

\ToolTeralizer{} demonstrates the feasibility of semantics-based test generalization,
transforming conventional unit tests into property-based tests
by analyzing both test and implementation code.
While property-based tests can validate entire input partitions (Section~\ref{sec:property-based-testing}),
they traditionally require manual specification of generators and properties~\cite{goldstein_2024_pbt_practice},
a barrier that limits adoption in practice.
Our approach automates this transformation through a five-stage pipeline (Figure~\ref{fig:approach-overview}):
test and assertion analysis identifies executable tests and their assertions,
tested method identification maps assertions to the methods they validate,
specification extraction recovers path conditions and symbolic outputs through single-path symbolic analysis~\cite{pasareanu_2013_symbolic},
generalized test creation produces property-based tests with constraint-aware input generation,
and test suite reduction selects tests that improve fault detection capability.

The oracle problem fundamentally shapes this design~\cite{barr_2015_oracle} (Section~\ref{sec:test-amplification}).
We can only generalize execution paths where developers have provided assertions,
as other paths lack validated oracles to distinguish intended behavior from incidental state changes or outputs.
This requirement leads us to follow the concrete execution of existing tests,
collecting conditions along their paths without exploring alternative branches.
Our white-box analysis of implementation code enables extraction of path-exact specifications,
unlike black-box approaches such as JARVIS~\cite{peleg_2018_jarvis}
that infer patterns solely from test input-output examples.
JARVIS applies predefined abstraction templates to these examples,
producing overapproximations that may be too general for the actual execution paths.

Figure~\ref{fig:approach-overview} presents \ToolTeralizer{}'s five-stage pipeline.
Throughout the pipeline, we apply systematic filtering to focus on tests
amenable to semantics-based generalization.
The filtering cascade operates at multiple levels:
test-level structural prerequisites (e.g., passing tests with standard @Test annotations),
assertion-level specification extraction requirements (e.g., identifiable method calls with numeric parameters),
specification-level symbolic analysis constraints (e.g., pure functions with supported types),
and generalization-level property generation feasibility (e.g., specifications where jqwik can generate satisfying inputs).
Each stage applies appropriate filters to ensure only viable tests proceed to subsequent processing.
The five stages are:

\begin{itemize}
    \item \textbf{Test and Assertion Analysis}:
    identifies executable test methods from JUnit reports
    and locates assertions within each test.
    \item \textbf{Tested Method Identification}:
    establishes assertion-to-method mappings through data flow analysis.
    \item \textbf{Specification Extraction}:
    uses \ToolSPFLong{} (\ToolSPF{})~\cite{pasareanu_2013_symbolic} in constraint collection mode
    to derive path conditions and symbolic outputs for each assertion-method pair.
    \item \textbf{Generalized Test Creation}:
    generates property-based \ToolJqwik{}~\cite{link_2022_jqwik} tests from extracted specifications,
    producing three test variants (\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{})
    to evaluate different input generation strategies.
    \item \textbf{Test Suite Reduction}:
    uses mutation testing to identify and retain only generalized tests
    that detect faults not caught by existing tests.
\end{itemize}

Consider the bonus calculation method in Listing~\ref{lst:bonus-method},
which returns different rates based on sales performance.
The unit test in Listing~\ref{lst:original-test} verifies that \texttt{calculate(2500, 1000)} returns \texttt{250}.
This test passes for the specified input but cannot detect regressions within the same partition.
For example, changing the condition from \texttt{sales/2 >= target} to \texttt{sales/2 > target}
preserves test success but breaks the boundary case.
\ToolTeralizer{} transforms this test into a property-based variant
that represents the entire partition, detecting such regressions.

Our implementation targets Java 5--8 projects (limited by \ToolSPF{})
with Maven or Gradle build systems and JUnit 4/5 test suites.
Before processing, \ToolTeralizer{} detects the build system
and injects necessary dependencies including \ToolJqwik{}~\cite{link_2022_jqwik}, \ToolPit{}~\cite{coles_2016_pit}, and \ToolJacoco{}\footnote{\url{https://github.com/jacoco/jacoco}}.
This instrumentation creates modified build files while preserving originals,
enabling the pipeline to execute tests and collect mutation testing results
for evaluating generalization effectiveness.

The following subsections detail each stage:
Sections~\ref{sec:test-assertion-analysis} and \ref{sec:tested-method-identification} explain test identification and assertion-to-method mapping,
Section~\ref{sec:specification-extraction} describes path condition and symbolic output extraction,
Section~\ref{sec:generalized-test-creation} presents the three generalization strategies,
and Section~\ref{sec:test-suite-reduction} describes the mutation-based test selection process.
Comprehensive metrics (available in our replication package~\cite{replicationpackage})
reveal what makes tests amenable to generalization
and highlight technical advances needed for broader applicability.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Implementation of the \texttt{calculate} method.}, label=lst:bonus-method]
class BonusCalculator {
  int calculate(int sales, int target) {
    if (sales / 2 >= target) {
      // exceptional performance
      return sales / 10; 
    } else if (sales >= target) {
      // good performance
      return sales / 20; 
    }
    // bad performance
    return 0; 
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Original test for the \texttt{calculate} method.}, label=lst:original-test]
@Test
void testCalculate() {
  BonusCalculator c = new BonusCalculator();
  int b1 = c.calculate(2500, 1000);
  int b2 = c.calculate(1500, 1000);
  int b3 = c.calculate(500, 1000);
  assertEquals(250, b1);
  assertEquals(75, b2);
  assertEquals(0, b3);
}
\end{lstlisting}

\begin{lstlisting}[caption={Input/output specifications of \texttt{calculate}.}, label=lst:input-output-specs]
exceptional performance:
- input:  sales / 2 >= target
- output: sales / 10 
good performance:
- input:  sales / 2 < target && sales >= target
- output: sales / 20
bad performance:
- input:  sales / 2 < target && sales < target
- output: 0
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Generalized test for the \textit{good performance} assertion.}, label=lst:generalized-test]
@Property(supplier = ..., tries = ...)
void testCalculate(TestParams _p_) {
  BonusCalculator c = new BonusCalculator();
  ...
  int b2 = c.calculate(_p_.sales, _p_.target);
  ...
  assertEquals(calculateExpected(_p_), b2);
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantBaseline{} supplier for \texttt{testCalculate(...)} inputs. The supplier uses the same inputs as the original test.}, label=lst:baseline-supplier]
class BaselineSupplier {
  Arbitrary get() {
    return Arbitraries.just(
      new TestParams(1500, 1000));
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantNaive{} supplier for \texttt{testCalculate(...)} inputs. The supplier generates random inputs and then filters them to only test cases that match the input specification.}, label=lst:naive-supplier]
class NaiveSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      sales -> Arbitraries.integers().map(
        target -> new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={\VariantImproved{} supplier for \texttt{testCalculate(...)} inputs. The supplier partially encodes the input specification during generation, reducing filtering failures compared to \VariantNaive{}.}, label=lst:improved-supplier]
class ImprovedSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      target -> Arbitraries.integers()
        // sales >= target is encoded
        // sales / 2 < target is not encoded
        .between(target, Integer.MAX_VALUE)
        .map(sales -> 
          new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Java encodings of input/output specifications.}, label=lst:java-spec-encodings]
boolean satisfiesInputSpec(TestParams _p_) {
  return _p_.sales / 2 < _p_.target
    && _p_.sales >= _p_.target;
}

int calculateExpected(TestParams _p_) {
  return _p_.sales / 20;
}
\end{lstlisting}
\end{minipage}
\end{genericfloat}
}

\subsection{Test and Assertion Analysis}
\label{sec:test-assertion-analysis}

The test and assertion analysis stage identifies executable tests
and locates the assertions within each test.
\ToolTeralizer{} first executes the original test suite and parses JUnit XML
reports to identify tests that successfully executed. 
Using JUnit reports rather than custom static analysis avoids reimplementing
JUnit's handling of ignored and failed tests,
ensuring we only process tests that actually pass
and can serve as validated oracles for generalization.

For each identified passing test, \ToolTeralizer{} uses static analysis based on
Spoon~\cite{pawlak_2016_spoon} to locate assertions
(i.e., calls to methods in the \texttt{org.junit.Assert} package for JUnit~4
or the \texttt{org.junit.jupiter.api.Assertions} package for JUnit~5).
The current implementation supports \texttt{assertEquals}, \texttt{assertTrue},
\texttt{assertFalse}, and \texttt{assertThrows}.

Multiple filters apply at this stage to exclude tests and assertions
that cannot be generalized.
NonPassingTest filtering excludes tests that fail execution,
including those that fail after disabling EvoSuite's isolation features
to prevent crashes during mutation testing.
TestType filtering excludes non-standard test annotations
such as \texttt{@ParameterizedTest}.
Parameterized tests and other specialized test types
would require different transformation strategies not implemented in \ToolTeralizer{}.
NoAssertions filtering excludes tests lacking explicit assertions in the main test method body.
This includes two common patterns:
tests that validate behavior solely through successful execution
and tests that delegate assertion logic to helper methods.
\ToolTeralizer{}'s intraprocedural static analysis cannot trace assertions in helper methods.
Interprocedural analysis could address this engineering limitation.
ExcludedTest filtering ensures consistency by excluding assertions
from tests already filtered at the test level.
AssertionType filtering excludes assertion types not currently supported,
including reference equality checks (\texttt{assertSame}),
null checks (\texttt{assertNull}),
array comparisons (\texttt{assertArrayEquals}),
and type checks (\texttt{assertInstanceOf}).
Unsupported assertions like \texttt{assertSame}, \texttt{assertNull}, and \texttt{assertArrayEquals}
involve object identity or structural comparisons
that \ToolTeralizer{}'s specification extraction approach cannot handle.

\subsection{Tested Method Identification}
\label{sec:tested-method-identification}

The tested method identification stage maps each assertion to the specific method invocation it validates.
This mapping requires tracing data flow through tests that often invoke multiple methods,
store results in intermediate variables, and assert on those variables.
For example, the BonusCalculator test (Listing~\ref{lst:original-test}) calls
\texttt{calculate} several times and then asserts on separate variables.

\ToolTeralizer{} must trace each assertion backward to the call that produced
the asserted value. This mapping is essential because all input generators
must include the original concrete inputs encoded by developers.

We handle different assertion types systematically.
For \texttt{assertThrows}, we extract the executable body from the lambda expression
and identify the last method call within it,
which typically represents the operation expected to throw.
For value assertions (i.e., \texttt{assertEquals}, \texttt{assertTrue}, \texttt{assertFalse}),
we locate the actual parameter and trace its origin.
If the actual value comes from a direct method invocation, we use that method.
If it comes from a variable, we trace back to the variable's assignment
and extract the method call from the right-hand side.
This analysis targets well-structured unit tests that follow single-responsibility principles.
Complex data flow patterns such as method chaining or indirect calls through multiple variables
typically indicate test design issues outside our intended scope.

Multiple filters apply at this stage to exclude assertions
where method identification fails.
MissingValue filtering excludes assertions when static analysis
cannot identify which method call represents the tested method
or when the declaration of the tested method cannot be resolved by Spoon,
commonly due to conditional assignments across branches,
indirect method calls through variables,
or unresolved method declarations in inheritance hierarchies.
ParameterType filtering excludes assertions for tested methods
whose parameters are not generalizable types (numeric or boolean).
Methods with mixed parameter types can be partially generalized:
numeric parameters become property-based test inputs while non-numeric parameters
retain their original concrete values from the test.
While \ToolSPF{} can track symbolic values for strings, arrays, and objects,
it cannot extract complete constraints for these types.
Without accurate constraints for these complex types, we cannot precisely characterize their input partitions.
VoidReturnType filtering excludes assertions for methods with void return types,
as SPF does not report symbolic outputs for such methods.
When method identification cannot resolve a clear method invocation for an assertion,
\ToolTeralizer{} marks the assertion as excluded
and skips it during subsequent processing stages.

As a result of this analysis, \ToolTeralizer{} outputs assertion-method pairs that record which invocation
an assertion validates.
These pairs drive specification extraction, enabling extraction of
precise path conditions (constraints on tested method inputs)
and symbolic outputs (expressions representing tested method outputs)
for each method under test.


\subsection{Specification Extraction}
\label{sec:specification-extraction}

The specification extraction stage transforms assertion-method pairs from tested method identification
into input/output specifications using \ToolSPF{}.
Each specification comprises two elements:
the path condition that characterizes inputs following the same execution path as the test,
and the symbolic output expression that computes expected results for any input in that partition.
Listing~\ref{lst:input-output-specs} shows example specifications extracted from the \texttt{calculate} method.
These specifications enable generalized test creation
to produce property-based tests that validate assertions across entire input partitions.

We leverage \ToolSPF{}'s constraint collection mode,
which follows the test's concrete execution path while maintaining symbolic representations.
Unlike full symbolic execution, which explores all possible paths and faces exponential complexity,
this mode accumulates path conditions only along the executed path.
This focused approach aligns with the oracle problem discussed in Section~\ref{sec:test-amplification}:
validated oracles exist only for the executed path,
making exploration of alternative paths unnecessary.
To enable targeted symbolic analysis of each assertion-method pair,
we instrument the test, configure which parameters \ToolSPF{} treats symbolically,
and use a custom listener to capture specifications when the tested method completes.
During execution, \ToolSPF{} records conditions at each branch point.
When executing \texttt{calculate(1500, 1000)}, for example,
the condition \texttt{sales/2 >= target} evaluates to false (750 < 1000),
so \ToolSPF{} records the negated constraint \texttt{sales/2 < target}.
The subsequent condition \texttt{sales >= target} evaluates to true (1500 >= 1000),
yielding the complete path condition \texttt{sales/2 < target \&\& sales >= target}.
We modified \ToolSPF{} to consistently bypass constraint solving in collection mode,
as the original implementation occasionally invoked the solver despite the collection-only setting.

The approach operates within specific constraints.
\ToolTeralizer{}'s current implementation requires that tested methods are pure functions:
deterministic, side-effect-free, and dependent only on their input parameters.
This purity requirement is common across symbolic execution tools~\cite{cadar_2013_symbolic,baldoni_2018_survey}
and white-box test generation approaches such as Pex~\cite{tillmann_2008_pex},
as non-determinism undermines the fundamental assumption that inputs determine execution paths.
Without determinism, the same inputs could follow different paths on different executions,
invalidating our extracted specifications.
We support only boolean and numeric parameters
(\texttt{byte}, \texttt{short}, \texttt{int}, \texttt{long}, \texttt{float}, \texttt{double})
where \ToolSPF{} produces accurate constraints,
while strings, arrays, and objects remain concrete.

We impose resource limits to maintain tractability:
60-second timeout per assertion, 100,000-character path conditions, and 100-step recursion depth.
These limits ensure practical processing times based on evaluation findings
that show specification extraction consuming less than 10\% of total runtime (Section~\ref{sec:runtime-eval}).
Without these limits, processing hundreds of assertions in large test suites
could require days for methods with complex control flow.

SPF execution can fail for multiple reasons.
SPF exceptions constitute the majority of extraction failures,
occurring primarily due to missing models for native methods
in the current implementation of \ToolSPF{}
and implementation bugs in \ToolSPF{}/\ToolJPF{}.
The approach handles these failures gracefully by filtering failed extractions
and continuing with successful ones, maintaining robustness across diverse codebases.
Exceeded analysis limits account for another significant portion of failures:
extractions may be interrupted after exceeding the configured maximum specification size
to avoid timeouts and memory exhaustion in the presence of complex constraints,
while others exceed the configured maximum depth limit for similar reasons.
Timeouts and out-of-memory errors represent additional failure modes
that evade preemptive detection via the preceding measures.
Exceptions due to null pointer dereferences
in \ToolTeralizer{}'s current specification extraction implementation
represent the remaining failures.

Upon completion, \ToolSPF{} outputs the path condition and symbolic output expression,
then terminates immediately since only the single test path requires analysis.
For inputs \texttt{(1500, 1000)}, this yields
path condition \texttt{sales/2 < target \&\& sales >= target}
and output expression \texttt{sales/20},
providing the complete semantics for property-based test generation.

\subsection{Generalized Test Creation}
\label{sec:generalized-test-creation}

With specifications extracted for each assertion-method pair,
the generalized test creation stage converts JUnit tests into property-based \ToolJqwik{} tests.
We use \ToolJqwik{}\footnote{\url{https://github.com/jqwik-team/jqwik}}
as our property-based testing framework
because it is actively maintained, well-documented, and integrates seamlessly with JUnit 5,
unlike alternatives such as junit-quickcheck\footnote{\url{https://github.com/pholser/junit-quickcheck}}
and quicktheories\footnote{\url{https://github.com/quicktheories/QuickTheories}}.

\subsubsection{Transformation Pipeline}
\label{sec:transformation-pipeline}

For each assertion-method pair, we generate a new test class containing
a property-based test (as shown in Listing~\ref{lst:generalized-test}).
The transformation process:
(1) clones the original test class,
(2) removes all test methods except the target,
(3) removes all assertions except the target,
(4) replaces \texttt{@Test} with \texttt{@Property} annotations,
configuring the \texttt{supplier} parameter to specify which supplier class provides the input generators
and the \texttt{tries} parameter to control how many test executions to perform,
(5) adds a \texttt{TestParameters} class to encapsulate generalizable parameters
(a simple data structure with public fields for each numeric/boolean parameter),
(6) adds a \texttt{TestParametersSupplier} class that implements \ToolJqwik{}'s supplier interface---
suppliers are factory classes whose \texttt{get()} method returns arbitraries (the actual input generators),
and (7) replaces generalizable method arguments with values from \texttt{TestParameters}
while non-generalizable arguments retain their original concrete values.
We generate one class per assertion rather than combining multiple assertions
because \ToolPit{} requires a green test suite and only supports class-level exclusions.
This isolation ensures that failed generalizations do not prevent
successful ones from being included in mutation testing.
The original test method is preserved alongside the generalized version
to maintain backward compatibility.

\subsubsection{Three-Variant Ablation Design}
\label{sec:three-variant-design}

We employ systematic ablation study methodology,
generating three variants to isolate the effects of different input generation strategies.
This allows us to determine whether sophisticated constraint handling improves effectiveness
or whether simple random generation suffices.
The implementation differences between these variants are illustrated
in Listings~\ref{lst:baseline-supplier}--\ref{lst:improved-supplier}.

\textbf{\VariantBaseline{}} uses only the original test inputs,
measuring pure framework overhead without additional input generation.
This variant's supplier returns an arbitrary that always produces
a single \texttt{TestParameters} instance with the original values.
It serves as our control, isolating the cost of property-based test infrastructure
from the effects of input generation.

\textbf{\VariantNaive{}} first includes the original test inputs,
then generates additional random inputs matching parameter types
and filters them against the input specification.
This approach generates additional values independently for each parameter,
then applies a filter function encoding the complete path condition.
For complex constraints, this leads to high rejection rates
and \texttt{TooManyFilterMissesException}s when generation cannot find valid inputs.

\textbf{\VariantImproved{}} first includes the original test inputs,
then generates additional inputs by partially encoding constraints
during input generation to reduce filtering failures.
This variant analyzes the input specification to extract
equality and inequality constraints,
then generates additional values respecting these constraints
before applying the complete filter.
The next section details this constraint encoding strategy.

\subsubsection{Constraint Encoding Strategy}
\label{sec:constraint-encoding}

The \VariantImproved{} variant analyzes input specifications
to identify constraints that can be directly encoded in \ToolJqwik{} arbitraries.
\ToolJqwik{} uses the term \emph{arbitrary} for input generators that produce test data according to specified constraints---the jqwik equivalent of the generic "generators" concept in property-based testing.
We support five types of simple constraints:
equality (\texttt{x == y}),
strict inequality (\texttt{x < y}, \texttt{x > y}),
and non-strict inequality (\texttt{x <= y}, \texttt{x >= y}),
where \texttt{x} and \texttt{y} can be variables or constants.
Complex expressions involving arithmetic operations
(e.g., \texttt{x + y == z}) or function calls
(e.g., \texttt{sin(x) > 0}) cannot be encoded
and must be handled through filtering.
We handle only simple equality and inequality constraints
to keep the approach tractable.

To handle circular dependencies (e.g., \texttt{a >= b \&\& b >= a}),
we rewrite constraints to apply only to the variable with the highest index.
This index-based ordering provides a deterministic way to break constraint cycles
while maintaining generation feasibility, avoiding the computational overhead of full constraint solving.
Without this rewriting, such constraints create "dead ends" where
no valid assignment is possible.
For example, \texttt{a >= b} becomes \texttt{b <= a},
allowing us to first generate \texttt{a} without constraints,
then generate \texttt{b} respecting both \texttt{b <= a} and \texttt{b >= a}
(which together imply \texttt{b == a}).
This constraint rewriting enables us to generate custom jqwik arbitraries
that respect variable dependencies during runtime input generation.
Algorithm~\ref{alg:constraint-encoding} describes this constraint-aware generation process systematically.

Despite these optimizations, some valid constraints lead to unsatisfiable
situations during generation.
For example, with \texttt{b > a \&\& b <= 0},
if we generate \texttt{a = 0}, no valid \texttt{b} exists.
In such cases, we return an empty arbitrary,
prompting \ToolJqwik{} to retry with different values.

Generalized test creation can fail at multiple points.
\VariantBaseline{} generalized tests fail rarely,
as the transformation to property-based \ToolJqwik{} tests
is largely successful in producing compilable code
that matches the behavior of the original JUnit tests.
The few failures stem from inaccurate specifications caused by
implicit preconditions, assertions in loops,
assertions in transitively called methods,
or tested methods called within loops.
\VariantNaive{} and \VariantImproved{} variants
may be excluded to avoid Java's "code too large" compilation error
when specifications contain numerous complex constraints
that approach the 64KB method bytecode limit.
\VariantNaive{} variants show higher failure rates
due to \texttt{TooManyFilterMissesException}s,
occurring when \ToolJqwik{} cannot generate
inputs satisfying path constraints within its retry limit.
This reflects the fundamental inefficiency of filtering-based input generation
for complex constraints.
\VariantImproved{} variants reduce overall failure rates
through constraint-aware input generation by encoding simple equality and inequality constraints
during input generation,
but complex constraints involving arithmetic expressions must still be handled through filtering.
Failures from inaccurate specifications remain similar
across both variant types because specification extraction logic is shared.

\begin{algorithm}[t]
\caption{Constraint-Aware Input Generation (\VariantImproved{})}
\label{alg:constraint-encoding}
\begin{algorithmic}[1]
\REQUIRE Input specification $S$, Parameters $P = \{p_1, \ldots, p_n\}$
\ENSURE Generated test inputs satisfying $S$
\STATE Assign indices: $idx(p_i) = i$ for $i \in \{1, \ldots, n\}$
\FORALL{constraints $c \in S$ of form $p_i \odot p_j$ where $\odot \in \{=, <, \leq, >, \geq\}$}
    \IF{$idx(p_i) > idx(p_j)$}
        \STATE Add constraint to $p_i$ based on $p_j$
    \ENDIF
    \IF{$idx(p_j) > idx(p_i)$}
        \STATE Rewrite constraint and add to $p_j$ based on $p_i$
        \STATE E.g., $p_i < p_j$ becomes $p_j > p_i$
    \ENDIF
\ENDFOR
\FOR{each parameter $p_i$ in index order}
    \STATE $E_i \gets$ equality constraints for $p_i$
    \STATE $L_i \gets$ lower bound constraints for $p_i$  
    \STATE $U_i \gets$ upper bound constraints for $p_i$
    \IF{$E_i \neq \emptyset$}
        \STATE Generate $p_i = $ value from first equality constraint
    \ELSIF{$L_i \neq \emptyset$ or $U_i \neq \emptyset$}
        \STATE $lower \gets \max(L_i)$ if exists, else type minimum
        \STATE $upper \gets \min(U_i)$ if exists, else type maximum
        \STATE Generate $p_i \in [lower, upper]$
    \ELSE
        \STATE Generate $p_i$ randomly within type bounds
    \ENDIF
\ENDFOR
\STATE Apply filter for non-encodable constraints
\STATE \RETURN generated inputs if filter passes
\end{algorithmic}
\end{algorithm}

\subsection{Test Suite Reduction}
\label{sec:test-suite-reduction}

The test suite reduction stage uses mutation testing to identify and retain
only generalized tests that improve fault detection capability
beyond what the original test suite provides.
Since generalized tests exercise more inputs than their original counterparts,
they may detect additional mutants that the original test suite missed.
However, not all generalized tests provide this improvement:
some may only detect mutants already caught by other tests in the suite.

\ToolTeralizer{} uses \ToolPit{} to execute mutation testing
on three test suite variants:
\VariantOriginal{} (the original test suite),
\VariantInitial{} (the subset with successful specification extraction),
and the generalized test variants (\VariantBaseline{}, \VariantNaive{}, \VariantImproved{}).
By comparing which mutants each suite detects,
we identify generalized tests that catch mutants not detected by \VariantOriginal{}.

The selection criterion is straightforward:
retain only generalized tests that detect mutants
not caught by the original test suite.
This ensures that every generalized test in the final suite
contributes unique fault detection capability.
Generalized tests that only detect mutants already caught by existing tests
are excluded as redundant.

The final test suite combines the retained generalized tests
with the original test suite,
providing both the original developer-validated test cases
and the additional fault detection capability from generalization.
This approach maintains backward compatibility
while strengthening the test suite's ability to detect regressions
within the execution paths already covered by the original tests.


