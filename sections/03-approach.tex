\section{Approach}
\label{sec:approach}

\begin{figure}[b]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_approach.drawio}
  \caption{Overview of Teralizer's test generalization process.}
  \Description{System architecture diagram showing Teralizer's five-stage pipeline.
  Input (left): Java Project box containing Implementation and Test Suite components.
  Center: Teralizer system with five numbered stages:
  (1) Test and Assertion Analysis, (2) Tested Method Identification, 
  (3) Specification Extraction, (4) Generalized Test Creation, (5) Test Suite Reduction.
  Below stages: Intermediate Outputs listing Processing Logs, Test/Assertion/Generalization Data,
  Input/Output Specifications, and External Tool Reports.
  Bottom row shows integrated tools: JUnit, JaCoCo, PIT, jqwik, Spoon, and JPF/SPF.
  Output (right): Generalized Tests box containing three variants: BASELINE, NAIVE, and IMPROVED.
  Thick arrows connect input to Teralizer and Teralizer to output.}
  \label{fig:approach-overview}
\end{figure}

This section presents our semantics-based approach for automated test generalization,
which transforms conventional unit tests into property-based tests
by analyzing both test and implementation code.
While property-based tests can validate entire input partitions (Section~\ref{sec:property-based-testing}),
they traditionally require manual specification of generators and properties,
a barrier that limits adoption in practice~\cite{goldstein_2024_pbt_practice}.
Our approach automates this transformation by extracting path-exact specifications
through single-path symbolic analysis of existing test executions.
We implement this approach in \ToolTeralizer{}, a prototype tool for Java
that demonstrates the feasibility of semantics-based test generalization.

As shown in Figure~\ref{fig:approach-overview}, our approach follows a five-stage pipeline
that takes the implementation and corresponding tests of a software project as input
and produces generalized, property-based tests as output:
(1)~test and assertion analysis identifies potentially generalizable tests and their assertions,
(2)~tested method identification maps assertions to the methods they validate through data flow analysis,
(3)~specification extraction recovers input/output specifications from the tested methods through single-path symbolic analysis,
(4)~generalized test creation produces property-based tests with three input generation variants (\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{}),
and (5)~test suite reduction filters generalized tests to retain only those generalizations that measurably improve the mutation score of the overall test suite.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Implementation of the \texttt{calculate} method.}, label=lst:bonus-method]
class BonusCalculator {
  int calculate(int sales, int target) {
    if (sales / 2 >= target) {
      // exceptional performance
      return sales / 10; 
    } else if (sales >= target) {
      // good performance
      return sales / 20; 
    }
    // bad performance
    return 0; 
  }
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Original test for the \texttt{calculate} method.}, label=lst:original-test]
@Test
void testCalculate() {
  BonusCalculator c = new BonusCalculator();
  // exceptional performance:
  int b1 = c.calculate(2500, 1000);
  // good performance:
  int b2 = c.calculate(1500, 1000);
  // bad performance:
  int b3 = c.calculate(500, 1000);
  assertEquals(250, b1);
  assertEquals(75, b2);
  assertEquals(0, b3);
}
\end{lstlisting}

\end{minipage}
\end{genericfloat}
}

We illustrate our approach with a running example.
Listing~\ref{lst:bonus-method} shows a bonus calculation method with three distinct execution paths
for exceptional, good, and bad performance.
Listing~\ref{lst:original-test} shows a typical unit test that validates this method
by testing one representative input for each performance level.
While this test detects regressions that affect these three specific inputs,
it misses regressions that affect other inputs within the same partitions.
Consider a mutation that changes \texttt{sales / 2 >= target} to \texttt{sales / 2 > target},
replacing the non-strict inequality with a strict inequality.
Despite the change, the test still passes because all three tested inputs still follow the same execution paths.
However, all boundary cases where \texttt{sales = 2 * target} now incorrectly
return the good performance bonus instead of the exceptional performance bonus.
To detect such subtle regressions, \ToolTeralizer{} automatically transforms existing
unit tests into property-based tests that encode the intended behavior
for all inputs in the covered input partitions.

While the underlying generalization approach is independent of
specific programming languages or project environments,
our current implementation of \ToolTeralizer{} targets Java 5--8 projects
(imposed by our dependency on \ToolSPFLong{})
that use Maven or Gradle for dependency management and JUnit 4 or JUnit 5 for testing.
Before starting the main processing stages, \ToolTeralizer{} automatically detects the build system
that is used by the target project and injects necessary dependencies including
\ToolJqwik{}~\cite{link_2022_jqwik} for property-based testing,
\ToolPit{}~\cite{coles_2016_pit} for mutation testing,
and \ToolJacoco{}\footnote{\url{https://github.com/jacoco/jacoco}} for coverage tracking,
thus ensuring full automation without the need for manual preprocessing steps.
All changes that \ToolTeralizer{} applies are easily reversible via an integrated cleanup task.

The following subsections detail each stage of the main generalization process as implemented in \ToolTeralizer{}.
Section~\ref{sec:test-assertion-analysis} explains test and assertion analysis.
Section~\ref{sec:tested-method-identification} describes tested method identification.
Section~\ref{sec:specification-extraction} presents input/output specification extraction
and Section~\ref{sec:generalized-test-creation} covers generalized test creation.
Finally, Section~\ref{sec:test-suite-reduction} describes mutation-based test suite reduction.
The full implementation of \ToolTeralizer{} is available in our replication package~\cite{replicationpackage}.

\subsection{Test and Assertion Analysis}
\label{sec:test-assertion-analysis}

To transform unit tests into property-based tests, we must first identify which tests and assertions
are suitable candidates for generalization.
Not all tests reach this standard: real test suites frequently contain tests without assertions,
operations on complex data structures that resist symbolic analysis, or even failing tests.
Generalizable tests must contain explicit assertions that capture input-output relationships,
operate on data types that symbolic analysis can model accurately, and pass reliably.
This stage builds a database of tests and assertions that meet these criteria,
filtering out those that lack the necessary prerequisites for transformation into property-based tests.
By filtering unsuitable candidates early in the processing pipeline,
we avoid spending computational resources on unviable generalization attempts
and systematically track how prevalent different barriers to generalization are in practice.

To identify generalization candidates, \ToolTeralizer{} collects descriptions of all tests and assertions in the codebase.
First, it executes the original test suite to generate JUnit XML reports.
Next, it parses these reports to identify executed tests and their execution results.
For every test in the reports, \ToolTeralizer{} conducts static analysis via Spoon~\cite{pawlak_2016_spoon}
to extract source code locations, test annotations, used assertions, involved data types,
and various other structural information relevant for the generalization
(full data available in our replication package~\cite{replicationpackage}).
Based on the collected information, \ToolTeralizer{} applies filtering heuristics
to exclude tests and assertions that are unsuitable for generalization,
causing these tests and assertions to be skipped during subsequent processing steps.

Tests need to pass three filters:
\texttt{TestType}, \texttt{NonPassingTest}, and \texttt{NoAssertions}.
The \texttt{TestType} filter rejects tests that are not standard \texttt{@Test} methods,
such as \texttt{@ParameterizedTest}s that would require special handling not currently implemented in \ToolTeralizer{},
or \texttt{@RepeatedTest}s that indicate non-deterministic behavior incompatible
with symbolic analysis-based specification extraction (as described in Section~\ref{sec:symbolic-analysis}).
The \texttt{NonPassingTest} and \texttt{NoAssertions} filters both address the absence of validated oracles.
Failing tests do not reflect intended behavior,
so no property that encodes intended behavior can be inferred from them.
Similarly, tests without assertions lack explicit oracles,
preventing extraction of output specifications that match developer intent.
Assertions might be missing for two reasons:
(i) the test validates only that execution completes without crashing
or (ii) assertions are present but are contained in helper methods
that are not captured by \ToolTeralizer{}'s intraprocedural assertion detection.
While interprocedural analysis could detect delegated assertions
and test refactoring could make implicit validation explicit,
we leave such enhancements for future work.
Section~\ref{sec:limitations-eval} quantifies how frequently each filter excludes tests in practice.

Individual assertions within suitable tests require further analysis to determine generalizability.
\ToolTeralizer{} currently supports four assertion types: \texttt{assertEquals}, \texttt{assertTrue}, 
\texttt{assertFalse}, and \texttt{assertThrows} from both JUnit 4 and JUnit 5.
These assertions capture computational relationships that symbolic analysis can model.
The \texttt{AssertionType} filter excludes unsupported assertions such as reference equality checks 
(\texttt{assertSame}, \texttt{assertNull}) that validate object identity rather than computational properties,
and structural comparisons (\texttt{assertArrayEquals}, \texttt{assertInstanceOf})
that operate on complex data structures current symbolic execution cannot accurately model.
The \texttt{ExcludedTest} filter maintains consistency by excluding assertions from tests
already filtered at the test level.
These filters produce a refined database of potentially generalizable assertions,
annotated with the type information, arguments, and source locations
required for the next stage to identify which methods each assertion validates.


\subsection{Tested Method Identification}
\label{sec:tested-method-identification}

After identifying generalizable tests and assertions,
the next step in the generalization pipeline
is to identify which implementation methods are validated by the tests.
These methods under test (MUT) serve as targets for subsequent specification extraction.
The challenges encountered in this pipeline stage are twofold.
First, we must distinguish test setup code
from code that exercises and validates the MUT
to be able to restrict specification extraction
to relevant parts of the implementation.
Second, when a test validates multiple MUTs,
we must determine which MUT call corresponds to each assertion
so we can replace the expected value used in the assertion
with the right output specification when creating the corresponding generalized test.
Our current implementation of \ToolTeralizer{} achieves this
through static analysis based on Spoon~\cite{pawlak_2016_spoon}
that traces output values validated by assertions
back to the method calls that produced them.
As a result, \ToolTeralizer{} collects descriptions of
identified MUTs (source code locations, method signatures, etc.)
as well as mappings that connect each MUT call to a corresponding assertion.

Consider the \texttt{testCalculate} method in Listing~\ref{lst:original-test}.
To identify MUT calls in this method,
\ToolTeralizer{} uses Spoon to first get the \texttt{actual}
arguments that are passed into each assertion.
In our example, all three assertions are \texttt{assertEquals} calls with two integer parameters,
matching the method signature \texttt{static void assertEquals(int expected, int actual)}.
\ToolTeralizer{} thus identifies \texttt{b1}, \texttt{b2}, and \texttt{b3}
as the \texttt{actual} arguments of the three assertions.
Since all three arguments are local variables,
\ToolTeralizer{} uses Spoon to identify where they were defined.
For example, for \texttt{b1}, Spoon identifies \texttt{int b1 = c.calculate(2500, 1000)} as the definition.
Since the right side of the assignment is a method call,
\ToolTeralizer{} marks it as a MUT call, storing a description of the method
as well as a mapping to the corresponding \texttt{assertEquals(250, b1)} call in its database.
Processing \texttt{b2} and \texttt{b3} similarly identifies
\texttt{c.calculate(1500, 1000)} and \texttt{c.calculate(500, 1000)}
as the MUT calls for the second and third assertions.

Processing for other assertion types and code structures follows similar patterns.
\ToolTeralizer{} always identifies the assertion argument
that represents the (output of the) MUT call
as its first processing step.
If the argument is a method call,
\ToolTeralizer{} directly marks it as a MUT call.
Otherwise, \ToolTeralizer{} aims to identify
a corresponding method call from the argument.
For example, for local variables,
it traces them back to their definition
using the simple static data flow analysis based on Spoon
we described in the previous paragraph.
For lambda expressions, which are commonly used
as the \texttt{executable} argument of 
\texttt{assertThrows(Class expectedType, Executable executable)}
assertions,
\ToolTeralizer{} instead marks the last method call within the lambda expression as the MUT call,
following the heuristic that the last call typically triggers the expected exception.

Similar to test and assertion filtering,
\ToolTeralizer{} applies three filters to exclude MUTs that
are not viable candidates for generalization:
\texttt{MissingValue}, \texttt{ParameterType}, and \texttt{ReturnType}.
The \texttt{MissingValue} filter rejects cases where
\ToolTeralizer{} cannot identify a MUT for a given assertion
or cannot extract the full method signature for an identified MUT.
Common causes for this include
reversed \texttt{expected} and \texttt{actual} arguments (e.g., \texttt{assertEquals(abs(0), 0)}),
validation of object fields set as side effects rather than return values (e.g., \texttt{assertEquals(3, a.length)}),
and MUTs within inheritance hierarchies that Spoon cannot resolve.
The \texttt{ParameterType} and \texttt{ReturnType} filters reject MUTs
that use unsupported types for all of the MUT's parameters or return values.
While methods with mixed parameter types can be partially generalized
(numeric and boolean parameters become input parameters for property-based tests
while other parameters remain unchanged),
\ToolSPF{} cannot extract complete constraints
for parameters and return values that are strings, arrays, or objects.
This limitation to numeric and boolean types
aligns with current symbolic analysis capabilities
discussed in Section~\ref{sec:symbolic-analysis}.
Section~\ref{sec:limitations-eval} evaluates
how often all filtering-based exclusions occur.

\subsection{Specification Extraction}
\label{sec:specification-extraction}

The specification extraction stage takes
the MUT-to-assertion mappings from tested method identification
as its input and produces input/output specifications for every MUT as its output.
Each input/output specification captures two elements:
the path condition that describes which inputs
follow the same execution path through the MUT as the test,
and the symbolic output expression that describes expected results
for any input satisfying the path condition.
Our current implementation of \ToolTeralizer{}
extracts specifications through a two-step process:
first instrumenting tests to create controlled entry points for symbolic analysis,
then executing them with \ToolSPFLong{} (\ToolSPF{}) in constraint collection mode.
In this mode, \ToolSPF{} follows the test's concrete execution path
while maintaining symbolic representations,
extracting path-exact specifications without exploring alternative paths.

The first step, test instrumentation, generates three artifacts for each identified MUT:
an instrumented version of the test class,
a driver class, 
and a configuration file for \ToolSPF{}.
In our running example,
the \texttt{Instrumented} test class 
for the \textit{good performance} MUT call
(Listing~\ref{lst:instrumented-test})
wraps the original \texttt{c.calculate(1500, 1000)} call
in a new \texttt{wrapper} method that marks the starting point for symbolic analysis
while preserving the behavior of the original test.
The \texttt{Driver} class (Listing~\ref{lst:instrumented-test})
provides the entry point for \ToolSPF{}.
It first instantiates the instrumented test class,
then runs any setup code in methods annotated with one of JUnit's \texttt{@Before} annotations,
and finally executes the targeted test method \texttt{testCalculate}.
The \ToolSPF{} configuration (Listing~\ref{lst:jpf-config})
sets up symbolic analysis of the \texttt{wrapper} method,
registers a custom \texttt{TestGeneralizationListener} for specification extraction,
and configures relevant resource limits.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Driver and instrumented test class used for specification extraction of the \textit{good performance} case.}, label=lst:instrumented-test]
public class Driver {
  // Driver.main provides entry point for SPF:
  public static void main(String[] args) {
    Instrumented i = new Instrumented();
    i.testCalculate();
  }
}

public class Instrumented {
  @Test
  void testCalculate() {
    BonusCalculator c = new BonusCalculator();
    ...
    // Instrumented.wrapper marks the
    // starting point for symbolic analysis:
    int b2 = this.wrapper(c, 1500, 1000);
    ...
  }

  int wrapper(
    BonusCalculator c, int sales, int target
  ) {
    return c.calculate(sales, target);
  }
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[caption={\ToolSPFLong{} configuration used for specification extraction of the \textit{good performance} case.}, label=lst:jpf-config]
target=Driver
symbolic.method=Instrumented.wrapper(con#sym#sym)
symbolic.collect_constraints=true

listener=teralizer.jpf.TestGeneralizationListener

teralizer.max_execution_time=60.0
teralizer.max_path_condition_size=100000
search.depth_limit=100

...
\end{lstlisting}

\begin{lstlisting}[caption={Input/Output specifications extracted for the MUT calls in the \texttt{testCalculate} method.}, label=lst:extracted-specs]
exceptional performance:
- input:  sales / 2 >= target
- output: sales / 10

good performance:
- input:  sales / 2 < target && sales >= target
- output: sales / 20

bad performance:
- input:  sales / 2 < target && sales < target
- output: 0
\end{lstlisting}

\end{minipage}
\end{genericfloat}
}

The second step, symbolic analysis, executes these artifacts with \ToolSPF{},
which follows the test's concrete execution path
while maintaining symbolic representations of the wrapper method's numeric parameters.
For the \textit{good performance} case with concrete inputs (1500, 1000),
the first if condition \texttt{sales / 2 >= target}
in the \texttt{calculate} method (see Listing~\ref{lst:bonus-method})
evaluates to false (i.e., 1500 / 2 is \textit{not} greater than or equal to 1000),
so \ToolSPF{} records the negated constraint \texttt{sales / 2 < target}.
The second if condition \texttt{sales >= target} evaluates to true (i.e., 1500 \textit{is} greater than or equal to 1000),
adding \texttt{sales >= target} to the accumulated path condition.
When the wrapper method returns, our custom \texttt{TestGeneralizationListener}
captures the complete path condition
(\texttt{sales / 2 < target \&\& sales >= target}) and the symbolic output expression (\texttt{sales / 20}),
writes both concrete input/output values and symbolic input/output specifications to JSON files,
and then immediately terminates \ToolSPF{} without exploring alternative paths.
Listing~\ref{lst:extracted-specs} shows the input/output specifications
that are collected for the three identified MUT calls of our running example.

Single-path symbolic analysis requires tested methods to be pure functions, i.e.,
deterministic, side-effect-free, and dependent only on their input parameters~\cite{cadar_2013_symbolic,baldoni_2018_survey}.
Furthermore, \ToolSPF{} can only provide precise specifications for numeric and boolean values.
Because of this, \ToolTeralizer{} only targets generalization of
numeric and boolean inputs, leaving string, array, and object inputs unchanged.
If no input/output specification can be extracted for a given MUT,
\ToolTeralizer{} excludes this MUT from further processing.
The primary causes of such exclusions are \ToolSPF{} errors,
NullPointerExceptions for certain edge cases
in our current implementation of \ToolTeralizer{},
and exceeded resource limits.
By default, \ToolTeralizer{} uses a 60-second timeout per MUT,
a 100,000-character limit per path condition,
and a function call depth limit of 100.
We empirically determined these settings to provide
a reasonable trade-off between resource consumption and result quality.
Section~\ref{sec:limitations-eval} shows how often all of the mentioned causes
lead to exclusions in our evaluation.

\subsection{Generalized Test Creation}
\label{sec:generalized-test-creation}

With specifications extracted for each assertion-method pair,
the generalized test creation stage converts JUnit tests into property-based \ToolJqwik{} tests.
We use \ToolJqwik{}\footnote{\url{https://github.com/jqwik-team/jqwik}}
as our property-based testing framework
because it is actively maintained, well-documented, and integrates seamlessly with JUnit 5,
unlike alternatives such as junit-quickcheck\footnote{\url{https://github.com/pholser/junit-quickcheck}}
and quicktheories\footnote{\url{https://github.com/quicktheories/QuickTheories}}.

\subsubsection{Transformation Pipeline}
\label{sec:transformation-pipeline}

For each assertion-method pair, we generate a new test class containing
a property-based test (as shown in Listing~\ref{lst:generalized-test}).
The transformation process:
(1) clones the original test class,
(2) removes all test methods except the target,
(3) removes all assertions except the target,
(4) replaces \texttt{@Test} with \texttt{@Property} annotations,
configuring the \texttt{supplier} parameter to specify which supplier class provides the input generators
and the \texttt{tries} parameter to control how many test executions to perform,
(5) adds a \texttt{TestParameters} class to encapsulate generalizable parameters
(a simple data structure with public fields for each numeric/boolean parameter),
(6) adds a \texttt{TestParametersSupplier} class that implements \ToolJqwik{}'s supplier interface---
suppliers are factory classes whose \texttt{get()} method returns arbitraries (the actual input generators),
and (7) replaces generalizable method arguments with values from \texttt{TestParameters}
while non-generalizable arguments retain their original concrete values.
We generate one class per assertion rather than combining multiple assertions
because \ToolPit{} requires a green test suite and only supports class-level exclusions.
This isolation ensures that failed generalizations do not prevent
successful ones from being included in mutation testing.
The original test method is preserved alongside the generalized version
to maintain backward compatibility.

\subsubsection{Three-Variant Ablation Design}
\label{sec:three-variant-design}

We employ systematic ablation study methodology,
generating three variants to isolate the effects of different input generation strategies.
This allows us to determine whether sophisticated constraint handling improves effectiveness
or whether simple random generation suffices.
The implementation differences between these variants are illustrated
in Listings~\ref{lst:baseline-supplier}--\ref{lst:improved-supplier}.

\textbf{\VariantBaseline{}} uses only the original test inputs,
measuring pure framework overhead without additional input generation.
This variant's supplier returns an arbitrary that always produces
a single \texttt{TestParameters} instance with the original values.
It serves as our control, isolating the cost of property-based test infrastructure
from the effects of input generation.

\textbf{\VariantNaive{}} first includes the original test inputs,
then generates additional random inputs matching parameter types
and filters them against the input specification.
This approach generates additional values independently for each parameter,
then applies a filter function encoding the complete path condition.
For complex constraints, this leads to high rejection rates
and \texttt{TooManyFilterMissesException}s when generation cannot find valid inputs.

\textbf{\VariantImproved{}} first includes the original test inputs,
then generates additional inputs by partially encoding constraints
during input generation to reduce filtering failures.
This variant analyzes the input specification to extract
equality and inequality constraints,
then generates additional values respecting these constraints
before applying the complete filter.
The next section details this constraint encoding strategy.

\subsubsection{Constraint Encoding Strategy}
\label{sec:constraint-encoding}

The \VariantImproved{} variant analyzes input specifications
to identify constraints that can be directly encoded in \ToolJqwik{} arbitraries.
\ToolJqwik{} uses the term \emph{arbitrary} for input generators that produce test data according to specified constraints---the jqwik equivalent of the generic "generators" concept in property-based testing.
We support five types of simple constraints:
equality (\texttt{x == y}),
strict inequality (\texttt{x < y}, \texttt{x > y}),
and non-strict inequality (\texttt{x <= y}, \texttt{x >= y}),
where \texttt{x} and \texttt{y} can be variables or constants.
Complex expressions involving arithmetic operations
(e.g., \texttt{x + y == z}) or function calls
(e.g., \texttt{sin(x) > 0}) cannot be encoded
and must be handled through filtering.
We handle only simple equality and inequality constraints
to keep the approach tractable.

To handle circular dependencies (e.g., \texttt{a >= b \&\& b >= a}),
we rewrite constraints to apply only to the variable with the highest index.
This index-based ordering provides a deterministic way to break constraint cycles
while maintaining generation feasibility, avoiding the computational overhead of full constraint solving.
Without this rewriting, such constraints create "dead ends" where
no valid assignment is possible.
For example, \texttt{a >= b} becomes \texttt{b <= a},
allowing us to first generate \texttt{a} without constraints,
then generate \texttt{b} respecting both \texttt{b <= a} and \texttt{b >= a}
(which together imply \texttt{b == a}).
This constraint rewriting enables us to generate custom jqwik arbitraries
that respect variable dependencies during runtime input generation.
Algorithm~\ref{alg:constraint-encoding} describes this constraint-aware generation process systematically.

Despite these optimizations, some valid constraints lead to unsatisfiable
situations during generation.
For example, with \texttt{b > a \&\& b <= 0},
if we generate \texttt{a = 0}, no valid \texttt{b} exists.
In such cases, we return an empty arbitrary,
prompting \ToolJqwik{} to retry with different values.

Generalized test creation can fail at multiple points.
\VariantBaseline{} generalized tests fail rarely,
as the transformation to property-based \ToolJqwik{} tests
is largely successful in producing compilable code
that matches the behavior of the original JUnit tests.
The few failures stem from inaccurate specifications caused by
implicit preconditions, assertions in loops,
assertions in transitively called methods,
or tested methods called within loops.
\VariantNaive{} and \VariantImproved{} variants
may be excluded to avoid Java's "code too large" compilation error
when specifications contain numerous complex constraints
that approach the 64KB method bytecode limit.
\VariantNaive{} variants show higher failure rates
due to \texttt{TooManyFilterMissesException}s,
occurring when \ToolJqwik{} cannot generate
inputs satisfying path constraints within its retry limit.
This reflects the fundamental inefficiency of filtering-based input generation
for complex constraints.
\VariantImproved{} variants reduce overall failure rates
through constraint-aware input generation by encoding simple equality and inequality constraints
during input generation,
but complex constraints involving arithmetic expressions must still be handled through filtering.
Failures from inaccurate specifications remain similar
across both variant types because specification extraction logic is shared.

\begin{algorithm}[t]
\caption{Constraint-Aware Input Generation (\VariantImproved{})}
\label{alg:constraint-encoding}
\begin{algorithmic}[1]
\REQUIRE Input specification $S$, Parameters $P = \{p_1, \ldots, p_n\}$
\ENSURE Generated test inputs satisfying $S$
\STATE Assign indices: $idx(p_i) = i$ for $i \in \{1, \ldots, n\}$
\FORALL{constraints $c \in S$ of form $p_i \odot p_j$ where $\odot \in \{=, <, \leq, >, \geq\}$}
    \IF{$idx(p_i) > idx(p_j)$}
        \STATE Add constraint to $p_i$ based on $p_j$
    \ENDIF
    \IF{$idx(p_j) > idx(p_i)$}
        \STATE Rewrite constraint and add to $p_j$ based on $p_i$
        \STATE E.g., $p_i < p_j$ becomes $p_j > p_i$
    \ENDIF
\ENDFOR
\FOR{each parameter $p_i$ in index order}
    \STATE $E_i \gets$ equality constraints for $p_i$
    \STATE $L_i \gets$ lower bound constraints for $p_i$  
    \STATE $U_i \gets$ upper bound constraints for $p_i$
    \IF{$E_i \neq \emptyset$}
        \STATE Generate $p_i = $ value from first equality constraint
    \ELSIF{$L_i \neq \emptyset$ or $U_i \neq \emptyset$}
        \STATE $lower \gets \max(L_i)$ if exists, else type minimum
        \STATE $upper \gets \min(U_i)$ if exists, else type maximum
        \STATE Generate $p_i \in [lower, upper]$
    \ELSE
        \STATE Generate $p_i$ randomly within type bounds
    \ENDIF
\ENDFOR
\STATE Apply filter for non-encodable constraints
\STATE \RETURN generated inputs if filter passes
\end{algorithmic}
\end{algorithm}

\subsection{Test Suite Reduction}
\label{sec:test-suite-reduction}

The test suite reduction stage uses mutation testing to identify and retain
only generalized tests that improve fault detection capability
beyond what the original test suite provides.
Since generalized tests exercise more inputs than their original counterparts,
they may detect additional mutants that the original test suite missed.
However, not all generalized tests provide this improvement:
some may only detect mutants already caught by other tests in the suite.

\ToolTeralizer{} uses \ToolPit{} to execute mutation testing
on three test suite variants:
\VariantOriginal{} (the original test suite),
\VariantInitial{} (the subset with successful specification extraction),
and the generalized test variants (\VariantBaseline{}, \VariantNaive{}, \VariantImproved{}).
By comparing which mutants each suite detects,
we identify generalized tests that catch mutants not detected by \VariantOriginal{}.

The selection criterion is straightforward:
retain only generalized tests that detect mutants
not caught by the original test suite.
This ensures that every generalized test in the final suite
contributes unique fault detection capability.
Generalized tests that only detect mutants already caught by existing tests
are excluded as redundant.

The final test suite combines the retained generalized tests
with the original test suite,
providing both the original developer-validated test cases
and the additional fault detection capability from generalization.
This approach maintains backward compatibility
while strengthening the test suite's ability to detect regressions
within the execution paths already covered by the original tests.


