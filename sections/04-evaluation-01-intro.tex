\section{Evaluation}
\label{sec:evaluation}

Test generalization strengthens existing test suites by exploring additional inputs
within already-covered execution paths. \ToolTeralizer{} automates this transformation from
conventional unit tests to property-based tests. It thereby eliminates the manual effort
that is traditionally required to implement such a transformation. To assess the results that
\ToolTeralizer{} achieves, we examine its benefits and limitations through four research questions:

\begin{itemize}
  \item \textbf{RQ1:} To which degree does generalization affect the mutation score of the target test suites?
  \item \textbf{RQ2:} To which degree does generalization affect the size and runtime of the target test suites?
  \item \textbf{RQ3:} What are the runtime requirements of the generalization approach?
  \item \textbf{RQ4:} What are the causes of unsuccessful generalization attempts?
\end{itemize}

These questions progress from measuring direct effects to understanding practical constraints.
RQ1 establishes effectiveness through mutation score improvements.
RQ2 quantifies associated side effects in terms of increased test suite size and execution time.
RQ3 examines the runtime costs of the approach and compares its effiency to test generation via \ToolEvoSuite.
RQ4 analyzes failure cases to identify current limitations and guide future improvements.

Our evaluation uses a three-part dataset (Section~\ref{sec:target-programs}) ranging from
controlled benchmarks to real-world projects. The evaluation environment (Section~\ref{sec:evaluation-setup})
compares original unit tests and multiple generalized test variants to isolate
generalization effects. Results (Sections~\ref{sec:primary-effects-eval}--\ref{sec:filtering-eval-extended})
demonstrate consistent mutation score improvements in two of the three parts of the dataset,
quantify resource trade-offs, and reveal specific boundaries of the automated approach.
