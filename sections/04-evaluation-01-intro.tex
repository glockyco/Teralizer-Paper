\section{Evaluation}
\label{sec:evaluation}

Test generalization strengthens the mutation scores of existing test suites
by exploring additional inputs within already-covered execution paths.
Our approach automates the transformation from
conventional unit tests to property-based tests, reducing the manual effort
required to perform such a transformation.
We empirically evaluate the potential and limitations of semantics-based test generalization
through five research questions:

\begin{itemize}
  \item \textbf{RQ1:} How much does test generalization improve mutation detection?
  \item \textbf{RQ2:} How does constraint complexity affect constraint-aware versus random input generation?
  \item \textbf{RQ3:} To which degree does generalization affect the size and runtime of the target test suites?
  \item \textbf{RQ4:} How efficient is test generalization compared to extended test generation?
  \item \textbf{RQ5:} What are the causes of unsuccessful generalization attempts?
\end{itemize}

These questions progress from measuring direct effects to understanding practical constraints.
RQ1 establishes effectiveness through mutation score improvements.
RQ2 explores how constraint complexity affects mutation score improvements achieved by the \VariantNaive{} and \VariantImproved{} generation variants.
RQ3 quantifies effects on test suite size and execution time.
RQ4 examines the runtime costs and efficiency of test generalization compared to extended test generation via \ToolEvoSuite{}.
RQ5 analyzes failure cases to identify current limitations and guide future improvements.
Our experimental setup (Section~\ref{sec:experimental-framework}) establishes the evaluation methodology
and our evaluation results (Sections~\ref{sec:primary-effects-eval}--\ref{sec:limitations-eval})
provide the necessary data to answer RQ1-RQ5.
All experiments were run on a MacBook Air
with M2 processor and 24~GB of memory using default JVM settings.
All collected data is available in our replication package~\cite{replicationpackage}.
