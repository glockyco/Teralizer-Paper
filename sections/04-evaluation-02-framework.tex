\subsection{Experimental Setup}
\label{sec:experimental-framework}

To identify current capabilities and limitations
of semantics-based test generalization,
we systematically evaluate our implementation of \ToolTeralizer{}
across a multitude of projects which range from
controlled benchmark cases that are well-suited for test generalization
to real-world projects that demonstrate which future advances are needed
to improve practical applicability of test generalization tools.
In this section, we describe key components of our experimental setup
and establish a shared vocabulary
that we use throughout the evaluation
to refer to different stages of the processing pipeline,
different test suite variants,
and different groups of projects
that are part of our evaluation dataset.

\subsubsection{Processing Stages and Test Suite Variants}

As shown in Figure~\ref{fig:approach-overview},
\ToolTeralizer{}'s processing pipeline consists of five stages.
The first three
(test and assertion analysis, tested method identification, and specification extraction)
are \VariantShared{} stages that are only executed once per pipeline run
because their results can be reused across generalization strategies
(\VariantBaseline{}, \VariantNaive{}, and \VariantImproved{}).
Processing starts with all \VariantOriginal{} tests.
However, each stage can exclude tests
that are unsuitable for further processing.
We refer to the subset of \VariantOriginal{} tests that remain after
the \VariantShared{} processing stages
as the \VariantInitial{} test suite.
The last two processing stages
(generalized test creation and test suite reduction)
are executed once for the \VariantBaseline{} generalization strategy
and three times each for the \VariantNaive{} and \VariantImproved{} strategies
using different values for \ToolJqwik{}'s \tries{} setting.
Thus, we distinguish the following nine test suite variants:

\begin{itemize}
  \item \VariantOriginal{}: before any processing has taken place,
  \item \VariantInitial{}: after exclusions by \VariantShared{} processing stages,
  \item \VariantBaseline{}: after \VariantBaseline{} generalization and reduction,
  \item \VariantNaiveA{}, \VariantNaiveB{}, \VariantNaiveC{}: after \VariantNaive{} generalization and reduction (10/50/200~\tries{}),
  \item \VariantImprovedA{}, \VariantImprovedB{}, \VariantImprovedC{}: after \VariantImproved{} generalization and reduction (10/50/200~\tries{}).
\end{itemize}

As described in Section~\ref{sec:three-variant-design},
using \VariantInitial{} as a shared starting point
not only reduces the runtime costs of the evaluation
but also enables a fair comparison across strategies
by avoiding non-deterministic Stage 1--3 failures such as \texttt{OutOfMemoryError}s
from affecting one strategy more strongly than another.
The used \tries{} settings of 10, 50, and 200 were selected
to demonstrate the scaling behavior of higher \tries{}
%(both in terms of mutation score improvements as well as in terms of runtime costs)
while keeping runtime costs manageable.

\subsubsection{Evaluated Projects}

Our evaluation employs three complementary datasets that progressively
reveal the gap between controlled and real-world conditions for test generalization.
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench}
provides numeric-focused programs
that are well-suited for symbolic analysis.
Utility methods extracted from Apache Commons projects
bridge toward real-world complexity while maintaining the focus on numeric constraints.
Projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
expose real-world applicability challenges.
Table~\ref{tab:dataset-statistics} provides descriptive statistics of the datasets.
For the implementation, we show the
number of files, classes, and source lines of code (SLOC).
For tests, we additionally provide the number of test methods,
i.e., methods that are annotated with \texttt{@Test}, \texttt{@RepeatedTest}, or \texttt{@ParameterizedTest}.

\textit{EqBench.}
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench}
(rows \DatasetsEqBenchEs{} in Table~\ref{tab:dataset-statistics})
provides controlled conditions
for automated test generalization.
Originally designed for equivalence checking research,
its 652 Java classes implement equivalent and non-equivalent program pairs
focusing on numeric computations while deliberately avoiding
features that complicate automated reasoning (e.g., recursion, reflection, and complex object graphs).
Since \DatasetEqBench{} provides only implementation code without tests,
we generated test suites using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}
with three different search budgets (1s, 10s, and 60s per implementation class),
creating the dataset variants \DatasetEqBenchA{}, \DatasetEqBenchB{}, and \DatasetEqBenchC{}.
This design explores how initial test suite quality affects generalization effectiveness:
stronger initial suites offer better test diversity but less improvement potential
due to their higher initial mutation scores.

\textit{Apache Commons.}
To bridge toward real-world complexity,
we extracted numeric utility methods from Apache Commons projects
(rows \DatasetsCommons{} in Table~\ref{tab:dataset-statistics}).
Using Sourcegraph's code search~\cite{sourcegraph},
we identified public static methods with numeric or boolean parameters and return values ---
the types currently supported by \ToolTeralizer{}
(search queries are available in our replication package~\cite{replicationpackage}).
This yielded 247 classes from 17 Apache Commons projects
(commons-math, commons-numbers, commons-lang, etc.),
including all transitively called methods and dependencies
to ensure compilation (19,709 LOC total).
From this, we created four dataset variants:
\DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}
use \ToolEvoSuite{}-generated tests
with 1s, 10s, and 60s per-class search budgets.
In contrast, \DatasetCommonsDev{} preserves the 725 original developer-written tests (14,389 LOC).
This enables direct comparison of generalization effectiveness between
developer-written tests and tests generated by \ToolEvoSuite{}.

\textit{RepoReapers.}
To understand current limitations in practical settings,
we selected 632 projects from RepoReapers~\cite{munaiah_2017_reporeapers},
a curated collection of 1.9 million GitHub repositories
specifically filtered for their use of sound software engineering practices
(e.g., extensive development history,
use of software testing and issue tracking,
availability of documentation).
Our selection criteria balanced technical constraints with evaluation goals.
All selected projects target Java 5--8 (for \ToolSPF{} compatibility),
use JUnit 4 or 5 through Maven (for \ToolTeralizer{} compatibility),
have standard directory structures (for automated processing),
medium-sized codebases (5,000--50,000 LOC),
and substantial test suites (20--80\% of total code).
The selected projects collectively comprise 50,474 implementation classes 
and 30,894 test classes across diverse domains and coding styles.
While \ToolTeralizer{} succeeds on \DatasetEqBench{} and partially on Apache Commons
(RQ1--RQ5, Sections~\ref{sec:primary-effects-eval}--\ref{sec:limitations-eval}),
the RepoReapers projects expose current barriers to practical applicability
(RQ6, Section~\ref{sec:limitations-eval-extended}).

\input{tables/tab-dataset-statistics}
