\subsection{Experimental Setup}
\label{sec:experimental-framework}

To identify current capabilities and limitations
of semantics-based test generalization,
we systematically evaluate our implementation of \ToolTeralizer{}
across a multitude of projects which range from
controlled benchmark cases that are well-suited for test generalization
to real-world projects that demonstrate which future advances are needed
to improve practical applicability of test generalization tools.
In this section, we describe key components of our experimental setup
and establish a shared vocabulary
that we use throughout the evaluation
to refer to different stages of the processing pipeline,
different test suite variants,
and different groups of projects
that are part of our evaluation dataset.

\subsubsection{Processing Stages and Test Suite Variants}

As previously shown in Figure~\ref{fig:approach-overview},
\ToolTeralizer{}'s processing pipeline consists of five processing stages.
The first three of these stages
(i.e., test and assertion analysis, tested method identification, and specification extraction)
are \VariantShared{} stages that are only executed once per pipeline run
because their results can be reused by all three generalization strategies
(i.e., \VariantBaseline{}, \VariantNaive{}, and \VariantImproved{}).
Processing starts with all tests from the \VariantOriginal{} test suite.
However, each pipeline stage can exclude tests that it 
determines to be unsuitable for further processing.
We refer to the subset of \VariantOriginal{} tests that remain after
the three \VariantShared{} processing stages
as the \VariantInitial{} test suite.
The last two processing stages
(i.e., generalized test creation and test suite reduction)
are executed once for the \VariantBaseline{} generalization strategy
and three times each for the \VariantNaive{} and \VariantImproved{} strategies
using different values for \ToolJqwik{}'s \tries{} setting.
Thus, we distinguish the following nine test suite variants:

\begin{itemize}
  \item \VariantOriginal{}: before any processing has taken place,
  \item \VariantInitial{}: after exclusions by \VariantShared{} processing stages,
  \item \VariantBaseline{}: after \VariantBaseline{} generalization and reduction,
  \item \VariantNaiveA{}, \VariantNaiveB{}, \VariantNaiveC{}: after \VariantNaive{} generalization and reduction (10/50/200~\tries{}),
  \item \VariantImprovedA{}, \VariantImprovedB{}, \VariantImprovedC{}: after \VariantImproved{} generalization and reduction (10/50/200~\tries{}).
\end{itemize}

As previously described in Section~\ref{sec:three-variant-design},
using \VariantInitial{} as a shared starting point for all three generalization strategies
not only reduces the runtime costs of the evaluation
but also enables a fair comparison across strategies.
This is because it avoids non-deterministic Stage 1--3 failures such as \texttt{OutOfMemoryError}s
from affecting one generalized test suite variant more strongly than another.
The used \tries{} settings of 10, 50, and 200 were selected
to demonstrate the scaling behavior of higher \tries{} settings
(both in terms of mutation score improvements as well as in terms of runtime costs)
while keeping overall runtime costs of the evaluation manageable.

\subsubsection{Evaluated Projects}

Our evaluation employs three complementary datasets that progressively
reveal the gap between controlled and real-world conditions for test generalization.
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench}
provides numeric-focused programs
that are well-suited for symbolic analysis.
Utility methods extracted from Apache Commons projects
bridge toward real-world complexity while maintaining the focus on numeric constraints.
Real-world projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers}
expose the full spectrum of applicability challenges.
Table~\ref{tab:dataset-statistics} provides descriptive statistics of these datasets
through different project size metrics. For the implementation code, we show the
number of files, classes, and source lines of code (SLOC).
For the test code, we additionally provide the number of test methods,
i.e., methods that are annotated with \texttt{@Test}, \texttt{@RepeatedTest}, or \texttt{@ParameterizedTest}.
The following paragraphs provide further details
about the construction and contents of the three datasets and their variants.

\paragraph{EqBench}
The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench}
(rows \DatasetsEqBenchEs{} in Table~\ref{tab:dataset-statistics})
provides controlled conditions
for automated test generalization.
Originally designed for equivalence checking research,
its 652 Java classes implement equivalent and non-equivalent program pairs
focusing on numeric computations while deliberately avoiding
features that complicate automated reasoning (e.g., recursion, reflection, and complex object graphs).
Since \DatasetEqBench{} provides only implementation code without tests,
we generated test suites using \ToolEvoSuite{}~\cite{fraser_2011_evosuite}
with three different search budgets (1s, 10s, and 60s per implementation class),
creating the dataset variants \DatasetEqBenchA{}, \DatasetEqBenchB{}, and \DatasetEqBenchC{}.
This design explores how initial test suite quality affects generalization effectiveness:
stronger initial suites offer better test diversity but less improvement potential
due to their already-high mutation scores.

\paragraph{Apache Commons}
To bridge toward real-world complexity,
we systematically extracted numeric utility methods from Apache Commons projects
(rows \DatasetsCommons{} in Table~\ref{tab:dataset-statistics}).
Using Sourcegraph's code search\footnote{\url{https://sourcegraph.com/search}},
we identified public static methods with numeric or boolean parameters and return values ---
the types currently supported by \ToolTeralizer{}'s specification extraction
(search queries are available in our replication package~\cite{replicationpackage}).
This yielded 247 classes from 106 source files,
including all transitively called methods and dependencies
to ensure compilation (19,709 LOC total).
From this baseline, we created four dataset variants:
\DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}
use \ToolEvoSuite{}-generated tests
with 1s, 10s, and 60s per-class search budgets respectively.
In contrast, \DatasetCommonsDev{} preserves the 725 original developer-written tests (14,389 LOC).
This enables direct comparison of generalization effectiveness between
mature developer-written tests and automatically created tests generated by \ToolEvoSuite{}.

\paragraph{RepoReapers}
To understand current limitations in practical settings,
we selected 632 projects from RepoReapers~\cite{munaiah_2017_reporeapers},
a curated collection of 1.9 million GitHub repositories
specifically filtered for their use of sound software engineering practices
(e.g., extensive development history,
use of software testing and issue tracking,
availability of documentation).
Our selection criteria balanced technical constraints with evaluation goals.
All selected projects target Java 5--8 (for \ToolSPF{} compatibility),
use JUnit 4 or 5 through Maven (for \ToolTeralizer{} compatibility),
have standard directory structures (for automated processing),
medium-sized codebases (5,000--50,000 LOC),
and substantial test suites (20--80\% of total code).
The selected projects collectively comprise 50,474 implementation classes 
and 30,894 test classes across diverse domains and coding styles.
While \ToolTeralizer{} succeeds on \DatasetEqBench{} and partially on Apache Commons (RQ1--RQ5, Section~\ref{sec:primary-effects-eval}--\ref{sec:limitations-eval}),
the RepoReapers projects expose current barriers to practical applicability
(RQ6, Section~\ref{sec:limitations-eval-extended}).

\input{tables/tab-dataset-statistics}
