\subsection{Target Programs}
\label{sec:target-programs}

Evaluating automated test generalization requires a carefully designed
dataset that balances theoretical tractability with practical relevance.
Our approach demands projects that
(i)~provide source code for both implementation and tests,
(ii)~remain processable by SPF (limiting us to Java 8 and earlier),
(iii)~use JUnit 4 or 5 as their testing framework,
(iv)~employ Maven or Gradle for build management,
and (v)~contain methods with numeric inputs and outputs suitable for specification extraction.

Existing benchmarks such as Defects4J~\cite{TODO}, Bears~\cite{TODO}, and Bugs.jar~\cite{TODO} primarily target fault localization and program
repair research. While these benchmarks include real-world projects from Apache Commons, Google Gson, and other
major libraries, their focus on complex, production-grade code presents challenges for automated generalization.
More specifically, these libraries employ sophisticated implementation patterns such as deep inheritance hierarchies, extensive polymorphism, and
reflection that complicate specification extraction. Moreover, their mature test suites already achieve high coverage, limiting
opportunities to demonstrate improvement through generalization.

We therefore constructed a new evaluation dataset that combines projects from three complementary sources: the \DatasetEqBench{} benchmark~\cite{TODO}
representing ideal conditions for specification extraction, utility methods extracted from TODO Apache Commons projects to bridge
toward real-world complexity, and 1,160 projects from the RepoReapers dataset~\cite{TODO} that reveal current limitations in
practical settings. This design enables us to evaluate both where our approach succeeds and, equally importantly, where
its current boundaries are.

\subsubsection{EqBench}
\label{sec:eqbench}

The \DatasetEqBench{} benchmark~\cite{badihi_2021_eqbench} represents
an ideal evaluation target for automated test generalization.
Originally designed to evaluate equivalence checking tools,
the benchmark contains 652 Java classes implementing (non-)equivalent program pairs.
These programs focus on numeric computations while deliberately avoiding
features that complicate automated reasoning such as recursion, reflection, and complex object graphs.
This design philosophy aligns well with \ToolTeralizer{}'s current capabilities,
making \DatasetEqBench{} programs natural candidates for demonstrating
what automated test generalization can achieve under favorable conditions.

Since \DatasetEqBench{} provides only implementation code,
we generated test suites using \ToolEvoSuite{},
which has demonstrated consistent effectiveness in test generation competitions,
winning SBST 2021 and 2022~\cite{panichella_2022_sbst, panichella_2023_sbst}
and achieving a top placement in SBFT 2023~\cite{grano_2023_sbft}.
To explore how generalization performs across different test suite qualities,
we created three test suite variants using different \ToolEvoSuite{} search budgets:
1 second per class, 10 seconds, and 60 seconds (the \ToolEvoSuite{} default).
These variants, labeled \DatasetEqBenchA{}, \DatasetEqBenchB{}, and \DatasetEqBenchC{} respectively,
allow us to examine whether stronger initial test suites improve generalization outcomes
despite offering fewer remaining opportunities for mutation score improvement through generalization.

% tests generated with EvoSuite;
% came "third" in SBFT Tool Competition 2025 - Java Test Case Generation Track (https://arxiv.org/pdf/2504.09168), but differences between top 3 not statistically significant per the paper;
% can't find a summary paper for SBFT 2024;
% performed best on the SBFT Tool Competition 2023 - Java Test Case Generation Track for line and branch coverage metrics, and second-best for understandability metric;
% also the overall winner for SBST Tool Competition 2022 and SBST Tool Competition 2021 which use line + branch (via JaCoCo) and mutation coverage (via PIT) as metrics;

% 3 different test suites with search budgets: 1s, 10s, 60s (= EvoSuite default);
% no other changes to default EvoSuite configuration

\subsubsection{Apache Commons Utils}
\label{sec:apache-commons-utils}

Real-world libraries present different challenges than purpose-built benchmarks.
To explore this dimension, we extracted numeric utility methods from Apache Commons projects,
creating a bridge between ideal and practical evaluation scenarios.
Using Sourcegraph's code search capabilities\footnote{\url{https://sourcegraph.com/search}},
we systematically identified public static methods with numeric or boolean parameters and return values
across the Apache Commons ecosystem
(the exact search queries we used are available in our replication package~\cite{replicationpackage}).
This search yielded 247 classes from 106 source files spread across TODO Apache Commons projects,
representing the subset of production code amenable to our current generalization approach.

Beyond the matching utility methods themselves,
we included all transitively called methods as well as necessary dependencies to ensure compilation.
Furthermore, we identified and included all tests from the original test suites that cover the extracted utility methods.
This resulted in a self-contained project totaling 19,709 lines of implementation code
tested by 725 developer-written test methods with a total size of 14,389 lines of test code.

To enable systematic comparison
of developer written tests and automatically generated ones,
we created four project variants from this baseline project:
\DatasetCommonsDev{} preserves the original developer tests,
while \DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}
use \ToolEvoSuite{}-generated tests with 1, 10, and 60 second budgets respectively.

\subsubsection{RepoReapers}
\label{sec:additional-oss-projects}

Understanding where our current approach fails proves as valuable as demonstrating where it succeeds.
To explore current boundaries systematically,
we selected 1,160 projects from the RepoReapers dataset~\cite{munaiah_2017_reporeapers},
a curated collection of 2.9 million GitHub repositories
specifically designed for software engineering research.
The RepoReapers methodology filters projects through quality indicators
including documentation, testing practices, and development activity,
ensuring meaningful evaluation targets.

Our selection criteria balanced technical constraints with evaluation goals.
Projects needed to use Java 1.5--1.8 (for \ToolSPF{} compatibility),
employ JUnit 4/5 with Maven builds (for compatibility with our generalization prototype),
and follow standard directory structures
(to enable large-scale automated processing without manual special-casing).
We focused on mid-sized projects (5,000--50,000 lines of code)
with substantial test suites (20--80\% of total code)
to ensure sufficient complexity while maintaining tractable analysis.
The 1,160 selected projects collectively comprise 95,404 implementation classes
and 53,039 test classes, representing diverse domains and coding styles.

The RepoReapers results are insightful precisely through the generalization failures they reveal.
While \ToolTeralizer{} succeeds on \DatasetEqBench{} and partially on Apache Commons utilities,
it encounters fundamental obstacles in these real-world projects.
We therefore reserve detailed analysis of RepoReapers for RQ4 (Section~\ref{sec:filtering-eval-extended}),
where we highlight the causes of observed failures to provide guidance towards
promising directions for future research and engineering work.

\subsubsection{Dataset Statistics}
\label{sec:dataset-statistics}

Table~\ref{tab:dataset-statistics} presents descriptive statistics of the complete dataset,
revealing important patterns in project structure and test suite composition across the different projects.

\input{tables/tab-dataset-statistics}

The \DatasetEqBench{} projects demonstrate remarkable consistency in their generated test suites.
Despite test generation budgets varying from 1 to 60 seconds per class,
all three projects produce similar test suite sizes:
4,718--4,974 test methods across 35,666--37,836 lines of test code.
While these metrics alone do not determine test suite effectiveness,
the observed plateauing effect suggests that \ToolEvoSuite{} quickly achieves coverage saturation
even under strict time controls, with additional search time yielding minimal test additions.

The Apache Commons projects show a strong contrast between developer-written and generated tests.
In particular, the developer-written test suite (\DatasetCommonsDev{})
has significantly fewer test methods (725)
but similar lines of code (14,389)
compared to generated test suites (2,481--2,738 methods across 17,524--19,082 lines).
This 3.5$\times$ difference in test count despite similar line counts
highlights fundamentally different testing philosophies.
Developer tests exercise multiple scenarios within single test methods,
checking various edge cases and input combinations together.
Generated tests, conversely, follow a one-scenario-per-test approach,
resulting in more numerous but simpler test methods.

The \DatasetRepoReapers{} statistics underscore the diversity of real-world projects.
With a median of 57 implementation classes but a mean of 82,
and median test code comprising 2,023 lines versus a mean of 3,305,
the distribution shows most projects cluster on the smaller side,
with larger outliers pulling the mean upward.
Despite the size variation, generalization failures occur consistently
across most included projects regardless of their scale.
This indicates that the generalization challenges we encountered
are more fundamental rather than purely size-dependent,
as we analyze in more detail in RQ4 (Section~\ref{sec:filtering-eval-extended}).
