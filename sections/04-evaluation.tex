\section{Evaluation}
\label{sec:evaluation}

To evaluate the primary effects, secondary effects, runtime requirements, and current
limitations of our generalization approach, we investigate the following research questions:

\begin{itemize}
  \item RQ1: To which degree does generalization affect the mutation score of the target test suites?
  \item RQ2: To which degree does generalization affect the size and runtime of the target test suites?
  \item RQ3: What are the runtime requirements of the generalization approach?
  \item RQ4: What are the causes of unsuccessful generalization attempts?
\end{itemize}

Throughout the remainder of this section, we first describe the target programs used for
the evaluation in Section~\ref{sec:target-programs}. In Section~\ref{sec:evaluation-setup},
we provide further information about the used evaluation setup, including the used hardware,
relevant pre-/processing steps, and used configuration settings of Teralizer.
Sections~\ref{sec:primary-effects-eval}--\ref{sec:filtering-eval-extended}
describe the results of the evaluation, answering RQ1--RQ4.

\subsection{Target Programs}
\label{sec:target-programs}

Evaluation of the test generalization approach requires a dataset consisting of projects for which
(i) the source code of the implementation and corresponding tests is available.
Additionally, as discussed throughout Section~\ref{sec:approach}, included projects
(ii) need to be processable by SPF (which does not support newer Java versions than Java 8),
(iii) need to use JUnit 4 or JUnit 5 as a testing framework,
(iv) need to use Maven or Gradle as a build tool,
(v) should use (partially) numeric inputs and numeric outputs for methods targeted by the generalization
(because Teralizer currently only supports generalization of numeric inputs and outputs,
as discussed in Section~\ref{sec:specification-extraction}).

While various publicly available benchmarks exist that fulfill requirements i--iv,
these benchmarks are often based on code from popular libraries such as Apache Commons and Google Gson.
For example, ... (list + describe some benchmarks that support this claim).
Because these libraries need to be useable in different application scenarios
and maintainable by a large number of simultaneous contributors,
they tend to make heavy use of encapsulation, inheritance, polymorphism, reflection, etc.
As a result, such projects are largely incompatible
with the basic specification extraction approach used by Teralizer,
as most implementation code does not fulfill requirement (v).
Furthermore, due to their widespread use, active development communities, and long-running nature,
these libraries are often more thoroughly tested than other projects are.
Consequently, evaluating any test generalization approach on them
would provide limited opportunity to improve the effectiveness of the test suite.

To achieve a thorough evaluation of our approach
that provides generalizable results for both its strengths and weaknesses
in the presence of the requirements and considerations discussed above,
we built our own evaluation dataset.
This dataset uses projects from three different sources:
the \DatasetEqBench{} Benchmark \cite{badihi_2021_eqbench},
utility classes of {TODO: number} Apache Commons projects, and
{TODO: number} open source projects from the RepoReapers dataset \cite{munaiah_2017_reporeapers}.
Further details about the included projects
are provided in the following Sections~\ref{sec:eqbench}--\ref{sec:additional-oss-projects}.
Descriptive statistics are available in Section~\ref{sec:dataset-statistics}.

\subsubsection{EqBench}
\label{sec:eqbench}

To represent projects that are well-suited for processing by \ToolTeralizer{},
we include the \DatasetEqBench{} benchmark as the first part of our evaluation dataset.
The \DatasetEqBench{} benchmark is a dataset built by \citeauthor{badihi_2021_eqbench}
to evaluate the effectiveness of equivalence checking tools \cite{badihi_2021_eqbench}.
The dataset contains Java (and C) implementations of non-equivalent program pairs.
Since many equivalence checking tools such as ... \cite{TODO} and ... \cite{TODO}
rely (partially) on symbolic execution for their analysis,
\DatasetEqBench{} is well suited for processing with \ToolSPFLong{}-based tools such as \ToolTeralizer{}.
The dataset achieves this by focusing primarily on programs that process numeric inputs,
and by avoiding use of language features such as recursion and reflection
which contemporary equivalence checking approaches only offer limited support for \cite{TODO}.
Because the dataset only contains implementation code, but does not contain test code,
we used \ToolEvoSuite{} to generate test suites
that can be used as input for the generalization.
We chose \ToolEvoSuite{} for test generation
because it consistently achieved top placements in recent test generation tool competitions \cite{TODO}.
For the test generation, we used three different timeout settings
for \ToolEvoSuite{} to approximate different strengths of test suites:
60 seconds per class (\ToolEvoSuite{}'s default), 10 seconds per class, and 1 second per class.
All other configuration settings were left at \ToolEvoSuite{}'s default values. We refer to
the corresponding projects consisting of \DatasetEqBench{} implementation code and
\ToolEvoSuite{} generated test code as \textit{\DatasetEqBenchA{}}, \textit{\DatasetEqBenchB{}}, and \textit{\DatasetEqBenchC{}}.
Descriptive statistics for these projects are provided in Section~\ref{sec:dataset-statistics}.

% tests generated with EvoSuite;
% came "third" in SBFT Tool Competition 2025 - Java Test Case Generation Track (https://arxiv.org/pdf/2504.09168), but differences between top 3 not statistically significant per the paper;
% can't find a summary paper for SBFT 2024;
% performed best on the SBFT Tool Competition 2023 - Java Test Case Generation Track for line and branch coverage metrics, and second-best for understandability metric;
% also the overall winner for SBST Tool Competition 2022 and SBST Tool Competition 2021 which use line + branch (via JaCoCo) and mutation coverage (via PIT) as metrics;

% 3 different test suites with search budgets: 1s, 10s, 60s (= EvoSuite default);
% no other changes to default EvoSuite configuration

\subsubsection{Apache Commons Utils}
\label{sec:apache-commons-utils}

To have some representation of large open source libraries in our evaluation dataset,
we extracted source code and corresponding test code of utility methods from \{number\} Apache Commons projects.
We identified appropriate utility methods via Sourcegraph's RegEx-based repository search\footnote{\url{https://sourcegraph.com/search}}.
More specifically, we searched for public static methods
with at least one numeric or boolean input parameter
and a numeric or boolean output value.
The exact search query we used is available in our replication package~\cite{replicationpackage}.
To construct the dataset from the Sourcegraph search results,
we manually created a new project and added to it:
(i) all matching utility methods,
(ii) all methods that are (transitively) called by the utility methods,
(iii) all tests that cover the utility methods, and
(iv) all dependencies that are needed to compile the included code.
Beyond the original test suite consisting of test classes extracted from Apache Commons projects,
we also generated three additional test suites via \ToolEvoSuite{}
using the same settings that we used for the \DatasetEqBench{} test generation.
Thus, four variants of the created project are included in the evaluation dataset:
\textit{\DatasetCommonsDev{}} (which uses the original developer written test suite)
as well as \textit{\DatasetCommonsA{}}, \textit{\DatasetCommonsB{}}, and \textit{\DatasetCommonsC{}}
(which use \ToolEvoSuite{} generated test suites).

\subsubsection{RepoReapers}
\label{sec:additional-oss-projects}

To get a more representative view
of current limitations of the approach in a real-world setting,
we additionally applied Teralizer to <number> projects
from the RepoReapers dataset\cite{munaiah_2017_reporeapers}.
@TODO: Write 1-2 sentences about the RepoReapers dataset.
To select these projects from the full RepoReapers dataset,
we applied the following selection criteria: projects
(i)~have to be implemented in Java 1.5 to 1.8
(ii)~have to use JUnit 4 or 5 for testing,
(iii)~have to use Maven as a build tool,
(iv)~have to follow the expected folder structure (i.e., have implementation in src/main/java and tests in src/test/java) 
(v)~have to contain 5000-50000 LOC,
(vi)~have to contain 20\%-80\% of the total source code in test classes
(vii)~have to have a total repository size less than 100MB.
Selection criteria (i) is imposed by the \ToolSPF{} dependency of \ToolTeralizer{}.
Criteria (ii) to (iv) are current limitations of the implemented prototype
(as discussed in Section~\ref{sec:approach}),
but could be lifted with additional engineering effort.
The remaining selection criteria (v) to (vii) were applied
to keep the overall size of the dataset to a manageable amount
while focusing on a subset that has
a good balance between implementation code and test code.
We refer to this dataset as \textit{\DatasetRepoReapers{}} throughout the rest of this paper.
Note, however, that \DatasetRepoReapers{} is only discussed in detail in RQ4,
which covers current limitations of the prototype
(see Section~\ref{sec:filtering-eval-extended}),
but not throughout RQ1--RQ3.
This is because the current implementation of \ToolTeralizer{}
is largely unsuccessful at generalizing tests from these projects.

\subsubsection{Dataset Statistics}
\label{sec:dataset-statistics}

Table~\ref{tab:dataset-statistics} provides descriptive statistics
of the projects that are included in the dataset.
We separately list the number of files, classes, and source lines of code (SLOC)
for both the implementation code as well as the test code of the projects.
For the \DatasetRepoReapers{} projects,
we list the total, mean, and median values
across all 1160 included (sub-)projects.

@TODO: Describe the \DatasetsEqBenchEs{} numbers.

@TODO: Describe the \DatasetsCommons{} numbers.

The developer-written tests in \DatasetCommonsDev{}
often have several times as many lines of code
as the automatically generated \ToolEvoSuite{} tests
in \DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}.
This is because the developer-written tests often
test multiple sets of input values
and / or multiple target methods
in a single test method.
The \ToolEvoSuite{} generated test suites, on the other hand,
use very isolated / focused test methods
that generally cover only a single target method
using a single set of input values.
For practical examples of these differences, see Figure~\ref{fig:@TODO}.

@TODO: add a figure showing test cases in \DatasetCommonsDev{} vs. \DatasetCommonsA{} / \DatasetCommonsB{} / \DatasetCommonsC{}.

@TODO: Describe the \DatasetRepoReapers{} numbers.

descriptive statistics (evosuite runtime, number of classes, number of tests, LOC, ...)

\begin{table}[H]
  \caption{Number of files, classes, source lines of code (SLOC), and test methods per project.}
  \label{tab:dataset-statistics}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{3}{c}{Implementation} & \multicolumn{4}{c}{Test} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-8}
    Project & Files & Classes & SLOC & Files & Classes & SLOC & Methods \\
    \midrule
    \DatasetEqBenchA{} & 544 & 652 & 27,871 & 544 & 544 & 35,666 & 4,718 \\
    \DatasetEqBenchB{} & 544 & 652 & 27,871 & 543 & 543 & 36,937 & 4,875 \\
    \DatasetEqBenchC{} & 544 & 652 & 27,871 & 544 & 544 & 37,836 & 4,974 \\
    \midrule
    \DatasetCommonsA{} & 106 & 247 & 19,709 & 103 & 103 & 17,524 & 2,481 \\
    \DatasetCommonsB{} & 106 & 247 & 19,709 & 103 & 103 & 19,082 & 2,738 \\
    \DatasetCommonsC{} & 106 & 247 & 19,709 & 102 & 102 & 18,839 & 2,735 \\
    \midrule
    \DatasetCommonsDev{} & 106 & 247 & 19,709 & 80 & 119 & 14,389 & 725 \\
    \midrule
    \DatasetRepoReapers{} (total) & 79,789 & 96,589 & 5,203,516 & 40,872 & 55,038 & 3,905,071 & 167,046 \\
    \DatasetRepoReapers{} (mean) & 68 & 83 & 4,478 & 35 & 47 & 3,360 & 143 \\
    \DatasetRepoReapers{} (median) & 51 & 57 & 3,241 & 22 & 26 & 2,053 & 69 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Evaluation Setup}
\label{sec:evaluation-setup}

To collect the necessary data to answer RQ1-RQ4,
we executed \ToolTeralizer{} once for every project in the evaluation dataset
on a MacBook Air with M2 processor and 24~GB of memory
(for threats to validity resulting from this setup, see Section~\ref{sec:threats-to-validity}).
The JVM settings for \texttt{InitialHeapSize} and \texttt{MaxHeapSize}
were kept at their default values, i.e., 384~MB (${1/64}$th of memory) for \texttt{InitialialHeapSize} and 6~GB (${1/4}$th of memory) for \texttt{MaxHeapSize}.
Further details about the used execution environments and settings are provided
in the \textit{Setup} sections of the the individual research questions.
Since there are no existing approaches
that automatically create property-based tests from existing (unit) tests,
no other tools are included in the evaluation.
Instead, we compare the original,
human written and \ToolEvoSuite{} generated test suites in the evaluation dataset
to the augmented / generalized test suites created by \ToolTeralizer{}.
As described in Section~\ref{sec:approach},
\ToolTeralizer{} creates a total of nine variants
of each target project / test suite
throughout its execution
to analyze how different settings or combinations thereof
affect the generalization results.
We refer to these variants as:

\begin{itemize}
  \item \VariantOriginal{}:
    The original project without any modifications applied by \ToolTeralizer{}.
  \item \VariantInitial{}:
    The state of the project
    after project instrumentation, test analysis, and specification extraction have concluded 
    (see Sections~\ref{sec:project-instrumentation}--\ref{sec:specification-extraction})
    but before any tests have been generalized.
    Tests for which these steps were not successful
    are excluded from this project variant and,
    therefore, are not included in any further generalization steps.
    An overview of excluded tests is provided in Section~\ref{sec:filtering-eval}
    when discussing the results of RQ4.
  \item \VariantBaseline{}:
    The state of the project
    after \VariantBaseline{} generalization has been applied
    (see Section~\ref{sec:baseline-generalization}).
  \item \VariantNaiveA{}, \VariantNaiveB{}, \VariantNaiveC{}:
    The state of the project
    after \VariantNaive{} generalization has been applied.
    As described in Section~\ref{sec:naive-generalization},
    the subscript values indicate the values used for \ToolJqwik{}'s \tries{} setting.
  \item \VariantImprovedA{}, \VariantImprovedB{}, \VariantImprovedC{}:
    The state of the project
    after \VariantImproved{} generalization has been applied
    (see Section~\ref{sec:improved-generalization}).
    The subscript values indicate the values used for \ToolJqwik{}'s \tries{} setting.
\end{itemize}

In addition to the source code of the two pre-generalization variants
(\VariantOriginal{} and \VariantInitial)
as well as the seven post-generalization variants
(\VariantBaseline{},
\VariantNaive{} with 10 / 50 / 200 \tries{},
and \VariantImproved{} with 10 / 50 / 200 \tries{}),
\ToolTeralizer{} also creates and stores (meta) data and (intermediate) processing results
at various stages throughout its execution
in a PostgreSQL database and in log files.
This data includes, for example,
(i) descriptions of identified tests and assertions
(e.g., names, involved data types, lines of code),
(ii) information about executed processing tasks
(e.g., processing status, causes of failures, execution time),
and (iii) raw tool outputs
(e.g., console output logs, JUnit / \ToolJacoco{} / \ToolPit{} reports , extracted input-/output-specifications).
For a more detailed overview of the collected data, see Section~\ref{sec:collected-data}.
The evaluation results presented in the following sections
are generated from the collected data via Jupyter notebooks.
To aid independent validation and replication of our results,
all of the collected data, the full Java implementation of \ToolTeralizer{},
and the Jupyter notebooks used for the evaluation
are publicly available in our replication package~\cite{replicationpackage}.

\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

In the following, we first describe
how many tests, implementation classes, and corresponding mutants
from the evaluation dataset
are included in the evaluation
in Section~\ref{sec:included-mutants}.
Section~\ref{sec:overall-detection-rates}
focuses on the overall detection rates
achieved by the implemented generalization variants.
In Section~\ref{sec:detection-rates-per-mutator},
we provide the detection rate results for each individual mutator.
Section~\ref{sec:boundary-detection-effectiveness}
concludes the results for RQ1
by investigating the effectiveness of the partition boundary detection
used by \VariantImproved{} variants to guide input value selection.

\subsubsection{Included Mutants}
\label{sec:included-mutants}
To evaluate the effects that generalization via \ToolTeralizer{} has on mutation scores,
we compare the mutation scores reported by \ToolPit{}
for the \VariantInitial{} variants of the the target projects
to the mutation scores reported for
the \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.

The first three columns of Table~\ref{tab:mutants-per-project} show
how many of the test methods and implementation classes
of the \VariantOriginal{} projects are included in the \VariantInitial{} project variants.
Tests are counted as included
if they successfully pass test analysis (see Section~\ref{sec:test-analysis}) 
and specification extraction (see Section~\ref{sec:specification-extraction}).
Implementation classes are counted as included
if at least one line of the corresponding class
is covered by an included test
according to the \ToolJacoco{} coverage reports
of the \VariantInitial{} project variant.
For the projects with \ToolEvoSuite{} generated test suites
(i.e., \DatasetsEqBenchEs{} and \DatasetsCommonsEs{}), 
83--85 \% of test methods from \VariantOriginal{} are included in \VariantInitial{}.
For \DatasetCommonsDev{}, 63.6 \% are included.
Exclusions are primarily caused by 
(i) tests without assertions (... \%),
(ii) tests for which no tested method can be identified by \ToolTeralizer{},
and (iii) tests that exercise a tested method that has no generalizable input parameters.
Further details about the causes of test exclusions
are provided when discussing RQ4 in Section~\ref{sec:filtering-eval}.

The last three columns for Table~\ref{tab:mutants-per-project} show the number of total, covered, and uncovered mutants per project.
Total mutants are all mutants that are created by \ToolPit{} in the included classes.
Covered mutants are the subset of total mutants that are executed by at least one included test.
Uncovered mutants are the subset of total mutants that are not executed by any included tests.
@TODO: Describe actual values in the table.
@TODO: Reference the table with the per-mutant prevalence data.
%Around 8000 mutants for the apache-commons-utils projects. Around 24000 mutants for the eqbench projects.
%Total mutants for the ORIGINAL variant (i.e., before SPF execution) are around 5\% higher (not shown in this table). For filtering reasons, see Section~\ref{sec:filtering-eval}.

\begin{table}[H]
  \caption{Number of total, covered, and uncovered mutants in included classes per project.}
  \label{tab:mutants-per-project}
  \begin{tabular}{lrrrrr}
    \toprule
            & Included     & Included      & \multicolumn{3}{r}{Mutants} \\
                                             \cmidrule(lr){4-6}
    Project & Test Methods & Impl. Classes & Total & Covered & Uncovered \\
    \midrule
    \DatasetEqBenchA{} & 3937\; (83.4 \%) & 607\; (93.1 \%) & 23905 & 21492\; (89.9 \%) & 2413\; (10.1 \%) \\
    \DatasetEqBenchB{} & 4049\; (83.1 \%) & 600\; (92.0 \%) & 23654 & 21657\; (91.6 \%) & 1997\; (\phantom{0}8.4 \%) \\
    \DatasetEqBenchC{} & 4124\; (82.9 \%) & 603\; (92.5 \%) & 23663 & 22010\; (93.0 \%) & 1653\; (\phantom{0}7.0 \%) \\
    \midrule
    \DatasetCommonsA{} & 2079\; (83.8 \%) & 111\; (44.9 \%) & 8581 & 7536\; (87.8 \%) & 1045\; (12.2 \%) \\
    \DatasetCommonsB{} & 2330\; (85.1 \%) & 112\; (45.3 \%) & 8391 & 7939\; (94.6 \%) & 452\; (\phantom{0}5.4 \%) \\
    \DatasetCommonsC{} & 2326\; (85.0 \%) & 112\; (45.3 \%) & 8354 & 8109\; (97.1 \%) & 245\; (\phantom{0}2.9 \%) \\
    \midrule
    \DatasetCommonsDev{} & 461\; (63.6 \%) & 90\; (36.4 \%) & 8096 & 5215\; (64.4 \%) & 2881\; (35.6 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

% Overall observations:
\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}
Figure~\ref{fig:mutation-detection-results} shows
the percentage of detected mutants
and improvement over \VariantInitial{}
per project and generalization variant.
Detected mutants includes
killed (X\% of overall detections), timed-out (A\%), memory error (B\%), and run error (C\%),
which matches the classification used by \ToolPit{}.
Since test generalization via \ToolTeralizer{} does not affect coverage
(as described in Sections~\ref{sec:naive-generalization} and \ref{sec:improved-generalization}),
all results are relative to covered mutants
i.e., uncovered mutants are not considered in the comparison
because \ToolTeralizer{} --- by design --- cannot achieve any coverage improvements
but also does not decrease coverage.

We are using \VariantInitial{} as the baseline for mutation score comparisons
with generalized project variants,
which ensures that the only difference
across compared projects is the absence or presence of generalized test cases.
\VariantOriginal{} is not used for comparison
because it contains additional non-generalized tests
that are excluded in \VariantInitial{} as well as generalized project variants.
\VariantBaseline{} is not included in the comparison
because it always uses the same test inputs as \VariantInitial{}
(as described in Section~\ref{sec:baseline-generalization}).
Therefore, \VariantInitial{} and \VariantBaseline{} always achieve the same detection rates.

\paragraph{\VariantNaive{} Detection Rates}

\VariantNaive{} generalization improves detection rates
for \emph{all} projects included in the evaluation dataset.
Improvements are largest for the \DatasetsEqBenchEs{} projects.
Here, detection rates increase from 48.10--51.64 \% to 50.67--54.98 \%
across the different project and generalization variants.
Thus, \VariantNaive{} detects 2.33--3.93 \% more of the total mutants
than the corresponding \VariantInitial{} test suite,
which is a relative increase of 4.51--8.17 \%.
As a result, generating test suites with \ToolEvoSuite{}
and then generalizing them with \ToolTeralizer{}
often achieves higher detection rates than
increasing \ToolEvoSuite{}'s test generation timeout
from 1 second per class to 10 s or even 60 s
(for total runtime costs of test generation and generalization,
see Section~\ref{sec:runtime-eval}).
Furthermore, higher \tries{} settings generally achieve
better detection rates than lower \tries{} settings.
However, there seem to be diminishing returns:
increasing \tries{} from 10 to 50
achieves a larger improvement in detection rates
than increasing \tries{} from 50 to 200.

Results for \DatasetsCommonsEs{} projects follow similar trends,
albeit with overall smaller improvements
of 0.82--1.23 percentage points
(i.e., a relative increases of 1.43--2.17 \%)
compared to the \VariantInitial{} test suites.
For the \DatasetCommonsDev{} project,
improvements are even smaller,
increasing detection rates
by only 0.05--0.07 percentage points.
However, overall detection rates for \DatasetCommonsDev{}
are much higher than for the other projects
with an \VariantInitial{} detection rate of 80.35 \%.
Therefore, it is expected
that achievable improvements will be correspondingly smaller.
Deviations from the described trends
are primarily due to random variance
in \ToolEvoSuite{}'s test generation
and, to a lesser degree,
due to occasional random errors
(e.g., \texttt{OutOfMemoryError})
during test execution
which cause corresponding tests to be excluded from further processing
(see Sections~\ref{sec:filtering-eval} and \ref{sec:filtering-eval-extended}).

\paragraph{\VariantImproved{} Detection Rates}

The \VariantImprovedC{} generalization variant
achieves similar results as \VariantNaiveC{}
for all seven evaluated projects.
More specifically, it reaches relative improvements
of 6.04--7.82 \% for the \DatasetsEqBenchEs{} projects,
2.02--2.29 \% for \DatasetsCommonsEs{},
and 0.06 \% for \DatasetCommonsDev{}.
\VariantImprovedA{} and \VariantImprovedB{}
consistently achieve larger improvements
than the corresponding \VariantNaive{} variants
for the \DatasetsCommonsEs{} projects
(1.80--2.19 \% vs.\ 1.43--1.83 \%),
but smaller improvements
for the \DatasetsEqBenchEs{} projects
(2.36--6.88 \% vs. 4.51--8.17 \%).
However, these differences are not uniformly distributed
across different mutation operators.
As shown in Table~\ref{tab:detections-per-mutator}
and discussed in more detail in Section~\ref{sec:detection-rates-per-mutator},
\VariantNaive{} variants only outperform \VariantImproved{} ones
for the detection of \texttt{Math} mutants,
whereas all other mutants are either tied in terms of detection rates
or favor \VariantImproved{} variants.
Furthermore, \VariantImprovedA{} performs noticeably worse
than all other project-variant combinations
on the \DatasetsEqBenchEs{} projects.
This is likely because \VariantImproved{} variants
very effectively identify input partition boundaries
in the \DatasetsEqBenchEs{} projects.
As a result, most or even all \tries{} of the \VariantImprovedA{} variant
are spent on boundary testing, leaving little opportunity to detect mutants
introduced via non-boundary mutations.
For a more in-depth discussion of this, see Section~\ref{sec:boundary-detection-effectiveness}.

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}
To evaluate how capable generalized tests are
at detecting mutants created by different mutation operators,
we compare the detection rates
of \VariantNaiveC{} and \VariantImprovedC{}
to the detection rates achieved by the \VariantInitial{} test suite.
Table \ref{tab:detections-per-mutator}
shows the results of this evaluation,
listing for each mutant:
the total number of occurrences
across the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects;
the mean, minimum, and maximum prevalence per project
relative to the total number of mutants;
the detection rates of the \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} test suites,
as well as how much the two generalized variants improve detection rates relative to \VariantInitial{}.
Results for \VariantNaive{} and \VariantImproved{} generalizations with lower \tries{}
follow the same trends as for \VariantNaiveC{} and \VariantImprovedC{} but are omitted for brevity.
The full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Prevalence of Mutants}
The three most common mutants,
which represent~81.08 \% of total mutants, are
\texttt{Math}~(59.10~\% of mutants),
\texttt{Remove\-Conditional\-OrderElse}~(10.99~\%),
and \texttt{Conditionals\-Boundary}~(10.99~\%) mutants
(prevalence of \texttt{Remove\-Conditional\-Order\-Else}
and \texttt{Conditionals\-Boundary}
is the same because both mutators modify inequality checks).
On the other hand, the three least common mutants are
\texttt{Increments}~(0.52~\%),
\texttt{Boolean\-False\-Return\-Vals}~(0.24~\%),
and \texttt{Empty\-Object\-Return\-Vals}~(0.13~\%) mutants.
Due to these large differences in mutant prevalences,
overall mutation scores are much more strongly affected
by changes in, e.g., \texttt{Math} detection rates
than by changes in, e.g., \texttt{Increments} detection rates.
This holds not only on an overall, aggregate level
(i.e., across all evaluated projects),
but also for each individual project.
After all, prevalence of individual mutants
shows only comparatively small differences across projects,
as highlighted by the minimum and maximum prevalence results.
In other words,
mutants that are common in one project
are also common in the others,
whereas mutants that are uncommon in one project
are also uncommon in the others.
For example, \texttt{Math} mutants represent 52.34~\% of mutants
in the project where they are least common,
but \texttt{Increments} mutants
represent only 0.54~\% of mutants
in the project where they are most common.

\begin{table}[H]
  \caption{Number of mutants and percentage of detections per mutator.}
  \label{tab:detections-per-mutator}
  \begin{tabular}{lrrrrcrrrr}
    \toprule
    & & & & & \multicolumn{5}{c}{Detected \%} \\
    \cmidrule{6-10}
    Mutator & Total & Total \% & Min \% & Max \% & INITIAL & \multicolumn{2}{c}{NAIVE$_{200}$} & \multicolumn{2}{c}{IMPROVED$_{200}$} \\
    \midrule
    Math & 61841 & 59.10 & 52.34 & 62.16 & 50.99 & 54.98 & (+3.99) & 54.36 & (+3.37) \\
    RemoveConditionalOrderElse & 11501 & 10.99 & 8.50 & 11.94 & 61.08 & 62.29 & (+1.21) & 62.47 & (+1.39) \\
    ConditionalsBoundary & 11501 & 10.99 & 8.50 & 11.94 & 27.68 & 28.89 & (+1.21) & 30.23 & (+2.55) \\
    PrimitiveReturns & 7731 & 7.39 & 6.15 & 10.09 & 89.42 & 89.63 & (+0.20) & 89.90 & (+0.47) \\
    RemoveConditionalEqualElse & 5536 & 5.29 & 3.21 & 10.40 & 58.80 & 60.87 & (+2.07) & 61.00 & (+2.20) \\
    InvertNegs & 3122 & 2.98 & 2.94 & 3.12 & 58.91 & 60.61 & (+1.70) & 60.99 & (+2.08) \\
    VoidMethodCall & 973 & 0.93 & 0.58 & 1.35 & 24.96 & 24.96 & -- & 25.49 & (+0.53) \\
    NullReturnVals & 933 & 0.89 & 2.13 & 3.38 & 98.77 & 98.77 & -- & 98.77 & -- \\
    BooleanTrueReturnVals & 569 & 0.54 & 0.17 & 1.44 & 98.55 & 98.55 & -- & 98.55 & -- \\
    Increments & 546 & 0.52 & 0.50 & 0.54 & 72.81 & 73.38 & (+0.57) & 73.50 & (+0.69) \\
    BooleanFalseReturnVals & 250 & 0.24 & 0.09 & 0.63 & 87.87 & 87.87 & -- & 87.87 & -- \\
    EmptyObjectReturnVals & 141 & 0.13 & 0.41 & 0.43 & 90.30 & 90.30 & -- & 90.30 & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Detection Rate Results}
Detection rates for \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} as well as
detection rate improvements compared to \VariantInitial{}
are shown in the last three columns of the table.
Detection rates exhibit large differences across different mutators.
The highest detection rates of 90\%--99\% are achieved
for mutators that directly modify return values.
The lowest detection rates are observed
for the \texttt{VoidMethodCall} mutator (... \% detection rate), 
which removes calls to methods that do not produce a return value,
and the \texttt{ConditionalsBoundary} mutator (... \% detection rate),
which modifies the boundaries of conditionals by replacing
\texttt{<} with \texttt{<=} and \texttt{>} with \texttt{>=} (or vice-versa).

\VariantNaive{} achieves the largest improvements over \VariantInitial{}
for the ... (...), ... (...), and ... (...) mutations.
However, \VariantNaive{} does not improve detection rates
for mutants created by the ... mutators.
(@TODO: Possible explanations: (i) high initial detection rates, (ii) new assertions needed.)
\VariantImproved{} achieves the largest improvements
for the ... (...), ... (...), and ... (...) mutations.
It does not achieve any improvements
for mutants created by the ... mutators.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
(@TODO: Check the results for \VariantImprovedA{}. Those might be much worse for mutants that don't relate to conditionals.)
Full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Comparison of \VariantNaiveC{} and \VariantImprovedC{}}
\VariantImprovedC{} outperforms \VariantNaiveC{} in terms of detection rate
for 7 of 12 mutators,
is on par for 4 of 12 mutators (all of which achieve no improvement for either variant),
and underperforms for 1 of 12 mutators.
The largest benefit of \VariantImprovedC{} over \VariantNaiveC{} is observed
for the detection of mutants created by the \texttt{ConditionalsBound} mutator.
Here, \VariantImprovedC{} achieves an increase of ... percentage points over \VariantInitial{},
whereas \VariantNaiveC{} only increases detection rates by ... percentage points.
This suggests that the improvements implemented in \VariantImproved{} variants
to favor boundary values during input generation achieve their intended benefits.

However, \VariantImprovedC{}'s benefits for the detection rate of \texttt{ConditionalsBoundary} mutants
appear to come at the cost of decreased detection rates for \texttt{Math} mutants.
Since the only difference between the two variants is the way in which test inputs are selected
--- \VariantNaive{} variants select inputs randomly from the given input partition; 
\VariantImproved{} variants start with inputs at the edges of the partition  ---
this suggests that focusing too much on boundary values
can have detrimental effects for the detection of mutants
that are not related to input boundaries.
Due to the high prevalence of \texttt{Math} mutants,
this can also result in an \emph{overall} decrease in detection rates.

To counteract the detrimental effects on
detection rates of \texttt{Math} mutants
incurred by the input selection procedure used by \VariantImproved{} variants,
the number of \tries{} could, for example,
be increased for the \VariantImproved{} variants
to adjust for the additional \tries{} spent on boundary testing
while keeping non-boundary testing at \VariantNaive{} levels.
More sophisticated input selection approaches
that aim to better balance boundary and non-boundary testing
without large increases to the number of executed \tries{}
could perhaps further increase detection rates
while keeping the runtime impact
resulting from a higher number of \tries{} to a minimum.
For a more in-depth evaluation of runtime requirements
across different variants and numbers of \tries{}
see Section~\ref{sec:runtime-eval}.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

@TODO: Add intro. Basically: Offer more thorough explanations
why \VariantImproved{} (sometimes?) performs better than \VariantNaive{}
on the \DatasetsEqBenchEs{} projects, but not on the \DatasetsCommonsEs{} ones.

\paragraph{Evaluation Metrics}
Table~\ref{tab:mutation-detection-comparison} shows 
the model properties of mutants that are (not) detected
by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
The term \emph{model}, in this case, refers to an 
input specification extracted by \ToolTeralizer{}.
Therefore, the (input) model(s) of a mutant
are all input specifications
that describe an input partition
which contains at least one set of inputs
that cause the mutant to be exercised during test execution.
In other words, the model(s) of a mutant
are all input specifications
that were extracted by \ToolTeralizer{}'s
specification extraction step (see Section~\ref{sec:specification-extraction})
during execution of a test that covers the corresponding mutant.

Included model properties are
the median number of model operations
and the median number of model constraints.
%The number of (nested) operations is given by
%the total number of operators in the model.
%The number of constraints is determined
%by counting the number of boolean conditions or variables
%connected by \texttt{\&\&} operators.
%There can never be \texttt{||} operators in a model
%because the two operands on both sides of the \texttt{||}
%represent different input partitions and, therefore,
%describe different models.
%
In addition to the model properties,
the last column of Table~\ref{tab:mutation-detection-comparison}
lists the mean and median percentage of constraints
that are used by the created property-based tests
when generating inputs for test execution.
As described in more detail in Section~\ref{sec:improved-generalization}, 
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during \ToolPit{}'s default value filtering step.

For example, take the following (Java represented) model:
\texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains 5 operators
(i.e., \texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and \texttt{\&\&}),
so has a total of 5 (nested) operations.
Furthermore, the model's contains 3 constraints:
(i) \texttt{a < 0}, (ii) \texttt{a == (b + 1)}, and (iii) \texttt{c}.
Constraints (i) and (iii) are used by the \VariantImproved{} variants.
Constraint (ii) is not used
because it contains the compound term \texttt{b + 1}.

\begin{table}[H]
  \caption{Model properties of mutants that are (not) detected by the \VariantImprovedC{} variant.}
  \label{tab:mutation-detection-comparison}
  \begin{tabular}{lcrrrrrrr}
    \toprule
            &          &         & \multicolumn{2}{c}{Operations} & \multicolumn{2}{c}{Constraints} & \multicolumn{2}{r}{Constraints Used} \\
    \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    Project & Detected & Mutants & Mean & Median & Mean & Median & Mean & Median \\
    \midrule
    \DatasetEqBenchA{} & yes & 11145 & 147 & \phantom{0}9 & \phantom{0}6 & 2 & \phantom{0}47 \% & \phantom{0}80 \% \\
    \DatasetEqBenchA{} & no & 10347 & 224 & 16 & 11 & 5 & \phantom{0}23 \% & \phantom{0}50 \% \\
    \midrule
    \DatasetEqBenchB{} & yes & 11658 & 139 & \phantom{0}9 & \phantom{0}6 & 2 & \phantom{0}62 \% & 100 \% \\
    \DatasetEqBenchB{} & no & 9999 & 231 & 15 & \phantom{0}8 & 2 & \phantom{0}57 \% & 100 \% \\
    \midrule
    \DatasetEqBenchC{} & yes & 12052 & 137 & \phantom{0}9 & \phantom{0}5 & 2 & \phantom{0}69 \% & 100 \% \\
    \DatasetEqBenchC{} & no & 9958 & 218 & 11 & \phantom{0}6 & 2 & \phantom{0}67 \% & 100 \% \\
    \midrule
    \DatasetCommonsA{} & yes & 4390 & 290 & 15 & \phantom{0}7 & 5 & \phantom{0}43 \% & \phantom{0}84 \% \\
    \DatasetCommonsA{} & no & 3183 & 389 & 45 & 12 & 6 & \phantom{0}11 \% & \phantom{0}50 \% \\
    \midrule
    \DatasetCommonsB{} & yes & 4660 & 467 & 23 & \phantom{0}6 & 5 & \phantom{0}46 \% & \phantom{0}85 \% \\
    \DatasetCommonsB{} & no & 3309 & 507 & 46 & \phantom{0}8 & 6 & \phantom{0}10 \% & \phantom{0}56 \% \\
    \midrule
    \DatasetCommonsC{} & yes & 4821 & 374 & 20 & \phantom{0}6 & 5 & \phantom{0}47 \% & \phantom{0}85 \% \\
    \DatasetCommonsC{} & no & 3288 & 423 & 41 & 10 & 6 & \phantom{0}11 \% & \phantom{0}54 \% \\
    \midrule
    \DatasetCommonsDev{} & yes & 4193 & 107 & 11 & \phantom{0}4 & 4 & \phantom{0}25 \% & \phantom{0}75 \% \\
    \DatasetCommonsDev{} & no & 1022 & 173 & 10 & \phantom{0}4 & 4 & \phantom{0}19 \% & \phantom{0}75 \% \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Model Properties of Detected vs.\ Undetected Mutants}
As shown in Table~\ref{tab:mutation-detection-comparison},
undetected mutants often have more complex models
%(i.e., are easier to reach during testing)
than detected ones.
For example, in the \DatasetsEqBenchEs{} projects,
mean and median operation counts
are around 1.2--1.7 times larger
for undetected mutants than for detected ones
(e.g., mean of 231 vs.\ 139 and median of 15 vs.\ 9 for \DatasetEqBenchB{}).
Operation counts in the \DatasetsCommonsEs{} projects
show even more pronounced differences,
with values of undetected mutants being 1.3--3 times larger
than those of detected ones
(e.g., mean of 45 vs.\ 15 and median of 12 vs.\ 7 for \DatasetCommonsB{}).
The only project where models of undetected mutants
have lower median operation counts than detected ones (10 vs.\ 11)
is \DatasetCommonsDev{} with its developer written test suite.
However, mean operation counts are still larger for undetected mutants
at 173 vs.\ 107 for detected ones.
Constraint counts show a similar trend to operation counts,
with mean values being 1.0--1.7 times as large
and median values being 1.0--2.5 times as large
for undetected mutants as for detected ones.
Values for \DatasetCommonsDev{} are very different from
the other \DatasetsCommonsEs{} projects because it 
covers a much smaller subset of mutants,
as previously discussed in \ref{sec:included-mutants}.

\paragraph{Constraints Used by \VariantImproved{} Generalization}
The mean (median) number of used constraints across all projects
ranges from 25--69 \% (75--100 \%) for detected mutants
and 10--67 \% (50--100 \%) for undetected ones,
with usage rates being consistently higher
for detected mutants than for undetected ones.
Because the current implementation of \ToolTeralizer{} focuses on simple constraints,
this provides further evidence
that models of undetected mutants are not only larger than those of detected ones
(i.e., contain more operations and constraints)
but also more complex
(e.g., contain more compound terms,
show more common use of non-numeric data types,
and use more math functions that are not fully modeled by SPF).
This suggests that improving \ToolTeralizer{}'s support
for more complex constraints
holds further potential for additional increases in mutation detection rates.

Furthermore, the high constraint usage rates
as well as the comparatively small operation and constraint counts
of the \DatasetsEqBenchEs{} projects suggest that it is relatively easy
to find input values that match the models.
This could explain, at least in part,
why \VariantImproved{} generalization variants are more effective
relative to the corresponding \VariantNaive{} variants
when used in the \DatasetsCommons{} projects
than in the \DatasetsEqBenchEs{} projects
(as seen in Figure~\ref{fig:mutation-detection-results}).
After all, if mutated input partition boundaries are easy to identify,
there is a higher likelihood that they will be covered
even without the more sophisticated value selection
employed by \VariantImproved{} variants.
Thus, taking into account the complexity of the underlying models
during generalization might offer further opportunities
to improve the effectiveness and/or efficiency
of test generalization approaches such as \ToolTeralizer{}.

@TODO: Add answer for RQ1.

\subsection{RQ2: Effects on Test Suite Size and Runtime}
\label{sec:ancillary-effects-eval}

In this section, we describe the secondary effects caused by the generalization
in terms of changes in test suite size and test suite runtime.
To describe these changes, we employ the following four metrics: 
\begin{enumerate}
  \item the number of tests in the test suite (Section~\ref{sec:test-suite-test-count}),
  \item the number of lines of code in the test suite (Section~\ref{sec:test-suite-line-count}),
  \item the execution time of the test suite (Section~\ref{sec:test-suite-execution-time}).
\end{enumerate}

We focus primarily on the results
of the \VariantNaiveC{} and \VariantImprovedC{} generalization variants.
The results for the remaining variants follow similar trends
but are omitted for brevity.
The full results for all generalization variants
are available in our replication package~\cite{replicationpackage}.

\subsubsection{Number of Tests in the Test Suite}
\label{sec:test-suite-test-count}

Table~\ref{tab:tests-per-project} shows how many tests are added
by the \VariantNaiveC{} and \VariantImprovedC{} generalization variants,
how many can be removed after generalization,
and how this changes the total number of tests
relative to the \VariantOriginal{} test suites
across the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects.
The number of added tests is between 174--211
(3.5--4.3~\% of \VariantOriginal{} test suite size)
for the \DatasetsEqBenchEs{} projects,
between 60--75 (2.2--2.9\%)
for the \DatasetsCommonsEs{} projects,
and 3 (0.4~\%) for the \DatasetCommonsDev{} project.
\VariantImprovedC{} generally adds a larger number of tests than \VariantNaiveC{} 
because more of the tests that are created  by \VariantNaiveC{}
are excluded due to \texttt{Too\-Many\-Filter\-Misses\-Exception}s
(as described in Section~\ref{sec:naive-generalization}
and evaluated in Section~\ref{sec:filtering-eval}).
Furthermore, the number of added tests is much smaller
than the total number of generalization that are created and evaluated by \ToolTeralizer{}
because only tests that measurably increase the mutation score of the test suite are retained.

Added tests are largely compensated by removed tests
in the \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects
which use \ToolEvoSuite{} generated test suites.
As a result, total test suite size only increases by 0--1 test cases
(0--0.04\% of \VariantOriginal{} test suite size)
for these projects.
For the \DatasetCommonsDev{} project,
none of the tests for which generalizations are added can be removed.
This is because an \VariantOriginal{} test can only be removed
if generalized tests are successfully created for all assertions in the test
(as described in Section~\ref{sec:baseline-generalization}).
In the projects with \ToolEvoSuite{} generated tests,
this requirement is generally satisfied because most tests only contain a single assertion.
However, this requirement is much more difficult to satisfy for \DatasetCommonsDev{} 
because the developer written tests often contain multiple assertions.
More generally, increases in the number of tests are
expected to be smaller for projects with more focused tests
that each contain only a small number of assertions
than for less focused ones with a large number of assertions per test.

In future work, we also plan to explore opportunities
for generalization to decrease overall test suite size.
This can be achieved
by replacing multiple \VariantOriginal{} tests
that all cover the same input partition
with a single property-based test that covers the same partition instead.
For further discussion and related work about this, see Section~\ref{sec:test-suite-reduction}.

\begin{table}[H]
\caption{Number of tests before and after generalization, with changes, per project.}
\label{tab:tests-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Tests} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 4718 & 177 & 177 & 4718 & -- & -- \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 4875 & 174 & 173 & 4876 & +1 & +0.02 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 4974 & 174 & 174 & 4974 & -- & -- \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 4718 & 206 & 206 & 4718 & -- & -- \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 4875 & 211 & 210 & 4876 & +1 & +0.02 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 4974 & 210 & 210 & 4974 & -- & -- \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 2481 & 60 & 59 & 2482 & +1 & +0.04 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 2738 & 63 & 62 & 2739 & +1 & +0.04 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 2735 & 60 & 59 & 2736 & +1 & +0.04 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 2481 & 69 & 68 & 2482 & +1 & +0.04 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 2738 & 70 & 69 & 2739 & +1 & +0.04 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 2735 & 75 & 74 & 2736 & +1 & +0.04 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 725 & 3 & 0 & 728 & +3 & +0.41 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 725 & 3 & 0 & 728 & +3 & +0.41 \% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Lines of Code in the Test Suite}
\label{sec:test-suite-line-count}

While generalization has only minor effects
on the number of tests in the evaluated test suites,
the lines of code in the test suites are more strongly affected.
As shown in Table~\ref{tab:lines-per-project},
overall lines of code (LOC) in the test suites increase
by 31.5--58.7~\% for the \DatasetsEqBenchEs{} projects,
by 18.8--29.9~\% for the \DatasetsEqBenchEs{} projects,
and by 4.9--5.3~\% for the \DatasetCommonsDev{} project.

Increases in test suite size are strongly correlated with
the number of tests added to the test suite (see Table~\ref{tab:tests-per-project}),
even if added tests are compensated for by test removals.
For example, \VariantNaive{} adds 177 tests to \DatasetEqBenchA{}
and allows the same number of \VariantOriginal{} tests to be removed.
Despite this, 11780 LOC are added
but only 1127 LOC are removed
(as shown in Table~\ref{tab:lines-per-project}).
This is expected for two reasons:
(i) generalized test classes
contain all of the code that is needed
to set valid input ranges for \ToolPit{}'s input value generators
(as described in Section~\ref{sec:test-generalization}), and
(ii) when creating the classes
that contain the generalized test methods,
\ToolTeralizer{} copies all shared code
(i.e., setup/teardown methods, non-test methods, class fields, ...)
from the \VariantOriginal{} test classes
into each generalized test class
(as described in Section~\ref{sec:test-generalization})
which results in additional LOC increases due to duplicated code.

The effects of (i) are more pronounced
for \VariantImproved{} generalization variants
than for \VariantNaive{} ones
because of the more precise (but also more complex)
identification of input ranges used by \VariantImproved{} variants.
For example, the generalized test suite created by 
\VariantImprovedC{} for \DatasetCommonsDev{}
has 36 additional LOC compared to \VariantNaiveC{}
(9018 vs.\ 8982~LOC, see Table~\ref{tab:lines-per-project})
even though the same three tests are added as a result of generalization
(as shown in Table~\ref{tab:tests-per-project}).
This difference in LOC of individual tests generalized
via \VariantImproved{} vs.\ \VariantNaive{} variants
is generally larger for tested methods
with a larger number of input parameters.
The reason for this is that each input parameter
needs to be separately restricted
to match the constraints that involve that parameter
in the input specification.

Both of the causes mentioned above
could be resolved with additional engineering effort.
For example, generalized test methods created by \ToolTeralizer{}
could be added to the \VariantOriginal{} test classes
instead of creating a new classes for each one, thus resolving cause (ii).
The primary reason we avoid this in the current implementation
is to keep \ToolTeralizer{}'s changes as unintrusive and isolated
from the \VariantOriginal{} test suite (and other generalized tests) as possible
to avoid unintended adverse side-effects on developer written code
as well as unintended interactions between generalized tests.
However, there is no technical reason
that strictly requires generalized test methods
to be added to newly created classes.
Similarly, the code to set input value ranges
that satisfy the extracted input specifications
could be extracted to a library
that abstracts away the corresponding implementation details.
In the test code of the target projects,
only minor changes would remain then
(i.e., modified test annotations, inputs, and expected outputs),
thus resolving cause (i).

\begin{table}[H]
\caption{Number of test lines before and after generalization, with changes, per project.}
\label{tab:lines-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Lines} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 30989 & 11780 & 1127 & 41642 & +10653 & +34.4 \% \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 32503 & 11520 & 1061 & 42962 & +10459 & +32.2 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 33510 & 11623 & 1069 & 44064 & +10554 & +31.5 \% \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 30989 & 19019 & 1302 & 48706 & +17717 & +57.2 \% \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 32503 & 20353 & 1284 & 51572 & +19069 & +58.7 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 33510 & 20288 & 1285 & 52513 & +19003 & +56.7 \% \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 16563 & 3733 & 359 & 19937 & +3374 & +20.4 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 18124 & 3942 & 379 & 21687 & +3563 & +19.7 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 17886 & 3723 & 361 & 21248 & +3362 & +18.8 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 16563 & 5261 & 413 & 21411 & +4848 & +29.3 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 18124 & 5423 & 421 & 23126 & +5002 & +27.6 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 17886 & 5801 & 452 & 23235 & +5349 & +29.9 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 8561 & 421 & 0 & 8982 & +421 & +4.9 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 8561 & 457 & 0 & 9018 & +457 & +5.3 \% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Execution Time of the Test Suite}
\label{sec:test-suite-execution-time}

As shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{} and \VariantImprovedC{}
increases overall test suite runtimes
for all \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
More specifically, test suite runtimes
show increases of 574.5--1210.0~\% for the \DatasetsEqBenchEs{} projects,
444.1--2651.5~\% for the \DatasetsCommonsEs{} projects,
and 9.4--57.1~\% for the \DatasetCommonsDev{} project.
Runtime increases are primarily affected by the following factors:

\begin{enumerate}
  \item the number of tests added by the generalization, even if they are compensated for by removed tests,
  \item the number of \tries{} used during property-based testing,
  \item the used generalization approach, i.e., \VariantNaive{} vs. \VariantImproved{} generalization,
  \item the complexity of input partition constraints.
\end{enumerate}

In the following paragraphs,
we discuss how each of the above factors
influences overall test suite execution times
and how these factors interact with each other.

\paragraph{Added Tests}

Execution of conventional JUnit tests has a lower runtime cost
than execution of corresponding property-based jqwik tests.
More specifically, individual property-based tests
created with the \VariantBaseline{} generalization variant 
take, on average, 149.56 milliseconds (ms) longer to execute
than the corresponding \VariantOriginal{} tests
(as shown in the left plot of Figure~\ref{fig:test-runtime-differences})
which have a mean execution time of only 3.6~ms. % 0.0035570506476290337
% Exact number = 0.0035570506476290337 via:
% SELECT AVG(runtime)
% FROM junit_test_report jtr
% JOIN generalization g ON g.test_id = jtr.test_id
% WHERE stage = 'COLLECT_JUNIT_REPORTS_ORIGINAL'
% AND g.is_included;
Therefore, overall test suite execution time increases,
on average, by at least 149.56~ms per jqwik test
that is added during the generalization process,
even if no new test inputs are exercised
and the added tests are compensated for by removed JUnit tests.
Since these runtime increases are inherent to the use of jqwik,
they are orthogonal to our specific generalization approach.
That is, any manual or automated transformation of JUnit tests
to jqwik tests incurs the same runtime overhead,
and this overhead can only be reduced
if fewer jqwik tests are created
(e.g., through test suite reduction, as discussed in Section~\ref{sec:test-suite-reduction})
or if the runtime performance of jqwik is improved.

\paragraph{Number of \tries{}}

Increasing the number of \tries{}
directly increases the number test inputs
that need to be generated and exercised
during the execution of the property-based tests.
For example, as shown in Figure~\ref{fig:test-runtime-differences},
 execution time of \VariantNaive{} tests
is, on average, 286.13 milliseconds (ms) longer per test
than for \VariantOriginal{} tests when using 10 \tries{}
(a +91.3~\% increase compared to the \VariantBaseline{} overhead of 149.56~ms),
348.56~ms (+118.5~\%) longer with 50 \tries{},
and 1136.21~ms (+659.7~\%) longer with 200 \tries{}.
Similarly, execution time of \VariantImproved{} tests
is 189.17~ms (+26.5~\%) longer with 10 \tries{},
246.85~ms (+65.1~\%) longer with 50 \tries{},
and 395.26~ms (+164.3~\%) longer with 200 \tries{}.
While overall runtime increases
get larger as \texttt{tries} increase,
runtime increases per \texttt{try}
get smaller as \texttt{tries} increase.
More specifically, \VariantNaive{} variants
show per-\texttt{try} increases
of 28.61~ms, 6.97~ms, and 5.68~ms.
\VariantImproved{} variants show
per-\texttt{try} increases 
of 18.92~ms, 4.94~ms, and 1.98~ms.
However, to attribute these observed
improvements in per-\texttt{try} efficiency to any specific causes
would require a more thorough microbenchmarking setup
that properly accounts for confounding factors such JVM warmup,
which is beyond the scope of this evaluation.

\paragraph{\VariantNaive{} vs.\ \VariantImproved{} Generalization}

Runtime increases are generally larger
for \VariantNaive{} than for \VariantImproved{}
when comparing the same tests.
For example, \VariantNaiveC{} and \VariantImprovedC{}
both generalize the same three tests of \DatasetCommonsDev{}
(see Table~\ref{tab:tests-per-project}).
Nevertheless, as shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{}
increases overall test suite runtime by 4.54 seconds
(+57.1~\% compared to \VariantOriginal{}).
\VariantImprovedC{}, on the other hand,
only increases runtime by 0.74 seconds (+9.4\%).
The more pronounced runtime impact of \VariantNaive{} variants is caused by 
the input value generation approach used by these variants.
As described in Section~\ref{sec:naive-generalization},
\VariantNaive{} generalization selects inputs for test execution
by first randomly generating inputs
that match the parameter types of the tested method,
and then discarding any inputs that do not satisfy the input specification.
Especially for cases with restrictive input specifications (e.g., \texttt{a~==~b~\&\&~b~==~c}),
this causes a large runtime overhead
because many discard-and-regenerate cycles are required
until a valid set of inputs is identified
(or \ToolJqwik{} aborts the process due to \texttt{TooManyFilterMisses}).
\VariantImproved{} variants are less affected by this
because they already consider (some) input constraints
during initial input value generation
(as described in Section~\ref{sec:improved-generalization}).
As a result, fewer inputs need to be discarded and regenerated,
which lessens the overall runtime impact of \VariantImproved{} generalization
despite the more involved input value generation process.

\paragraph{Complexity of Constraints}

As complexity of input specifications increases,
required runtime also increases.
This is because more complex constraints
cause more filter-and-regenerate cycles to occur
during execution of property-based tests.
While \VariantNaive{} generalization
is more strongly affected by this than \VariantImproved{}
because it does not consider any constraints during value generation
(as discussed in the previous paragraph),
the issue also affects \VariantImproved{} generalization
in cases where less than 100~\% of constraints can be used
(for further details on constraint use,
see Sections~\ref{sec:improved-generalization} and \ref{sec:boundary-detection-effectiveness}).
For example, as shown in Table~\ref{tab:tests-per-project},
\VariantImprovedC{} adds 3 times as many tests (206 vs.\ 69) to \DatasetEqBenchA{}
as it adds to \DatasetCommonsA{}.
Nevertheless, the difference in runtime increases
is much less pronounced at +578.7~\% vs.\ +680.5~\%.
This is because input specifications in \DatasetEqBenchA{}
have fewer operations (mean: 159.1 vs.\ 208.0, median: 10 vs.\ 15)
and constraints (mean: 6.5 vs.\ 7.5, median: 2 vs.\ 5)
than in \DatasetCommonsA{}, 
and fewer of these constraints
can be used during input value generation
(mean: 42.9~\% vs.61.5~\%, median: 80~\% vs.\ 100~\%).
This indicates that better support for more complex input constraints
not only holds the potential to increase detection rates
(as suggested in Section~\ref{sec:boundary-detection-effectiveness})
but could also reduce the runtime cost of generalized tests at the same time.

@TODO: Add answer for RQ2.

\begin{table}[H]
\caption{Test suite runtime before and after generalization, with changes, per project.}
\label{tab:runtime-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Runtime (in seconds)} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 17.44 & 100.94 & 0.74 & 117.65 & +100.20 & +574.5 \% \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 16.70 & 106.19 & 0.66 & 122.23 & +105.53 & +632.0 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 18.21 & 221.07 & 0.76 & 238.52 & +220.31 & +1210.0 \% \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 17.44 & 101.62 & 0.68 & 118.38 & +100.94 & +578.7 \% \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 16.70 & 139.64 & 0.56 & 155.77 & +139.08 & +832.9 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 18.21 & 124.68 & 0.69 & 142.20 & +123.99 & +681.0 \% \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 4.31 & 114.42 & 0.09 & 118.64 & +114.33 & +2651.5 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 7.40 & 148.54 & 0.14 & 155.80 & +148.40 & +2005.2 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 6.30 & 122.11 & 0.07 & 128.34 & +122.04 & +1936.2 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 4.31 & 29.49 & 0.14 & 33.66 & +29.35 & +680.5 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 7.40 & 71.50 & 0.21 & 78.70 & +71.30 & +963.3 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 6.30 & 28.07 & 0.08 & 34.29 & +27.99 & +444.1 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 7.95 & 4.54 & 0.00 & 12.48 & +4.54 & +57.1 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 7.95 & 0.74 & 0.00 & 8.69 & +0.74 & +9.4 \% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_test_runtime_differences}
  \caption{Runtime Comparison: Test vs. Generalization.}
  %\Description{@TODO}
  \label{fig:test-runtime-differences}
\end{figure}

@TODO: Split Figure~\ref{fig:test-runtime-differences} into two (sub-)figures to make each one individually referenceable.

\subsection{RQ3: Runtime Requirements}
\label{sec:runtime-eval}

As previously described in Section~\ref{sec:evaluation-setup},
we measure the runtime requirements of \ToolTeralizer{}
by executing the tool once for each of the seven \DatasetsEqBenchEs{} and \DatasetsCommons{} projects
using a MacBook Air with M2 processor and 24~GB of memory.
Since all generalization variants require the same input data
(i.e., source code of target projects as well as extracted input/output specifications),
\ToolTeralizer{} executes the corresponding processing steps
(i.e., project instrumentation, test analysis, and specification extraction)
only once and then reuses the collected specifications
across all generalizations of the project.
Because no other tools exist
that transform conventional unit tests to property-based tests,
we measure the efficiency of \ToolTeralizer{}
by comparing the mutation score increase that \ToolTeralizer{} achieves 
when increasing the used \tries{} (thus increasing its runtime)
to the corresponding increase that \ToolEvoSuite{} achieves
when increasing the test generation timeout.

\subsubsection{Execution Time of \ToolTeralizer{}}
\label{sec:execution-time}

\subsubsection{Efficiency of \ToolTeralizer{} vs.\ \ToolEvoSuite{}}
\label{sec:execution-efficience}

\begin{table}[H]
  \caption{Total runtimes of Teralizer for all evaluated projects.}
  \label{tab:teralizer-runtimes}
  \begin{tabular}{lr}
    \toprule
    Project & Runtime \\ 
    \midrule
    \DatasetEqBenchA{} & 24h 46min 27s \\ 
    \DatasetEqBenchB{} & 28h 16min 32s \\ 
    \DatasetEqBenchC{} & 30h 53min 22s \\ 
    \midrule
    \DatasetCommonsA{} & 8h 13min 55s \\ 
    \DatasetCommonsB{} & 9h 46min 48s \\ 
    \DatasetCommonsC{} & 9h 04min 32s \\ 
    \midrule
    \DatasetCommonsDev{} & 3h 23min 22s \\ 
    \bottomrule 
  \end{tabular} 
\end{table}

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetsCommons{} results in Figure~\ref{fig:teralizer-runtimes}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_teralizer_runtimes}
  \caption{Teralizer runtimes per project, processing stage, and generalization variant.}
  %\Description{@TODO}
  \label{fig:teralizer-runtimes}
\end{figure}

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetsCommons{} results in Figure~\ref{fig:teralizer-efficiency}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{fig_teralizer_efficiency.pdf}
  \caption{Pareto fronts for EvoSuite and Teralizer variants across projects.}
  \label{fig:teralizer-efficiency}
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project commons-utils.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & \ToolEvoSuite{} & \ToolTeralizer{} & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 56.8 & 4648.7 \\
          2 & 10s & - & 57.3 & 5597.3 \\
          3 & 1s & IMPROVED$_{10}$ & 57.9 & 7294.5 \\
          4 & 60s & - & 58.1 & 10239.8 \\
          5 & 10s & NAIVE$_{10}$ & 58.1 & 10445.1 \\
          6 & 10s & IMPROVED$_{50}$ & 58.4 & 10603.4 \\
          7 & 10s & IMPROVED$_{10}$ & 58.4 & 11081.5 \\
          8 & 10s & IMPROVED$_{200}$ & 58.5 & 13270.4 \\
          9 & 60s & IMPROVED$_{10}$ & 59.3 & 13938.7 \\
          10 & 60s & IMPROVED$_{50}$ & 59.4 & 14727.5 \\
          11 & 60s & IMPROVED$_{200}$ & 59.5 & 15735.7 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project \DatasetEqBench{}.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & \ToolEvoSuite{} & \ToolTeralizer{} & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 48.1 & 26479.3 \\
          2 & 10s & - & 50.6 & 29861.1 \\
          3 & 1s & NAIVE$_{10}$ & 50.7 & 36727.9 \\
          4 & 1s & IMPROVED$_{50}$ & 51.4 & 37457.4 \\
          5 & 1s & NAIVE$_{50}$ & 51.7 & 37531.9 \\
          6 & 10s & IMPROVED$_{10}$ & 51.9 & 41525.3 \\
          7 & 10s & IMPROVED$_{50}$ & 53.6 & 42256.2 \\
          8 & 10s & NAIVE$_{50}$ & 53.8 & 45398.0 \\
          9 & 10s & IMPROVED$_{200}$ & 53.8 & 48268.6 \\
          10 & 10s & NAIVE$_{200}$ & 54.1 & 62938.2 \\
          11 & 60s & IMPROVED$_{50}$ & 54.5 & 68092.9 \\
          12 & 60s & NAIVE$_{50}$ & 54.7 & 68782.2 \\
          13 & 60s & IMPROVED$_{200}$ & 54.8 & 75081.4 \\
          14 & 60s & NAIVE$_{200}$ & 55.0 & 93017.1 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
\end{figure}

large potential for runtime improvements, see examples in the discussion

\subsection{RQ4 (Part 1 of 2): Causes of Unsuccessful Generalizations in the Evaluation Dataset}
\label{sec:filtering-eval}

In this section, we describe causes of unsuccessful generalizations in the main
evaluation dataset (commons-utils + evosuite-variants, eqbench + evosuite variants).
To get more generalizable results that enable better informed decisions about future
research directions, we also evaluated generalization success vs. failure in <number>
additional open source projects beyond the main evaluation dataset. Results of the
extended evaluation are discussed in Section~\ref{sec:filtering-eval-extended}.

overall test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Included and excluded counts by variant and level.}
  \label{tab:exclusions-summary}
  \begin{tabular}{llrrr}
    \toprule
    Variant & Type & Total & \multicolumn{1}{c}{Included} & \multicolumn{1}{c}{Excluded} \\
    \midrule
    SHARED & Test & 23246 & 19306\; (83.1\%) & 3940\; (16.9\%) \\
    SHARED & Assertion & 28923 & 13836\; (47.8\%) & 15087\; (52.2\%) \\
    BASELINE & Generalization & 13836 & 13814\; (99.8\%) & 22\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & 13836 & 10743\; (77.6\%) & 3093\; (22.4\%) \\
    NAIVE$_{50}$ & Generalization & 13836 & 9964\; (72.0\%) & 3872\; (28.0\%) \\
    NAIVE$_{200}$ & Generalization & 13836 & 9881\; (71.4\%) & 3955\; (28.6\%) \\
    IMPROVED$_{10}$ & Generalization & 13836 & 11788\; (85.2\%) & 2048\; (14.8\%) \\
    IMPROVED$_{50}$ & Generalization & 13836 & 11660\; (84.3\%) & 2176\; (15.7\%) \\
    IMPROVED$_{200}$ & Generalization & 13836 & 11597\; (83.8\%) & 2239\; (16.2\%) \\
    \bottomrule
  \end{tabular}
\end{table}

filtering-based test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Filtering results for tests, assertions, and generalizations by filter and (generalization) variant.}
  \label{tab:exclusions-filtering}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 23246 & 21719\; (93.4\%) & 1527.0\; (\phantom{0}6.6\%) \\
    SHARED & Test & TestType & 23246 & 23066\; (99.2\%) & 180.0\; (\phantom{0}0.8\%) \\
    SHARED & Test & NoAssertions & 21532 & 19306\; (89.7\%) & 2226.0\; (10.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 28923 & 27326\; (94.5\%) & 1597.0\; (\phantom{0}5.5\%) \\
    SHARED & Assertion & MissingValue & 28923 & 21766\; (75.3\%) & 7157.0\; (24.7\%) \\
    SHARED & Assertion & ParameterType & 28923 & 17835\; (61.7\%) & 11088.0\; (38.3\%) \\
    SHARED & Assertion & UnsupportedAssertion & 28923 & 28180\; (97.4\%) & 743.0\; (\phantom{0}2.6\%) \\
    SHARED & Assertion & VoidReturnType & 28923 & 21763\; (75.2\%) & 7160.0\; (24.8\%) \\
    \midrule
    BASELINE & Generalization & NonPassingTest & 13836 & 13814\; (99.8\%) & 22.0\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & NonPassingTest & 13804 & 10743\; (77.8\%) & 3061.0\; (22.2\%) \\
    NAIVE$_{50}$ & Generalization & NonPassingTest & 13804 & 9964\; (72.2\%) & 3840.0\; (27.8\%) \\
    NAIVE$_{200}$ & Generalization & NonPassingTest & 13804 & 9881\; (71.6\%) & 3923.0\; (28.4\%) \\
    IMPROVED$_{10}$ & Generalization & NonPassingTest & 13804 & 11788\; (85.4\%) & 2016.0\; (14.6\%) \\
    IMPROVED$_{50}$ & Generalization & NonPassingTest & 13804 & 11660\; (84.5\%) & 2144.0\; (15.5\%) \\
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 13804 & 11597\; (84.0\%) & 2207.0\; (16.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

failing tests

\begin{table}[H]
  \caption{Number of test execution failures by exception type and (generalization) variant.}
  \label{tab:exclusions-test-fails}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Variant & ORIGINAL & BASELINE & \multicolumn{3}{c}{NAIVE} & \multicolumn{3}{c}{IMPROVED} \\
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
    Tries & - & - & 10 & 50 & 200 & 10 & 50 & 200 \\
    \midrule
    ArithmeticException & 0 & 0 & 99 & 99 & 99 & 57 & 58 & 58 \\
    AssertionFailedError & 132 & 22 & 729 & 803 & 819 & 752 & 845 & 866 \\
    NumberFormatException & 0 & 0 & 0 & 0 & 0 & 18 & 18 & 18 \\
    TooManyFilterMissesException & 0 & 0 & 2233 & 2938 & 3005 & 1189 & 1223 & 1265 \\
    \bottomrule
  \end{tabular}
\end{table}

@TODO: Remove Table~\ref{tab:exclusions-spf}. Describe SPF execution failures in text only.

\begin{table}[H]
  \centering
  \caption{Number of SPF execution failures by error type.}
  \label{tab:exclusions-spf}
  \begin{tabular}{lrr}
    \toprule
    Error Type & Total & Percent \\
    \midrule
    SPF exception & 1540 & 51.42 \\
    PC size limit exceeded & 790 & 26.38 \\
    Depth limit exceeded & 524 & 17.50 \\
    Teralizer exception & 97 & 3.24 \\
    Execution timeout & 28 & 0.93 \\
    OutOfMemoryError & 16 & 0.53 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ4 (Part 2 of 2): Causes of Unsuccessful Generalizations in Other Open Source Projects}
\label{sec:filtering-eval-extended}

This evaluation only uses the IMPROVED$_{200}$
generalization variant, but results also apply to all other variants.

\begin{table}[H]
  \caption{Number of processing failures and remaining projects per processing stage.}
  \label{tab:processing-failures-per-stage}
  \begin{tabular}{l r r}
    \toprule
    Processing Stage & Failures & Remaining Projects \\
    \midrule
    Total projects & - & 1160\; (\phantom{.}100 \%) \\
    \cmidrule(lr){1-3}
    SETUP\_PROJECT & 355 & 805\; (69.4 \%) \\
    BUILD\_PROJECT\_ORIGINAL & 189 & 616\; (53.1 \%) \\
    BUILD\_SPOON\_MODEL & 8 & 608\; (52.4 \%) \\
    EXECUTE\_TESTS\_ORIGINAL & 61 & 547\; (47.2 \%) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & 31 & 516\; (44.5 \%) \\
    BUILD\_PROJECT\_INSTRUMENTED & 1 & 515\; (44.4 \%) \\
    EXECUTE\_TESTS\_INITIAL & 130 & 385\; (33.2 \%) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & 41 & 344\; (29.7 \%) \\
    COLLECT\_PIT\_DATA\_INITIAL & 64 & 280\; (24.1 \%) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & 270 & 10\; (\phantom{0}0.9 \%) \\
    \cmidrule(lr){1-3}
    Successfully processed & - & 10\; (\phantom{0}0.9 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \caption{Causes of processing failures per processing stage.}
  \label{tab:processing-failure-causes}
  \begin{tabularx}{\textwidth}{l X}
    \toprule
    Processing Stage & Causes of Processing Failures \\
    \midrule
    SETUP\_PROJECT & dependency resolution error (329), sources / tests not found (26) \\
    BUILD\_PROJECT\_ORIGINAL & compilation error (171), compilation outputs not found (18) \\
    BUILD\_SPOON\_MODEL & Spoon execution error (8) \\
    EXECUTE\_TESTS\_ORIGINAL & JUnit execution error (13), timeout exceeded (48) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & JUnit outputs not found (31) \\
    BUILD\_PROJECT\_INSTRUMENTED & compilation error (1) \\
    EXECUTE\_TESTS\_INITIAL & all tests excluded (129), timeout exceeded (1) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & JaCoCo execution error (1), JaCoCo outputs not found (40) \\
    COLLECT\_PIT\_DATA\_INITIAL & PIT execution error (16), PIT outputs not found (4), all classes excluded (3), failed to map PIT data to a test (1), timeout exceeded (40) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & all generalizations excluded (269), failed to map PIT data to a generalization (1) \\
    \bottomrule
  \end{tabularx}
\end{table}

Broadly speaking, there are three main ways to increase the number of projects
that can be successfully processed: (i)~reducing exclusions caused by filtering
(129 + 3 + 269 = 401 projects), (ii)~adding support for more varied project structures
(26 + 18 + 31 + 40 + 4 = 119 projects), and (iii)~increasing timeouts (48 + 1 + 40 = 89 projects).
Depedency resolution and compilation errors in the original project code (329 + 171 = 500 projects)
as well as external tool errors (8 + 13 + 1 + 16 = 38 projects) are less actionable.
Furthermore, Teralizer errors only occur in a small number of cases (1 + 1 + 1 = 3 projects),
so also offer comparatively little opportunity for improvements.

\begin{table}[H]
  \caption{Filtering results of the extended dataset for tests, assertions, and generalizations.}
  \label{tab:exclusions-filtering-extended}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 74332 & 65579\; (88.2\%) & 8753.0\; (11.8\%) \\
    SHARED & Test & TestType & 74332 & 65052\; (87.5\%) & 9280.0\; (12.5\%) \\
    SHARED & Test & NoAssertions & 56853 & 33390\; (58.7\%) & 23463.0\; (41.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 122166 & 101519\; (83.1\%) & 20647.0\; (16.9\%) \\
    SHARED & Assertion & MissingValue & 122166 & 51430\; (42.1\%) & 70736.0\; (57.9\%) \\
    SHARED & Assertion & ParameterType & 122166 & 5393\; (\phantom{0}4.4\%) & 116773.0\; (95.6\%) \\
    SHARED & Assertion & ReturnType & 122166 & 11650\; (\phantom{0}9.5\%) & 110516.0\; (90.5\%) \\
    SHARED & Assertion & UnsupportedAssertion & 122166 & 92996\; (76.1\%) & 29170.0\; (23.9\%) \\
    \midrule
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 229 & 206\; (90.0\%) & 23.0\; (10.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

The most common filtering reasons are unsupported parameter and return types.

@TODO: Better explain these filtering results by prooviding information about:
(i) which types are identified in the tested method signatures, and
(ii) which assertions are used in the tests.

Successful generalizations did not improve mutation scores in any of the successfully
processed projects. However, some generalizations indicate that there weak
preconditions are used in the source / test code of the original implementation.

@TODO: Check which generalizations fail the "NonPassingTest" filter due to (i) bugs in
our generalization implemenation vs. (ii) weak preconditions.

% Performance improvements of Teralizer itself can NOT be used as a substitute for
% (i) because the timeouts occur for tasks that are performed by external tools.
