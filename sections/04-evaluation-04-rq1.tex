\subsection{RQ1: How much does test generalization improve mutation detection?}
\label{sec:primary-effects-eval}

Mutation testing provides a rigorous measure of test effectiveness
by evaluating a test suite's ability to detect deliberately introduced faults.
For \ToolTeralizer{}, mutation scores reveal whether testing additional inputs
within existing execution paths achieves the intended improvement in fault detection capabilities.
Section~\ref{sec:included-mutants} first establishes which tests and mutants are included in the evaluation,
revealing systematic differences between generated and developer-written test suites that influence generalization outcomes.
Section~\ref{sec:overall-detection-rates} quantifies detection improvements
across projects and variants,
demonstrating that generalization improves mutation scores
for \DatasetsEqBenchEs{} and \DatasetsCommons{} projects, albeit with varying improvement magnitudes.
Section~\ref{sec:detection-rates-per-mutator} dissects these improvements by mutation operator,
uncovering how much detection of different mutants benefits from generalization.
Finally, Section~\ref{sec:boundary-detection-effectiveness} provides a
detailed analysis of the \VariantImproved{} generalization variant,
demonstrating that its effectiveness correlates with constraint complexity,
i.e., projects with more complex input constraints show greater improvements
from constraint-aware input generation.

\subsubsection{Included Mutants}
\label{sec:included-mutants}

To evaluate mutation detection effects,
we compare \ToolPit{} results for \VariantInitial{} variants
against \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.
This comparison isolates generalization effects by using \VariantInitial{} as the baseline,
ensuring that differences stem solely from the presence or absence of generalized test cases.
Comparison to \VariantOriginal{} would not isolate generalization effects
because the original test suite contains tests excluded during
test analysis and specification extraction (Section~\ref{sec:all-filtering}),
i.e., before any transformation to property-based tests occurs.

Test inclusion rates reveal fundamental differences between generated and developer-written test suites
that shape generalization outcomes.
EvoSuite-generated projects achieve 83.4--85.1\% inclusion rates across variants,
while \DatasetCommonsDev{} includes only 63.6\% of original test methods
(Table~\ref{tab:mutants-per-project}).
This 20-percentage-point gap reflects how different test creation approaches
align with \ToolTeralizer{}'s current processing capabilities.

\input{tables/tab-mutants-per-project}

EvoSuite generates tests with characteristics that facilitate automated analysis:
simple assertion patterns that compare single values,
explicit parameter passing without complex setup logic,
and method-level focus that isolates individual behaviors.
Developer-written tests, conversely, combine multiple scenarios within single methods,
employ elaborate fixture setup spanning multiple classes,
and validate system state changes through sequences of operations.
While these patterns enable comprehensive integration testing,
they create barriers for specification extraction, explaining
why over one-third of developer tests cannot be generalized.

The three primary exclusion causes reflect current limitations in our prototype:
(i)~tests without assertions provide no specification to generalize,
(ii)~tests where no tested method can be identified typically involve
program constructs that our simple static analysis implementation cannot resolve,
such as loops in test code or modifications of object state rather than direct return values,
and (iii)~tests exercising methods without generalizable parameters have no numeric
or boolean inputs to vary.
Section~\ref{sec:filtering-eval} quantifies how frequently all exclusion causes occur,
providing guidance for extending generalization capabilities.

Mutation coverage patterns highlight further differences across the project environments.
\ToolPit{} generates approximately 8,000 total mutants for Apache Commons projects
and 23,000--24,000 for EqBench projects (Table~\ref{tab:mutants-per-project}).
Covered mutants, i.e., those mutants that are executed by at least one included test, range
from 64.4\% for \DatasetCommonsDev{} to 97.1\% for \DatasetCommonsC{}.
This coverage disparity primarily reflects dataset construction rather than test quality:
\DatasetCommonsDev{} includes only tests directly covering the extracted utility methods
(but no tests for transitively called methods),
while EvoSuite variants generate comprehensive suites for all included code.
This focused scope reflects our intent to evaluate code amenable to generalization in this part of the dataset,
as discussed in Section~\ref{sec:apache-commons-utils}.

% With the evaluation scope established, we now analyze how effectively
% the generalized tests detect mutations within the included subset.

\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}

Generalization improves mutation detection across all evaluated projects,
though the degree of improvement varies by project type
(Figure~\ref{fig:mutation-detection-results}).
\DatasetsEqBenchEs{} projects show the largest improvements:
detection rates increase from 48.1--51.6\% to 49.5--55.0\%
across different \tries{} and generalization settings,
representing absolute improvements of 1.2--3.9 percentage points
(2.4--8.2\% relative increase).
\DatasetsCommonsEs{} projects improve by 0.82--1.33 percentage points
(1.4--2.3\% relative increase),
while \DatasetCommonsDev{} improves by only 0.05--0.07 percentage points
from its 80.4\% baseline.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  \Description{Multi-panel bar chart showing mutation detection results.
  Left column displays detected percentages for seven projects:
  eqbench-es-default-1s, -10s, -60s, commons-utils-es-default-1s, -10s, -60s, and commons-utils.
  Each panel shows bars for INITIAL (blue), NAIVE variants (orange), and IMPROVED variants (green).
  Variants have subscripts (10, 50, 200) indicating tries parameter.
  Y-axis ranges from approximately 48% to 81% detection rate.
  Right column shows corresponding improvement percentages over INITIAL baseline,
  with values in parentheses and +/- indicators.
  Improvements range from minimal (0.05%) for commons-utils developer tests
  to substantial (7.82%) for eqbench-es-default-1s.
  Color legend at top: INITIAL (blue), NAIVE Variants (orange), IMPROVED Variants (green).}
  \label{fig:mutation-detection-results}
\end{figure}

In general, two factors strongly affect achievable generalization improvements:
initial test suite strength and input constraint complexity.
\DatasetsEqBenchEs{} projects offer the most suitable conditions for generalization.
Their EvoSuite-generated tests leave more room for enhancement
(starting from 48.1--51.6\% detection rates)
than the thorough \DatasetCommonsDev{} test suite,
and the \DatasetEqBench{} benchmark's input constraints
are simpler than the input constraints of \DatasetsCommons{},
thus making valid input generation easier
(as discussed in more detail in Section~\ref{sec:boundary-detection-effectiveness}).
\DatasetsCommonsEs{} projects face one additional challenge:
while they also start from EvoSuite-generated tests,
the \DatasetsCommons{} methods involve more complex input specifications,
making it harder to generate inputs that satisfy input constraints.
\DatasetCommonsDev{} faces both challenges:
the mature developer-written test suite already achieves 80.4\% detection
through years of refinement of these widely-used libraries,
and it shares the same complex constraint characteristics as other \DatasetsCommons{} variants.
This leaves little opportunity for automated improvement.

Comparing the relative effectiveness of \VariantNaive{} and \VariantImproved{}
shows opposite results across \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
On \DatasetsEqBenchEs{} projects, \VariantNaive{} variants
outperform \VariantImproved{} for all \tries{} settings,
with gaps ranging from 0.17--1.21 percentage points.
This advantage is most pronounced with limited \tries{}:
\VariantNaiveA{} achieves 50.67--52.99\% detection rate
while \VariantImprovedA{} reaches only 49.46--51.86\%.
In contrast, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 comparisons,
with \VariantImprovedC{} detecting 57.97--59.45\% of mutants
compared to \VariantNaiveC{}'s 58.00--59.35\%.
These opposite results reflect the following patterns:
\VariantNaive{}'s better performance on \texttt{Math} mutations (59.1\% of all mutants)
versus \VariantImproved{}'s better detection of most other mutation types
as well as more effective handling of complex constraints.
In \DatasetsEqBenchEs{}, the high prevalence of \texttt{Math} mutations
and low constraint complexity favor \VariantNaive{},
while in \DatasetsCommonsEs{}, \VariantImproved{}'s
advantages on other mutations and constraint handling overcome
its \texttt{Math} mutant detection disadvantage.
We investigate the mechanisms behind these results in further detail in
Sections~\ref{sec:detection-rates-per-mutator} and \ref{sec:boundary-detection-effectiveness}.
\DatasetCommonsDev{} shows no meaningful differences between approaches,
with all variants achieving approximately 80.4\% detection.

Higher \tries{} settings showed improved detection rates with diminishing returns.
The increase from 10 to 50 \tries{} typically produces larger gains
than increasing from 50 to 200 \tries{}.
One notable exception to this pattern appears in \VariantImproved{} variants with only 10 \tries{}:
on \DatasetsEqBenchEs{} projects,
\VariantImprovedA{} achieves only 2.4--2.8\% relative improvement
versus 5.6--6.9\% for \VariantImprovedB{} and 6.0--7.8\% for \VariantImprovedC{}.
This comparatively low increase in detection rates likely stems from boundary-focused generation
consuming most of the limited \tries{} of this variant,
leaving insufficient attempts for testing intermediate values.
With more \tries{}, \VariantImproved{} variants perform comparably to or better than \VariantNaive{} variants
as enough attempts remain for both boundary and non-boundary testing.
\DatasetsCommonsEs{} projects show less pronounced degradation at low \tries{},
because their more complex constraints (Section~\ref{sec:boundary-detection-effectiveness})
cause more boundary inputs to fail filtering,
forcing the algorithm to explore non-boundary values sooner.

Combining short test generation with subsequent generalization
can outperform longer generation alone.
On \DatasetsEqBenchEs{} projects,
1-second \ToolEvoSuite{} generation followed by \VariantNaiveC{} generalization
achieves 52.0\% detection,
surpassing 60-second generation alone (51.6\%).
Similarly on \DatasetsCommonsEs{},
10-second generation plus \VariantNaiveC{} reaches 58.5\%
versus 58.1\% for 60-second generation.
These comparisons demonstrate that generalization can effectively compensate for reduced generation time.
Section~\ref{sec:execution-efficiency} analyzes the efficiency trade-offs
between different combinations of \ToolEvoSuite{} timeouts and \ToolTeralizer{} \tries{}
in further detail through Pareto front analysis.

The reported improvements are based on single runs per project configuration.
While property-based test generation includes randomness that could produce variation across runs,
the consistent patterns across projects suggest the observed trends are robust.
Multiple runs would strengthen confidence in the precise magnitude of improvements
(a limitation we discuss in Section~\ref{sec:threats-to-validity}).

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}

Generalization effectiveness varies significantly across mutation operators,
revealing which fault types benefit most from additional test inputs
(Table~\ref{tab:detections-per-mutator}).
Three operators dominate the mutation landscape:
\texttt{Math} (59.1\% of all mutants),
\texttt{Conditionals\-Boundary} (11.0\%),
and \texttt{Remove\-Conditional\-Order\-Else} (11.0\%).
In contrast, the least common operators, i.e., \texttt{Increments} (0.52\%),
\texttt{Boolean\-False\-ReturnVals} (0.24\%),
and \texttt{Empty\-Object\-Return\-Vals} (0.13\%), together
account for less than 1\% of all mutants.
This large difference in mutant prevalence means that
improvements to \texttt{Math} detection rates
have proportionally larger impact on overall mutation scores
than improvements to less commonly occurring mutation types.

\input{tables/tab-detections-per-mutator}

Initial detection rates reveal a clear pattern:
return value mutations are caught
in the large majority of cases (87.9--98.8\% detection),
while behavioral mutations prove more elusive.
For example, \texttt{VoidMethodCall} mutations achieve a detection rate of only 25.0\%
because removing void method calls typically affects only internal state
or produces side effects that are not directly verified by test assertions.
\texttt{ConditionalsBoundary} mutations fare only slightly better at 27.7\% total detection rate,
highlighting an opportunity for automated improvement.
After all, the \texttt{Conditionals\-Boundary} mutations
change comparison operators at partition boundaries,
which is exactly where \VariantImproved{} generalization
aims to generate additional test inputs.

\VariantNaiveC{} achieves its largest improvements on
\texttt{Math} (4.0~percentage points),
\texttt{RemoveConditionalEqualElse} (2.1~percentage points),
and \texttt{InvertNegs} (1.7~percentage points).
The success with \texttt{RemoveConditionalEqualElse} stems from
testing diverse inputs that trigger both branches of equality checks,
exposing mutations that force false branches.
\texttt{Math} mutation detection similarly benefits from a more diverse
range of tested input values because arithmetic operations often produce
different results across input ranges, exposing mutations that might
coincidentally produce correct results for any individual value.
In contrast, 4 of 5 return value mutators show zero improvement:
\texttt{Null\-Return\-Vals}, \texttt{Boolean\-True\-Return\-Vals},
\texttt{Boolean\-False\-ReturnVals}, and \texttt{EmptyObjectReturnVals}.
These already achieve high detection rates in the \VariantInitial{} variant (87.9--98.8\%)
because return value mutations often directly violate test assertions,
leaving little room for improvement through input variation.
Increasing detection rates beyond this high starting point
would likely require additional assertions to be introduced,
rather than test inputs to be varied.

\VariantImprovedC{} demonstrates different strengths than \VariantNaive{}
through constraint-aware input generation.
Comparing the two variants across all 12 mutation operators:
\VariantImprovedC{} outperforms \VariantNaiveC{} for 7 operators,
achieves the same detection rate for 4 operators (the zero-improvement return value mutations),
and underperforms for only 1 operator (\texttt{Math}).
The largest advantage is observed for \texttt{Conditionals\-Boundary} detection,
where \VariantImprovedC{} achieves 2.5 percentage points improvement
compared to \VariantNaiveC{}'s 1.2 percentage points.
These results confirm that the constraint-aware input generation strategy
followed by \VariantImproved{} variants achieves its intended purpose.
Furthermore, the slightly higher detection rates for several other mutators
suggest that testing at partition boundaries may also provide slight benefits
for some mutators that do not directly modify partition boundaries.

The \texttt{Math} mutation results highlight the inherent trade-off
in boundary versus non-boundary testing:
\VariantImprovedC{} achieves 3.4 percentage points improvement
compared to \VariantNaiveC{}'s 4.0 percentage points.
The \VariantImprovedC{} variant achieves smaller improvements here
because constraint-aware input generation produces less diverse arithmetic inputs,
concentrating on boundary values rather than exploring the full numeric range
where arithmetic mutations might create more varied outputs.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this difference significantly impacts overall mutation scores.
To counteract these detrimental effects,
\VariantImproved{} variants could use higher \tries{} settings
to maintain non-boundary input coverage of \VariantNaive{}
while adding additional boundary coverage.
Alternatively, more sophisticated input selection strategies
could be used to achieve a better balance between
boundary and non-boundary testing even at lower \tries{} settings,
thus avoiding the runtime cost of higher \tries{}.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
For example, \VariantImprovedA{} achieves expected results for most mutation operators,
including its characteristic advantage for \texttt{ConditionalsBoundary} mutations (2.0\% vs 0.9\% for \VariantNaiveA{}).
The exception is \texttt{Math} mutation detection,
where \VariantImprovedA{} achieves only 1.1\% improvement compared to 2.8\% for \VariantNaiveA{}.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this explains the low overall detection rate of \VariantImprovedA{} on \DatasetsEqBenchEs{} projects
observed in Section~\ref{sec:overall-detection-rates}.
With 50 \tries{}, the \texttt{Math} detection gap is noticeably smaller
(\VariantImprovedB{} achieves 3.1\% versus \VariantNaiveB{}'s 3.6\%),
confirming that limited \tries{} constrain arithmetic diversity
only when boundary testing consumes most attempts.
Full results for all variants are available in our replication package~\cite{replicationpackage}.

TODO: Answer to RQ1.
