\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

Mutation testing provides a rigorous measure of test effectiveness
by evaluating a test suite's ability to detect deliberately introduced faults.
For \ToolTeralizer{}, mutation scores reveal whether testing additional inputs
within existing execution paths achieves the intended improvement in fault detection capabilities.
Section~\ref{sec:included-mutants} first establishes which tests and mutants are included in the evaluation,
revealing systematic differences between generated and developer-written test suites that influence generalization outcomes.
Section~\ref{sec:overall-detection-rates} then quantifies detection improvements
across projects and variants,
demonstrating that generalization consistently improves mutation scores,
though with varying improvement magnitudes.
Section~\ref{sec:detection-rates-per-mutator} dissects these improvements by mutation operator,
uncovering how much detection of different mutants benefits from generalization.
Finally, Section~\ref{sec:boundary-detection-effectiveness} provides a
detailed analysis of the \VariantImproved{} generalization variant,
demonstrating that its effectiveness correlates with constraint complexity,
i.e., projects with more complex input constraints show greater improvements
from constraint-aware input generation.

\subsubsection{Included Mutants}
\label{sec:included-mutants}

To evaluate mutation score effects,
we compare \ToolPit{} results for \VariantInitial{} variants
against \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.
This comparison isolates generalization effects by using \VariantInitial{} as the baseline,
ensuring that differences stem solely from the presence or absence of generalized test cases.
Comparison to \VariantOriginal{} would not isolate generalization effects
because the original test suite contains tests excluded during
test analysis and specification extraction (Section~\ref{sec:all-filtering})â€”before
any transformation to property-based tests occurs.

Test inclusion rates reveal fundamental differences between generated and developer-written test suites
that shape generalization outcomes.
EvoSuite-generated projects achieve 83.4--85.1\% inclusion rates across variants,
while \DatasetCommonsDev{} includes only 63.6\% of original test methods
(Table~\ref{tab:mutants-per-project}).
This 20-percentage-point gap reflects how different test creation approaches
align with \ToolTeralizer{}'s current processing capabilities.

EvoSuite generates tests with characteristics that facilitate automated analysis:
simple assertion patterns that compare single values,
explicit parameter passing without complex setup logic,
and method-level focus that isolates individual behaviors.
Developer-written tests, conversely, combine multiple scenarios within single methods,
employ elaborate fixture setup spanning multiple classes,
and validate system state changes through sequences of operations.
While these patterns enable comprehensive integration testing,
they create barriers for specification extraction, explaining
why over one-third of developer tests cannot be generalized.

The three primary exclusion causes reflect \ToolTeralizer{}'s current limitations:
(i)~tests without assertions provide no specification to generalize,
(ii)~tests where no tested method can be identified typically involve
program constructs that \ToolTeralizer{}'s simple static analysis cannot resolve,
such as loops in test code or modifications of object state rather than direct return values,
and (iii)~tests exercising methods without generalizable parameters have no numeric
or boolean inputs to vary.
Section~\ref{sec:filtering-eval} quantifies how frequently all exclusion causes occur,
providing guidance for extending generalization capabilities.

Mutation coverage patterns highlight further differences across the project environments.
\ToolPit{} generates approximately 8,000 total mutants for Apache Commons projects
and 23,000--24,000 for EqBench projects (Table~\ref{tab:mutants-per-project}).
Covered mutants, i.e., those mutants that are executed by at least one included test, range
from 64.4\% for \DatasetCommonsDev{} to 97.1\% for \DatasetCommonsC{}.
This coverage disparity primarily reflects dataset construction rather than test quality:
\DatasetCommonsDev{} includes only tests directly covering the extracted utility methods
(but no tests for transitively called methods),
while EvoSuite variants generate comprehensive suites for all included code.
This focused scope reflects our intent to evaluate code amenable to generalization in this part of the dataset,
as discussed in Section~\ref{sec:apache-commons-utils}.

With the evaluation scope established, we now analyze how effectively
the generalized tests detect mutations within the included subset.

\input{tables/tab-mutants-per-project}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}

Generalization improves mutation detection across all evaluated projects,
though the magnitude varies by project type
(Figure~\ref{fig:mutation-detection-results}).
\DatasetsEqBenchEs{} projects show the largest improvements:
detection rates increase from 48.1--51.6\% to 49.5--55.0\%
across different \tries{} and generalization settings,
representing absolute improvements of 1.2--3.9 percentage points
(2.4--8.2\% relative increase).
\DatasetsCommonsEs{} projects improve by 0.82--1.33 percentage points
(1.4--2.3\% relative increase),
while \DatasetCommonsDev{} improves by only 0.05--0.07 percentage points
from its 80.4\% baseline.

Two factors strongly affect achievable generalization improvements:
initial test suite strength and input constraint complexity.
\DatasetsEqBenchEs{} projects offer the most favorable conditions.
Their EvoSuite-generated tests leave more room for enhancement
(starting from 48.1--51.6\% detection rates)
than the thorough \DatasetCommonsDev{} test suite,
and the \DatasetEqBench{} benchmark's input constraints
are simpler than the input constraints of \DatasetsCommons{},
thus making valid input generation easier
(as discussed in more detail in Section~\ref{sec:boundary-detection-effectiveness}).
\DatasetsCommonsEs{} projects face one additional challenge:
while they also start from EvoSuite-generated tests,
the \DatasetsCommons{} methods involve more complex input specifications,
making it harder to generate inputs that satisfy input constraints.
\DatasetCommonsDev{} faces both challenges:
the mature developer-written test suite already achieves 80.4\% detection
through decades of refinement in these widely-used libraries,
and it shares the same complex constraint characteristics as other \DatasetsCommons{} variants.
This leaves minimal opportunity for automated improvement.

Higher \tries{} settings improve detection rates but with diminishing returns.
The jump from 10 to 50 \tries{} typically produces larger gains
than increasing from 50 to 200 \tries{}.
One notable exception to this pattern appears in \VariantImproved{} variants with only 10 \tries{}:
on \DatasetsEqBenchEs{} projects,
\VariantImprovedA{} achieves only 2.4--2.8\% relative improvement
versus 5.6--6.9\% for \VariantImprovedB{} and 6.0--7.8\% for \VariantImprovedC{}.
This comparatively low increase in detection rates likely stems from boundary-focused generation
consuming most of the limited \tries{} of this variant,
leaving insufficient attempts for testing intermediate values.
With more \tries{}, \VariantImproved{} variants perform comparably to or better than \VariantNaive{} variants
as enough attempts remain for both boundary and non-boundary testing.
\DatasetsCommonsEs{} projects likely show less pronounced degradation,
because their more complex constraints (Section~\ref{sec:boundary-detection-effectiveness})
cause more boundary inputs to fail filtering,
forcing the algorithm to explore non-boundary values sooner.

Combining short test generation with subsequent generalization
can outperform longer generation alone.
On \DatasetsEqBenchEs{} projects,
1-second \ToolEvoSuite{} generation followed by \VariantNaiveC{} generalization
achieves 52.0\% detection,
surpassing 60-second generation alone (51.6\%).
Similarly on \DatasetsCommonsEs{},
10-second generation plus \VariantNaiveC{} reaches 58.5\%
versus 58.1\% for 60-second generation.
These comparisons demonstrate that generalization can effectively compensate for reduced generation time.
Section~\ref{sec:execution-efficiency} analyzes the efficiency trade-offs
between different combinations of \ToolEvoSuite{} timeouts and \ToolTeralizer{} \tries{}
in further detail through Pareto front analysis.

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}
To evaluate how capable generalized tests are
at detecting mutants created by different mutation operators,
we compare the detection rates
of \VariantNaiveC{} and \VariantImprovedC{}
to the detection rates achieved by the \VariantInitial{} test suite.
Table \ref{tab:detections-per-mutator}
shows the results of this evaluation,
listing for each mutant:
the total number of occurrences
across the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects;
the mean, minimum, and maximum prevalence per project
relative to the total number of mutants;
the detection rates of the \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} test suites,
as well as how much the two generalized variants improve detection rates relative to \VariantInitial{}.
Results for \VariantNaive{} and \VariantImproved{} generalizations with lower \tries{}
follow the same trends as for \VariantNaiveC{} and \VariantImprovedC{} but are omitted for brevity.
The full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Prevalence of Mutants}
The three most common mutants,
which represent~81.08 \% of total mutants, are
\texttt{Math}~(59.10~\% of mutants),
\texttt{Remove\-Conditional\-OrderElse}~(10.99~\%),
and \texttt{Conditionals\-Boundary}~(10.99~\%) mutants
(prevalence of \texttt{Remove\-Conditional\-Order\-Else}
and \texttt{Conditionals\-Boundary}
is the same because both mutators modify inequality checks).
On the other hand, the three least common mutants are
\texttt{Increments}~(0.52~\%),
\texttt{Boolean\-False\-Return\-Vals}~(0.24~\%),
and \texttt{Empty\-Object\-Return\-Vals}~(0.13~\%) mutants.
Due to these large differences in mutant prevalences,
overall mutation scores are much more strongly affected
by changes in, e.g., \texttt{Math} detection rates
than by changes in, e.g., \texttt{Increments} detection rates.
This holds not only on an overall, aggregate level
(i.e., across all evaluated projects),
but also for each individual project.
After all, prevalence of individual mutants
shows only comparatively small differences across projects,
as highlighted by the minimum and maximum prevalence results.
In other words,
mutants that are common in one project
are also common in the others,
whereas mutants that are uncommon in one project
are also uncommon in the others.
For example, \texttt{Math} mutants represent 52.34~\% of mutants
in the project where they are least common,
but \texttt{Increments} mutants
represent only 0.54~\% of mutants
in the project where they are most common.

\input{tables/tab-detections-per-mutator}

\paragraph{Detection Rate Results}
Detection rates for \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} as well as
detection rate improvements compared to \VariantInitial{}
are shown in the last three columns of the table.
Detection rates exhibit large differences across different mutators.
The highest detection rates of 90\%--99\% are achieved
for mutators that directly modify return values.
The lowest detection rates are observed
for the \texttt{VoidMethodCall} mutator (... \% detection rate), 
which removes calls to methods that do not produce a return value,
and the \texttt{ConditionalsBoundary} mutator (... \% detection rate),
which modifies the boundaries of conditionals by replacing
\texttt{<} with \texttt{<=} and \texttt{>} with \texttt{>=} (or vice-versa).

\VariantNaive{} achieves the largest improvements over \VariantInitial{}
for the ... (...), ... (...), and ... (...) mutations.
However, \VariantNaive{} does not improve detection rates
for mutants created by the ... mutators.
(@TODO: Possible explanations: (i) high initial detection rates, (ii) new assertions needed.)
\VariantImproved{} achieves the largest improvements
for the ... (...), ... (...), and ... (...) mutations.
It does not achieve any improvements
for mutants created by the ... mutators.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
(@TODO: Check the results for \VariantImprovedA{}. Those might be much worse for mutants that don't relate to conditionals.)
Full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Comparison of \VariantNaiveC{} and \VariantImprovedC{}}
\VariantImprovedC{} outperforms \VariantNaiveC{} in terms of detection rate
for 7 of 12 mutators,
is on par for 4 of 12 mutators (all of which achieve no improvement for either variant),
and underperforms for 1 of 12 mutators.
The largest benefit of \VariantImprovedC{} over \VariantNaiveC{} is observed
for the detection of mutants created by the \texttt{ConditionalsBound} mutator.
Here, \VariantImprovedC{} achieves an increase of ... percentage points over \VariantInitial{},
whereas \VariantNaiveC{} only increases detection rates by ... percentage points.
This suggests that the improvements implemented in \VariantImproved{} variants
to favor boundary values during input generation achieve their intended benefits.

However, \VariantImprovedC{}'s benefits for the detection rate of \texttt{ConditionalsBoundary} mutants
appear to come at the cost of decreased detection rates for \texttt{Math} mutants.
Since the only difference between the two variants is the way in which test inputs are selected
--- \VariantNaive{} variants select inputs randomly from the given input partition; 
\VariantImproved{} variants start with inputs at the edges of the partition  ---
this suggests that focusing too much on boundary values
can have detrimental effects for the detection of mutants
that are not related to input boundaries.
Due to the high prevalence of \texttt{Math} mutants,
this can also result in an \emph{overall} decrease in detection rates.

To counteract the detrimental effects on
detection rates of \texttt{Math} mutants
incurred by the input selection procedure used by \VariantImproved{} variants,
the number of \tries{} could, for example,
be increased for the \VariantImproved{} variants
to adjust for the additional \tries{} spent on boundary testing
while keeping non-boundary testing at \VariantNaive{} levels.
More sophisticated input selection approaches
that aim to better balance boundary and non-boundary testing
without large increases to the number of executed \tries{}
could perhaps further increase detection rates
while keeping the runtime impact
resulting from a higher number of \tries{} to a minimum.
For a more in-depth evaluation of runtime requirements
across different variants and numbers of \tries{}
see Section~\ref{sec:runtime-eval}.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

@TODO: Add intro. Basically: Offer more thorough explanations
why \VariantImproved{} (sometimes?) performs better than \VariantNaive{}
on the \DatasetsEqBenchEs{} projects, but not on the \DatasetsCommonsEs{} ones.

\paragraph{Evaluation Metrics}
Table~\ref{tab:mutation-detection-comparison} shows 
the model properties of mutants that are (not) detected
by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
The term \emph{model}, in this case, refers to an 
input specification extracted by \ToolTeralizer{}.
Therefore, the (input) model(s) of a mutant
are all input specifications
that describe an input partition
which contains at least one set of inputs
that cause the mutant to be exercised during test execution.
In other words, the model(s) of a mutant
are all input specifications
that were extracted by \ToolTeralizer{}'s
specification extraction step (see Section~\ref{sec:specification-extraction})
during execution of a test that covers the corresponding mutant.

Included model properties are
the median number of model operations
and the median number of model constraints.
%The number of (nested) operations is given by
%the total number of operators in the model.
%The number of constraints is determined
%by counting the number of boolean conditions or variables
%connected by \texttt{\&\&} operators.
%There can never be \texttt{||} operators in a model
%because the two operands on both sides of the \texttt{||}
%represent different input partitions and, therefore,
%describe different models.
%
In addition to the model properties,
the last column of Table~\ref{tab:mutation-detection-comparison}
lists the mean and median percentage of constraints
that are used by the created property-based tests
when generating inputs for test execution.
As described in more detail in Section~\ref{sec:improved-generalization}, 
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during \ToolPit{}'s default value filtering step.

For example, take the following (Java represented) model:
\texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains 5 operators
(i.e., \texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and \texttt{\&\&}),
so has a total of 5 (nested) operations.
Furthermore, the model's contains 3 constraints:
(i) \texttt{a < 0}, (ii) \texttt{a == (b + 1)}, and (iii) \texttt{c}.
Constraints (i) and (iii) are used by the \VariantImproved{} variants.
Constraint (ii) is not used
because it contains the compound term \texttt{b + 1}.

\input{tables/tab-mutation-detection-comparison}

\paragraph{Model Properties of Detected vs.\ Undetected Mutants}
As shown in Table~\ref{tab:mutation-detection-comparison},
undetected mutants often have more complex models
%(i.e., are easier to reach during testing)
than detected ones.
For example, in the \DatasetsEqBenchEs{} projects,
mean and median operation counts
are around 1.2--1.7 times larger
for undetected mutants than for detected ones
(e.g., mean of 231 vs.\ 139 and median of 15 vs.\ 9 for \DatasetEqBenchB{}).
Operation counts in the \DatasetsCommonsEs{} projects
show even more pronounced differences,
with values of undetected mutants being 1.3--3 times larger
than those of detected ones
(e.g., mean of 45 vs.\ 15 and median of 12 vs.\ 7 for \DatasetCommonsB{}).
The only project where models of undetected mutants
have lower median operation counts than detected ones (10 vs.\ 11)
is \DatasetCommonsDev{} with its developer written test suite.
However, mean operation counts are still larger for undetected mutants
at 173 vs.\ 107 for detected ones.
Constraint counts show a similar trend to operation counts,
with mean values being 1.0--1.7 times as large
and median values being 1.0--2.5 times as large
for undetected mutants as for detected ones.
Values for \DatasetCommonsDev{} are very different from
the other \DatasetsCommonsEs{} projects because it 
covers a much smaller subset of mutants,
as previously discussed in \ref{sec:included-mutants}.

\paragraph{Constraints Used by \VariantImproved{} Generalization}
The mean (median) number of used constraints across all projects
ranges from 25--69 \% (75--100 \%) for detected mutants
and 10--67 \% (50--100 \%) for undetected ones,
with usage rates being consistently higher
for detected mutants than for undetected ones.
Because the current implementation of \ToolTeralizer{} focuses on simple constraints,
this provides further evidence
that models of undetected mutants are not only larger than those of detected ones
(i.e., contain more operations and constraints)
but also more complex
(e.g., contain more compound terms,
show more common use of non-numeric data types,
and use more math functions that are not fully modeled by SPF).
This suggests that improving \ToolTeralizer{}'s support
for more complex constraints
holds further potential for additional increases in mutation detection rates.

Furthermore, the high constraint usage rates
as well as the comparatively small operation and constraint counts
of the \DatasetsEqBenchEs{} projects suggest that it is relatively easy
to find input values that match the models.
This could explain, at least in part,
why \VariantImproved{} generalization variants are more effective
relative to the corresponding \VariantNaive{} variants
when used in the \DatasetsCommons{} projects
than in the \DatasetsEqBenchEs{} projects
(as seen in Figure~\ref{fig:mutation-detection-results}).
After all, if mutated input partition boundaries are easy to identify,
there is a higher likelihood that they will be covered
even without the more sophisticated value selection
employed by \VariantImproved{} variants.
Thus, taking into account the complexity of the underlying models
during generalization might offer further opportunities
to improve the effectiveness and/or efficiency
of test generalization approaches such as \ToolTeralizer{}.

@TODO: Add answer for RQ1.
