\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

Mutation testing provides a rigorous measure of test effectiveness
by evaluating a test suite's ability to detect deliberately introduced faults.
For \ToolTeralizer{}, mutation scores reveal whether testing additional inputs
within existing execution paths achieves the intended improvement in fault detection capabilities.
Section~\ref{sec:included-mutants} first establishes which tests and mutants are included in the evaluation,
revealing systematic differences between generated and developer-written test suites that influence generalization outcomes.
Section~\ref{sec:overall-detection-rates} then quantifies detection improvements
across projects and variants,
demonstrating that generalization consistently improves mutation scores,
though with varying improvement magnitudes.
Section~\ref{sec:detection-rates-per-mutator} dissects these improvements by mutation operator,
uncovering how much detection of different mutants benefits from generalization.
Finally, Section~\ref{sec:boundary-detection-effectiveness} provides a
detailed analysis of the \VariantImproved{} generalization variant,
demonstrating that its effectiveness correlates with constraint complexity,
i.e., projects with more complex input constraints show greater improvements
from constraint-aware input generation.

\subsubsection{Included Mutants}
\label{sec:included-mutants}

To evaluate mutation score effects,
we compare \ToolPit{} results for \VariantInitial{} variants
against \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.
This comparison isolates generalization effects by using \VariantInitial{} as the baseline,
ensuring that differences stem solely from the presence or absence of generalized test cases.
Comparison to \VariantOriginal{} would not isolate generalization effects
because the original test suite contains tests excluded during
test analysis and specification extraction (Section~\ref{sec:all-filtering})â€”before
any transformation to property-based tests occurs.

Test inclusion rates reveal fundamental differences between generated and developer-written test suites
that shape generalization outcomes.
EvoSuite-generated projects achieve 83.4--85.1\% inclusion rates across variants,
while \DatasetCommonsDev{} includes only 63.6\% of original test methods
(Table~\ref{tab:mutants-per-project}).
This 20-percentage-point gap reflects how different test creation approaches
align with \ToolTeralizer{}'s current processing capabilities.

EvoSuite generates tests with characteristics that facilitate automated analysis:
simple assertion patterns that compare single values,
explicit parameter passing without complex setup logic,
and method-level focus that isolates individual behaviors.
Developer-written tests, conversely, combine multiple scenarios within single methods,
employ elaborate fixture setup spanning multiple classes,
and validate system state changes through sequences of operations.
While these patterns enable comprehensive integration testing,
they create barriers for specification extraction, explaining
why over one-third of developer tests cannot be generalized.

The three primary exclusion causes reflect \ToolTeralizer{}'s current limitations:
(i)~tests without assertions provide no specification to generalize,
(ii)~tests where no tested method can be identified typically involve
program constructs that \ToolTeralizer{}'s simple static analysis cannot resolve,
such as loops in test code or modifications of object state rather than direct return values,
and (iii)~tests exercising methods without generalizable parameters have no numeric
or boolean inputs to vary.
Section~\ref{sec:filtering-eval} quantifies how frequently all exclusion causes occur,
providing guidance for extending generalization capabilities.

Mutation coverage patterns highlight further differences across the project environments.
\ToolPit{} generates approximately 8,000 total mutants for Apache Commons projects
and 23,000--24,000 for EqBench projects (Table~\ref{tab:mutants-per-project}).
Covered mutants, i.e., those mutants that are executed by at least one included test, range
from 64.4\% for \DatasetCommonsDev{} to 97.1\% for \DatasetCommonsC{}.
This coverage disparity primarily reflects dataset construction rather than test quality:
\DatasetCommonsDev{} includes only tests directly covering the extracted utility methods
(but no tests for transitively called methods),
while EvoSuite variants generate comprehensive suites for all included code.
This focused scope reflects our intent to evaluate code amenable to generalization in this part of the dataset,
as discussed in Section~\ref{sec:apache-commons-utils}.

With the evaluation scope established, we now analyze how effectively
the generalized tests detect mutations within the included subset.

\input{tables/tab-mutants-per-project}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}

Generalization improves mutation detection across all evaluated projects,
though the magnitude varies by project type
(Figure~\ref{fig:mutation-detection-results}).
\DatasetsEqBenchEs{} projects show the largest improvements:
detection rates increase from 48.1--51.6\% to 49.5--55.0\%
across different \tries{} and generalization settings,
representing absolute improvements of 1.2--3.9 percentage points
(2.4--8.2\% relative increase).
\DatasetsCommonsEs{} projects improve by 0.82--1.33 percentage points
(1.4--2.3\% relative increase),
while \DatasetCommonsDev{} improves by only 0.05--0.07 percentage points
from its 80.4\% baseline.

In general, two factors strongly affect achievable generalization improvements:
initial test suite strength and input constraint complexity.
\DatasetsEqBenchEs{} projects offer the most favorable conditions.
Their EvoSuite-generated tests leave more room for enhancement
(starting from 48.1--51.6\% detection rates)
than the thorough \DatasetCommonsDev{} test suite,
and the \DatasetEqBench{} benchmark's input constraints
are simpler than the input constraints of \DatasetsCommons{},
thus making valid input generation easier
(as discussed in more detail in Section~\ref{sec:boundary-detection-effectiveness}).
\DatasetsCommonsEs{} projects face one additional challenge:
while they also start from EvoSuite-generated tests,
the \DatasetsCommons{} methods involve more complex input specifications,
making it harder to generate inputs that satisfy input constraints.
\DatasetCommonsDev{} faces both challenges:
the mature developer-written test suite already achieves 80.4\% detection
through decades of refinement in these widely-used libraries,
and it shares the same complex constraint characteristics as other \DatasetsCommons{} variants.
This leaves little opportunity for automated improvement.

Comparing the relative effectiveness \VariantNaive{} and \VariantImproved{}
shows opposite results across \DatasetsEqBenchEs{} and \DatasetsCommonsEs{}.
On \DatasetsEqBenchEs{} projects, \VariantNaive{} variants consistently
outperform \VariantImproved{} for all \tries{} settings,
with gaps ranging from 0.17--1.21 percentage points.
This advantage is most pronounced with limited \tries{}:
\VariantNaiveA{} achieves 50.67--52.99\% detection rate
while \VariantImprovedA{} reaches only 49.46--51.86\%.
In contrast, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 comparisons,
with \VariantImprovedC{} detecting 57.97--59.45\% of mutants
compared to \VariantNaiveC{}'s 58.00--59.35\%.
\DatasetCommonsDev{} shows no meaningful differences between approaches,
with all variants achieving approximately 80.4\% detection.
These opposite results reflect the following patterns:
\VariantNaive{}'s better performance on \texttt{Math} mutations (59.1\% of all mutants)
versus \VariantImproved{}'s better detection of most other mutation types
as well as more effective handling of complex constraints.
In \DatasetsEqBenchEs{}, the high prevalence of \texttt{Math} mutations
and low constraint complexity favor \VariantNaive{},
while in \DatasetsCommonsEs{}, \VariantImproved{}'s
advantages on other mutations and constraint handling overcome
its \texttt{Math} mutant detection disadvantage.
We investigate the mechanisms behind these results in further detail in
Sections~\ref{sec:detection-rates-per-mutator} and \ref{sec:boundary-detection-effectiveness}.

Higher \tries{} settings improve detection rates but with diminishing returns.
The increase from 10 to 50 \tries{} typically produces larger gains
than increasing from 50 to 200 \tries{}.
One notable exception to this pattern appears in \VariantImproved{} variants with only 10 \tries{}:
on \DatasetsEqBenchEs{} projects,
\VariantImprovedA{} achieves only 2.4--2.8\% relative improvement
versus 5.6--6.9\% for \VariantImprovedB{} and 6.0--7.8\% for \VariantImprovedC{}.
This comparatively low increase in detection rates likely stems from boundary-focused generation
consuming most of the limited \tries{} of this variant,
leaving insufficient attempts for testing intermediate values.
With more \tries{}, \VariantImproved{} variants perform comparably to or better than \VariantNaive{} variants
as enough attempts remain for both boundary and non-boundary testing.
\DatasetsCommonsEs{} projects likely show less pronounced degradation,
because their more complex constraints (Section~\ref{sec:boundary-detection-effectiveness})
cause more boundary inputs to fail filtering,
forcing the algorithm to explore non-boundary values sooner.

Combining short test generation with subsequent generalization
can outperform longer generation alone.
On \DatasetsEqBenchEs{} projects,
1-second \ToolEvoSuite{} generation followed by \VariantNaiveC{} generalization
achieves 52.0\% detection,
surpassing 60-second generation alone (51.6\%).
Similarly on \DatasetsCommonsEs{},
10-second generation plus \VariantNaiveC{} reaches 58.5\%
versus 58.1\% for 60-second generation.
These comparisons demonstrate that generalization can effectively compensate for reduced generation time.
Section~\ref{sec:execution-efficiency} analyzes the efficiency trade-offs
between different combinations of \ToolEvoSuite{} timeouts and \ToolTeralizer{} \tries{}
in further detail through Pareto front analysis.

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}

Generalization effectiveness varies significantly across mutation operators,
revealing which fault types benefit most from additional test inputs
(Table~\ref{tab:detections-per-mutator}).
Three operators dominate the mutation landscape:
\texttt{Math} (59.1\% of all mutants),
\texttt{Conditionals\-Boundary} (11.0\%),
and \texttt{Remove\-Conditional\-Order\-Else} (11.0\%).
In contrast, the least common operators, i.e., \texttt{Increments} (0.52\%),
\texttt{Boolean\-False\-ReturnVals} (0.24\%),
and \texttt{Empty\-Object\-Return\-Vals} (0.13\%), together
account for less than 1\% of all mutants.
This large difference in mutant prevalence means that
improvements to \texttt{Math} detection rates
have proportionally larger impact on overall mutation scores
than improvements to less commonly occurring mutation types.

\input{tables/tab-detections-per-mutator}

Initial detection rates reveal a clear pattern:
return value mutations are caught
in the large majority of cases (87.9--98.8\% detection),
while behavioral mutations prove more elusive.
For example, \texttt{VoidMethodCall} mutations achieve a detection rate of only 25.0\%
because removing void method calls typically affects only internal state
or produces side effects that are not directly verified by test assertions.
\texttt{ConditionalsBoundary} mutations fare only slightly better at 27.7\% total detection rate,
highlighting an opportunity for automated improvement.
After all, the \texttt{Conditionals\-Boundary} mutations
change comparison operators at partition boundaries,
which is exactly where \VariantImproved{} generalization
aims to generate additional test inputs.

\VariantNaiveC{} achieves its largest improvements on
\texttt{Math} (4.0~percentage points),
\texttt{RemoveConditionalEqualElse} (2.1~percentage points),
and \texttt{InvertNegs} (1.7~percentage points).
The success with \texttt{RemoveConditionalEqualElse} stems from
testing diverse inputs that trigger both branches of equality checks,
exposing mutations that force false branches.
\texttt{Math} mutation detection similarly benefits from a more diverse
range of tested input values because arithmetic operations often produce
different results across input ranges, exposing mutations that might
coincidentally produce correct results for any individual value.
In contrast, 4 of 5 return value mutators show zero improvement:
\texttt{Null\-Return\-Vals}, \texttt{Boolean\-True\-Return\-Vals},
\texttt{Boolean\-False\-ReturnVals}, and \texttt{EmptyObjectReturnVals}.
These already achieve high detection rates in the \VariantInitial{} variant (87.9--98.8\%)
because return value mutations often directly violate test assertions,
leaving little room for improvement through input variation.

\VariantImprovedC{} demonstrates different strengths through boundary-aware generation.
Comparing the two variants across all 12 mutation operators:
\VariantImprovedC{} outperforms \VariantNaiveC{} for 7 operators,
achieves the same detection rate for 4 operators (the zero-improvement return value mutations),
and underperforms for only 1 operator (\texttt{Math}).
The largest advantage is observed for \texttt{Conditionals\-Boundary} detection,
where \VariantImprovedC{} achieves 2.5 percentage points improvement
compared to \VariantNaiveC{}'s 1.2 percentage points.
These results confirm that the boundary-focused input generation strategy
followed by \VariantImproved{} variants achieves its intended purpose,
and suggests that testing at partition boundaries may also provide slight benefits
for some mutators that don't directly modify partition boundaries.

The \texttt{Math} mutation results highlight the inherent trade-off
in boundary versus non-boundary testing:
\VariantImprovedC{} achieves 3.4 percentage points improvement
compared to \VariantNaiveC{}'s 4.0 percentage points.
The \VariantImprovedC{} variant achieves smaller improvements here
because boundary-focused input generation produces less diverse arithmetic inputs,
concentrating on boundary values rather than exploring the full numeric range
where arithmetic mutations might create more varied outputs.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this difference significantly impacts overall mutation scores.
To counteract these detrimental effects,
\VariantImproved{} variants could use higher \tries{} settings
to maintain non-boundary input coverage of \VariantNaive{}
while adding additional boundary coverage.
Alternatively, more sophisticated input selection strategies
could be used to achieve a better balance between
boundary and non-boundary testing even at lower \tries{} settings,
thus avoiding the runtime cost of higher \tries{}.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
For example, \VariantImprovedA{} achieves expected results for most mutation operators,
including its characteristic advantage for \texttt{ConditionalsBoundary} mutations (2.0\% vs 0.9\% for \VariantNaiveA{}).
The exception is \texttt{Math} mutation detection,
where \VariantImprovedA{} achieves only 1.1\% improvement compared to 2.8\% for \VariantNaiveA{}.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this explains the low overall detection rate of \VariantImprovedA{} on \DatasetsEqBenchEs{} projects
observed in Section~\ref{sec:overall-detection-rates}.
With 50 \tries{}, the \texttt{Math} detection gap is noticeably smaller
(\VariantImprovedB{} achieves 3.1\% versus \VariantNaiveB{}'s 3.6\%),
confirming that limited \tries{} severely constrain arithmetic diversity
only when boundary testing consumes most attempts.
Full results for all variants are available in our replication package~\cite{replicationpackage}.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

@TODO: Add intro. Basically: Offer more thorough explanations
why \VariantImproved{} (sometimes?) performs better than \VariantNaive{}
on the \DatasetsEqBenchEs{} projects, but not on the \DatasetsCommonsEs{} ones.

\paragraph{Evaluation Metrics}
Table~\ref{tab:mutation-detection-comparison} shows 
the model properties of mutants that are (not) detected
by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
The term \emph{model}, in this case, refers to an 
input specification extracted by \ToolTeralizer{}.
Therefore, the (input) model(s) of a mutant
are all input specifications
that describe an input partition
which contains at least one set of inputs
that cause the mutant to be exercised during test execution.
In other words, the model(s) of a mutant
are all input specifications
that were extracted by \ToolTeralizer{}'s
specification extraction step (see Section~\ref{sec:specification-extraction})
during execution of a test that covers the corresponding mutant.

Included model properties are
the median number of model operations
and the median number of model constraints.
%The number of (nested) operations is given by
%the total number of operators in the model.
%The number of constraints is determined
%by counting the number of boolean conditions or variables
%connected by \texttt{\&\&} operators.
%There can never be \texttt{||} operators in a model
%because the two operands on both sides of the \texttt{||}
%represent different input partitions and, therefore,
%describe different models.
%
In addition to the model properties,
the last column of Table~\ref{tab:mutation-detection-comparison}
lists the mean and median percentage of constraints
that are used by the created property-based tests
when generating inputs for test execution.
As described in more detail in Section~\ref{sec:improved-generalization}, 
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during \ToolPit{}'s default value filtering step.

For example, take the following (Java represented) model:
\texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains 5 operators
(i.e., \texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and \texttt{\&\&}),
so has a total of 5 (nested) operations.
Furthermore, the model's contains 3 constraints:
(i) \texttt{a < 0}, (ii) \texttt{a == (b + 1)}, and (iii) \texttt{c}.
Constraints (i) and (iii) are used by the \VariantImproved{} variants.
Constraint (ii) is not used
because it contains the compound term \texttt{b + 1}.

\input{tables/tab-mutation-detection-comparison}

\paragraph{Model Properties of Detected vs.\ Undetected Mutants}
As shown in Table~\ref{tab:mutation-detection-comparison},
undetected mutants often have more complex models
%(i.e., are easier to reach during testing)
than detected ones.
For example, in the \DatasetsEqBenchEs{} projects,
mean and median operation counts
are around 1.2--1.7 times larger
for undetected mutants than for detected ones
(e.g., mean of 231 vs.\ 139 and median of 15 vs.\ 9 for \DatasetEqBenchB{}).
Operation counts in the \DatasetsCommonsEs{} projects
show even more pronounced differences,
with values of undetected mutants being 1.3--3 times larger
than those of detected ones
(e.g., mean of 45 vs.\ 15 and median of 12 vs.\ 7 for \DatasetCommonsB{}).
The only project where models of undetected mutants
have lower median operation counts than detected ones (10 vs.\ 11)
is \DatasetCommonsDev{} with its developer written test suite.
However, mean operation counts are still larger for undetected mutants
at 173 vs.\ 107 for detected ones.
Constraint counts show a similar trend to operation counts,
with mean values being 1.0--1.7 times as large
and median values being 1.0--2.5 times as large
for undetected mutants as for detected ones.
Values for \DatasetCommonsDev{} are very different from
the other \DatasetsCommonsEs{} projects because it 
covers a much smaller subset of mutants,
as previously discussed in \ref{sec:included-mutants}.

\paragraph{Constraints Used by \VariantImproved{} Generalization}
The mean (median) number of used constraints across all projects
ranges from 25--69 \% (75--100 \%) for detected mutants
and 10--67 \% (50--100 \%) for undetected ones,
with usage rates being consistently higher
for detected mutants than for undetected ones.
Because the current implementation of \ToolTeralizer{} focuses on simple constraints,
this provides further evidence
that models of undetected mutants are not only larger than those of detected ones
(i.e., contain more operations and constraints)
but also more complex
(e.g., contain more compound terms,
show more common use of non-numeric data types,
and use more math functions that are not fully modeled by SPF).
This suggests that improving \ToolTeralizer{}'s support
for more complex constraints
holds further potential for additional increases in mutation detection rates.

Furthermore, the high constraint usage rates
as well as the comparatively small operation and constraint counts
of the \DatasetsEqBenchEs{} projects suggest that it is relatively easy
to find input values that match the models.
This could explain, at least in part,
why \VariantImproved{} generalization variants are more effective
relative to the corresponding \VariantNaive{} variants
when used in the \DatasetsCommons{} projects
than in the \DatasetsEqBenchEs{} projects
(as seen in Figure~\ref{fig:mutation-detection-results}).
After all, if mutated input partition boundaries are easy to identify,
there is a higher likelihood that they will be covered
even without the more sophisticated value selection
employed by \VariantImproved{} variants.
Thus, taking into account the complexity of the underlying models
during generalization might offer further opportunities
to improve the effectiveness and/or efficiency
of test generalization approaches such as \ToolTeralizer{}.

@TODO: Add answer for RQ1.
