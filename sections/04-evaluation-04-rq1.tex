\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

Mutation testing provides a rigorous measure of test effectiveness
by evaluating a test suite's ability to detect deliberately introduced faults.
For \ToolTeralizer{}, mutation scores reveal whether testing additional inputs
within existing execution paths achieves the intended improvement in fault detection capabilities.
Section~\ref{sec:included-mutants} first establishes which tests and mutants are included in the evaluation,
revealing systematic differences between generated and developer-written test suites that influence generalization outcomes.
Section~\ref{sec:overall-detection-rates} quantifies detection improvements
across projects and variants,
demonstrating that generalization improves mutation scores
for \DatasetsEqBenchEs{} and \DatasetsCommons{} projects, albeit with varying improvement magnitudes.
Section~\ref{sec:detection-rates-per-mutator} dissects these improvements by mutation operator,
uncovering how much detection of different mutants benefits from generalization.
Finally, Section~\ref{sec:boundary-detection-effectiveness} provides a
detailed analysis of the \VariantImproved{} generalization variant,
demonstrating that its effectiveness correlates with constraint complexity,
i.e., projects with more complex input constraints show greater improvements
from constraint-aware input generation.

\subsubsection{Included Mutants}
\label{sec:included-mutants}

To evaluate mutation detection effects,
we compare \ToolPit{} results for \VariantInitial{} variants
against \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.
This comparison isolates generalization effects by using \VariantInitial{} as the baseline,
ensuring that differences stem solely from the presence or absence of generalized test cases.
Comparison to \VariantOriginal{} would not isolate generalization effects
because the original test suite contains tests excluded during
test analysis and specification extraction (Section~\ref{sec:all-filtering}),
i.e., before any transformation to property-based tests occurs.

Test inclusion rates reveal fundamental differences between generated and developer-written test suites
that shape generalization outcomes.
EvoSuite-generated projects achieve 83.4--85.1\% inclusion rates across variants,
while \DatasetCommonsDev{} includes only 63.6\% of original test methods
(Table~\ref{tab:mutants-per-project}).
This 20-percentage-point gap reflects how different test creation approaches
align with \ToolTeralizer{}'s current processing capabilities.

\input{tables/tab-mutants-per-project}

EvoSuite generates tests with characteristics that facilitate automated analysis:
simple assertion patterns that compare single values,
explicit parameter passing without complex setup logic,
and method-level focus that isolates individual behaviors.
Developer-written tests, conversely, combine multiple scenarios within single methods,
employ elaborate fixture setup spanning multiple classes,
and validate system state changes through sequences of operations.
While these patterns enable comprehensive integration testing,
they create barriers for specification extraction, explaining
why over one-third of developer tests cannot be generalized.

The three primary exclusion causes reflect \ToolTeralizer{}'s current limitations:
(i)~tests without assertions provide no specification to generalize,
(ii)~tests where no tested method can be identified typically involve
program constructs that \ToolTeralizer{}'s simple static analysis cannot resolve,
such as loops in test code or modifications of object state rather than direct return values,
and (iii)~tests exercising methods without generalizable parameters have no numeric
or boolean inputs to vary.
Section~\ref{sec:filtering-eval} quantifies how frequently all exclusion causes occur,
providing guidance for extending generalization capabilities.

Mutation coverage patterns highlight further differences across the project environments.
\ToolPit{} generates approximately 8,000 total mutants for Apache Commons projects
and 23,000--24,000 for EqBench projects (Table~\ref{tab:mutants-per-project}).
Covered mutants, i.e., those mutants that are executed by at least one included test, range
from 64.4\% for \DatasetCommonsDev{} to 97.1\% for \DatasetCommonsC{}.
This coverage disparity primarily reflects dataset construction rather than test quality:
\DatasetCommonsDev{} includes only tests directly covering the extracted utility methods
(but no tests for transitively called methods),
while EvoSuite variants generate comprehensive suites for all included code.
This focused scope reflects our intent to evaluate code amenable to generalization in this part of the dataset,
as discussed in Section~\ref{sec:apache-commons-utils}.

% With the evaluation scope established, we now analyze how effectively
% the generalized tests detect mutations within the included subset.

\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}

Generalization improves mutation detection across all evaluated projects,
though the magnitude varies by project type
(Figure~\ref{fig:mutation-detection-results}).
\DatasetsEqBenchEs{} projects show the largest improvements:
detection rates increase from 48.1--51.6\% to 49.5--55.0\%
across different \tries{} and generalization settings,
representing absolute improvements of 1.2--3.9 percentage points
(2.4--8.2\% relative increase).
\DatasetsCommonsEs{} projects improve by 0.82--1.33 percentage points
(1.4--2.3\% relative increase),
while \DatasetCommonsDev{} improves by only 0.05--0.07 percentage points
from its 80.4\% baseline.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

In general, two factors strongly affect achievable generalization improvements:
initial test suite strength and input constraint complexity.
\DatasetsEqBenchEs{} projects offer the most favorable conditions.
Their EvoSuite-generated tests leave more room for enhancement
(starting from 48.1--51.6\% detection rates)
than the thorough \DatasetCommonsDev{} test suite,
and the \DatasetEqBench{} benchmark's input constraints
are simpler than the input constraints of \DatasetsCommons{},
thus making valid input generation easier
(as discussed in more detail in Section~\ref{sec:boundary-detection-effectiveness}).
\DatasetsCommonsEs{} projects face one additional challenge:
while they also start from EvoSuite-generated tests,
the \DatasetsCommons{} methods involve more complex input specifications,
making it harder to generate inputs that satisfy input constraints.
\DatasetCommonsDev{} faces both challenges:
the mature developer-written test suite already achieves 80.4\% detection
through decades of refinement in these widely-used libraries,
and it shares the same complex constraint characteristics as other \DatasetsCommons{} variants.
This leaves little opportunity for automated improvement.

Comparing the relative effectiveness of \VariantNaive{} and \VariantImproved{}
shows opposite results across \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
On \DatasetsEqBenchEs{} projects, \VariantNaive{} variants
outperform \VariantImproved{} for all \tries{} settings,
with gaps ranging from 0.17--1.21 percentage points.
This advantage is most pronounced with limited \tries{}:
\VariantNaiveA{} achieves 50.67--52.99\% detection rate
while \VariantImprovedA{} reaches only 49.46--51.86\%.
In contrast, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 comparisons,
with \VariantImprovedC{} detecting 57.97--59.45\% of mutants
compared to \VariantNaiveC{}'s 58.00--59.35\%.
These opposite results reflect the following patterns:
\VariantNaive{}'s better performance on \texttt{Math} mutations (59.1\% of all mutants)
versus \VariantImproved{}'s better detection of most other mutation types
as well as more effective handling of complex constraints.
In \DatasetsEqBenchEs{}, the high prevalence of \texttt{Math} mutations
and low constraint complexity favor \VariantNaive{},
while in \DatasetsCommonsEs{}, \VariantImproved{}'s
advantages on other mutations and constraint handling overcome
its \texttt{Math} mutant detection disadvantage.
We investigate the mechanisms behind these results in further detail in
Sections~\ref{sec:detection-rates-per-mutator} and \ref{sec:boundary-detection-effectiveness}.
\DatasetCommonsDev{} shows no meaningful differences between approaches,
with all variants achieving approximately 80.4\% detection.

Higher \tries{} settings showed improved detection rates with diminishing returns.
The increase from 10 to 50 \tries{} typically produces larger gains
than increasing from 50 to 200 \tries{}.
One notable exception to this pattern appears in \VariantImproved{} variants with only 10 \tries{}:
on \DatasetsEqBenchEs{} projects,
\VariantImprovedA{} achieves only 2.4--2.8\% relative improvement
versus 5.6--6.9\% for \VariantImprovedB{} and 6.0--7.8\% for \VariantImprovedC{}.
This comparatively low increase in detection rates likely stems from boundary-focused generation
consuming most of the limited \tries{} of this variant,
leaving insufficient attempts for testing intermediate values.
With more \tries{}, \VariantImproved{} variants perform comparably to or better than \VariantNaive{} variants
as enough attempts remain for both boundary and non-boundary testing.
\DatasetsCommonsEs{} projects show less pronounced degradation at low \tries{},
because their more complex constraints (Section~\ref{sec:boundary-detection-effectiveness})
cause more boundary inputs to fail filtering,
forcing the algorithm to explore non-boundary values sooner.

Combining short test generation with subsequent generalization
can outperform longer generation alone.
On \DatasetsEqBenchEs{} projects,
1-second \ToolEvoSuite{} generation followed by \VariantNaiveC{} generalization
achieves 52.0\% detection,
surpassing 60-second generation alone (51.6\%).
Similarly on \DatasetsCommonsEs{},
10-second generation plus \VariantNaiveC{} reaches 58.5\%
versus 58.1\% for 60-second generation.
These comparisons demonstrate that generalization can effectively compensate for reduced generation time.
Section~\ref{sec:execution-efficiency} analyzes the efficiency trade-offs
between different combinations of \ToolEvoSuite{} timeouts and \ToolTeralizer{} \tries{}
in further detail through Pareto front analysis.

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}

Generalization effectiveness varies significantly across mutation operators,
revealing which fault types benefit most from additional test inputs
(Table~\ref{tab:detections-per-mutator}).
Three operators dominate the mutation landscape:
\texttt{Math} (59.1\% of all mutants),
\texttt{Conditionals\-Boundary} (11.0\%),
and \texttt{Remove\-Conditional\-Order\-Else} (11.0\%).
In contrast, the least common operators, i.e., \texttt{Increments} (0.52\%),
\texttt{Boolean\-False\-ReturnVals} (0.24\%),
and \texttt{Empty\-Object\-Return\-Vals} (0.13\%), together
account for less than 1\% of all mutants.
This large difference in mutant prevalence means that
improvements to \texttt{Math} detection rates
have proportionally larger impact on overall mutation scores
than improvements to less commonly occurring mutation types.

\input{tables/tab-detections-per-mutator}

Initial detection rates reveal a clear pattern:
return value mutations are caught
in the large majority of cases (87.9--98.8\% detection),
while behavioral mutations prove more elusive.
For example, \texttt{VoidMethodCall} mutations achieve a detection rate of only 25.0\%
because removing void method calls typically affects only internal state
or produces side effects that are not directly verified by test assertions.
\texttt{ConditionalsBoundary} mutations fare only slightly better at 27.7\% total detection rate,
highlighting an opportunity for automated improvement.
After all, the \texttt{Conditionals\-Boundary} mutations
change comparison operators at partition boundaries,
which is exactly where \VariantImproved{} generalization
aims to generate additional test inputs.

\VariantNaiveC{} achieves its largest improvements on
\texttt{Math} (4.0~percentage points),
\texttt{RemoveConditionalEqualElse} (2.1~percentage points),
and \texttt{InvertNegs} (1.7~percentage points).
The success with \texttt{RemoveConditionalEqualElse} stems from
testing diverse inputs that trigger both branches of equality checks,
exposing mutations that force false branches.
\texttt{Math} mutation detection similarly benefits from a more diverse
range of tested input values because arithmetic operations often produce
different results across input ranges, exposing mutations that might
coincidentally produce correct results for any individual value.
In contrast, 4 of 5 return value mutators show zero improvement:
\texttt{Null\-Return\-Vals}, \texttt{Boolean\-True\-Return\-Vals},
\texttt{Boolean\-False\-ReturnVals}, and \texttt{EmptyObjectReturnVals}.
These already achieve high detection rates in the \VariantInitial{} variant (87.9--98.8\%)
because return value mutations often directly violate test assertions,
leaving little room for improvement through input variation.
Increasing detection rates beyond this high starting point
would likely require additional assertions to be introduced,
rather than test inputs to be varied.

\VariantImprovedC{} demonstrates different strengths than \VariantNaive{}
through boundary-aware input generation.
Comparing the two variants across all 12 mutation operators:
\VariantImprovedC{} outperforms \VariantNaiveC{} for 7 operators,
achieves the same detection rate for 4 operators (the zero-improvement return value mutations),
and underperforms for only 1 operator (\texttt{Math}).
The largest advantage is observed for \texttt{Conditionals\-Boundary} detection,
where \VariantImprovedC{} achieves 2.5 percentage points improvement
compared to \VariantNaiveC{}'s 1.2 percentage points.
These results confirm that the boundary-focused input generation strategy
followed by \VariantImproved{} variants achieves its intended purpose.
Furthermore, the slightly higher detection rates for several other mutators
suggest that testing at partition boundaries may also provide slight benefits
for some mutators that do not directly modify partition boundaries.

The \texttt{Math} mutation results highlight the inherent trade-off
in boundary versus non-boundary testing:
\VariantImprovedC{} achieves 3.4 percentage points improvement
compared to \VariantNaiveC{}'s 4.0 percentage points.
The \VariantImprovedC{} variant achieves smaller improvements here
because boundary-focused input generation produces less diverse arithmetic inputs,
concentrating on boundary values rather than exploring the full numeric range
where arithmetic mutations might create more varied outputs.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this difference significantly impacts overall mutation scores.
To counteract these detrimental effects,
\VariantImproved{} variants could use higher \tries{} settings
to maintain non-boundary input coverage of \VariantNaive{}
while adding additional boundary coverage.
Alternatively, more sophisticated input selection strategies
could be used to achieve a better balance between
boundary and non-boundary testing even at lower \tries{} settings,
thus avoiding the runtime cost of higher \tries{}.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
For example, \VariantImprovedA{} achieves expected results for most mutation operators,
including its characteristic advantage for \texttt{ConditionalsBoundary} mutations (2.0\% vs 0.9\% for \VariantNaiveA{}).
The exception is \texttt{Math} mutation detection,
where \VariantImprovedA{} achieves only 1.1\% improvement compared to 2.8\% for \VariantNaiveA{}.
Given that \texttt{Math} mutations comprise 59.1\% of all mutants,
this explains the low overall detection rate of \VariantImprovedA{} on \DatasetsEqBenchEs{} projects
observed in Section~\ref{sec:overall-detection-rates}.
With 50 \tries{}, the \texttt{Math} detection gap is noticeably smaller
(\VariantImprovedB{} achieves 3.1\% versus \VariantNaiveB{}'s 3.6\%),
confirming that limited \tries{} severely constrain arithmetic diversity
only when boundary testing consumes most attempts.
Full results for all variants are available in our replication package~\cite{replicationpackage}.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

As discussed in Section~\ref{sec:overall-detection-rates},
\VariantNaive{} variants
outperformed \VariantImproved{}
on \DatasetsEqBenchEs{} projects.
However, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 cases.
To provide a more thorough understanding of these contrasting results, we examine
how constraint complexity differs between project types and
what the underlying mechanisms are that explain how complexity affects mutation detection differences.
Table~\ref{tab:mutation-detection-comparison} shows the constraint characteristics
of mutants that are (not) detected by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
Models represent the constraints that inputs must satisfy
to reach each mutant along a specific execution path.
We measure model complexity through operation count (total operators)
and constraint count (individual boolean conditions),
while tracking which percentage of constraints \VariantImproved{} can encode
for use during input generation versus enforce through post-generation filtering.

\input{tables/tab-mutation-detection-comparison}

As described in more detail in Section~\ref{sec:improved-generalization},
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants during input value generation.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during input value filtering,
which takes place after initial input value generation.
For instance, consider the model \texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains three constraints:
\texttt{a < 0}, \texttt{a == (b + 1)}, and~\texttt{c}.
\VariantImproved{} encodes the simple comparison \texttt{a < 0} and the boolean variable \texttt{c}
in the created input value generation code,
but encodes \texttt{a~==~(b~+~1)} only in the input value filtering code
because it contains the compound term \texttt{b + 1}.
Thus, \VariantImproved{} uses 2 of 3 total constraints for input value generation (66.7\% utilization),
and the model contains 5 operators:
\texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and another \texttt{\&\&}.
With these metrics established, we are ready to examine the observed results.

Undetected mutants have more complex models than detected ones
across all evaluated projects.
Operation counts for undetected mutants are 1.2--3$\times$ higher:
\DatasetsEqBenchEs{} projects show mean counts of 218--231 versus 138--147 for detected mutants,
while \DatasetsCommonsEs{} exhibit even larger gaps with 389--507 versus 290--468 operations.
Constraint counts follow similar patterns,
with undetected mutants having 1.0--2.5$\times$ more constraints.
Even though both \VariantNaive{} and \VariantImproved{}
achieve better generalization outcomes for simpler constraints,
more complex constraints have a stronger detrimental effect on \VariantNaive{},
which produces 2-2.5$\times$ as many \texttt{TooManyFilterMissesExceptions} as \VariantImproved{},
as discussed in more detail in Section~\ref{sec:filtering-eval}.

Constraint utilization rates show large differences across project types.
\DatasetsEqBenchEs{} achieve 47--70\% mean constraint utilization for detected mutants,
while \DatasetsCommonsEs{} achieve only 25--47\% mean utilization.
The higher utilization in \DatasetsEqBenchEs{} reflects their simpler constraint structures:
these projects primarily use basic numeric comparisons
that match \VariantImproved{}'s encoding capabilities.
\DatasetsCommonsEs{} projects contain more compound terms
and mathematical functions that are not modeled by \ToolSPF{},
reducing the percentage of constraints that can guide input generation.

These utilization differences explain the contrasting performance between project types.
In the \DatasetsEqBenchEs{} projects, simple constraints enable effective boundary targeting for \VariantImproved{} variants,
yet these same simple constraints make random generation followed by \VariantNaive{} variants viable.
In fact, the higher constraint utilization even has detrimental effects
on the overall detection rates of \VariantImproved{} variants for these projects
because the focus on boundary testing detracts from testing of intermediate values.
As a result, detection rates for the very common \texttt{Math} mutations decrease,
causing overall detection rates to go down despite detection rates for most other mutants increasing.

\DatasetsCommonsEs{} present a different scenario.
Complex constraints reduce utilization to 25--47\%,
causing \VariantImproved{} variants to generate inputs from broader ranges
that overapproximate the actual partition boundaries.
As a result, fewer partition boundaries are accurately identified,
and the number of generated inputs that need to be excluded during filtering increases.
Nevertheless, constraint utilization still reduces \texttt{TooManyFilterMissesException} failures
relative to \VariantNaive{} (Section~\ref{sec:filtering-eval}), which
enables \VariantImproved{} variants to achieve overall higher mutation detection rates
than \VariantNaive{} in 7 of 9 cases
despite its current \texttt{Math} mutation detection disadvantage.

Three paths emerge to further enhance \VariantImproved{}'s effectiveness.
First, the \texttt{Math} mutation trade-off can be addressed through
higher \tries{} settings or balanced generation strategies
that maintain boundary detection advantages while improving arithmetic coverage.
Second, extending constraint support to handle more complex constraints
would further increase utilization rates,
thus enabling more effective boundary-aware input generation.
Third, adaptive strategies could select generation approaches based on
measured constraint complexity and mutation distribution,
applying boundary-aware generation where it provides the largest benefit.

\begin{center}
\fbox{\begin{minipage}{\textwidth}
\paragraph{Answer to RQ1:}
\ToolTeralizer{} improves mutation detection rates across all evaluated datasets,
with effectiveness strongly influenced by initial test suite quality and constraint complexity.
EvoSuite-generated test suites achieve the largest improvements:
1.2--3.9 percentage points (2.4--8.2\% relative) on \DatasetsEqBenchEs{}
and 0.82--1.33 percentage points (1.4--2.3\% relative) on \DatasetsCommonsEs{}.
Developer-written Apache Commons tests, already achieving 80.4\% mutation score,
show minimal improvement (0.05--0.07 percentage points),
demonstrating diminishing returns for mature test suites.
\VariantNaive{} and \VariantImproved{} variants show complementary strengths:
\VariantNaive{} excels at detecting \texttt{Math} mutations through diverse numeric inputs,
while \VariantImproved{} better detects \texttt{ConditionalsBoundary} mutations
through boundary-focused generation.
Higher \tries{} settings consistently improve detection with diminishing returns,
though \VariantImproved{} requires sufficient attempts to balance boundary and non-boundary testing.
\end{minipage}}
\end{center}
