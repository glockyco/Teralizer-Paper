\section{Related Work}
\label{sec:related-work}

Test generalization combines elements from multiple testing research areas:
test amplification that enhances existing tests,
property-based testing that explores input spaces,
and specification extraction that enables automation.
\ToolTeralizer{} demonstrates the feasibility of fully automated semantics-based test generalization,
eliminating the manual template design that limits prior generalization approaches~\cite{peleg_2018_jarvis}.
This section positions our work within the testing literature,
identifying the specific gap we address
and explaining our design choices relative to existing techniques.

\subsection{Test Amplification and Enhancement}

Test amplification research systematically improves existing test suites
rather than replacing them~\cite{danglot_2019_snowballing}.
Within Danglot et al.'s taxonomy,
\ToolTeralizer{} implements an \textbf{AMP\_mod} strategy:
transforming concrete tests into property-based specifications.
This differs from existing amplification approaches
that generate additional concrete tests or modify individual assertions.

DSpot~\cite{baudry_2015_dspot} amplifies tests through input perturbation
and assertion generation, achieving 10--40\% mutation score improvements.
DSpot generates multiple concrete test variants,
each validating a specific input-output pair.
\ToolTeralizer{} instead converts single-input tests into property-based tests
that validate behavior across input partitions.
Our smaller improvements (1--4 percentage points) reflect a different optimization target:
strengthening testing within already-covered paths
rather than discovering new test scenarios.
This establishes test generalization as a complementary amplification strategy.

Recent amplification approaches leverage machine learning.
Small-Amp~\cite{abdi_2019_smallamp} uses genetic programming
to generate test amplifications for Pharo projects.
CAmpGen~\cite{kechagia_2021_campgen} combines search-based and constraint-based techniques
for test amplification.
Both approaches generate concrete tests following learned patterns,
while \ToolTeralizer{} transforms tests into property-based specifications
using symbolic analysis of the implementation.

\ToolEvoSuite{}~\cite{fraser_2011_evosuite} generates test suites
optimized for coverage and mutation score,
achieving 40--80\% mutation scores on Java projects~\cite{fraser_2011_evosuite}.
Our evaluation demonstrates complementary strengths:
combining 1-second \ToolEvoSuite{} generation with \ToolTeralizer{}
can outperform 60-second \ToolEvoSuite{} generation alone.
\ToolEvoSuite{} discovers execution paths through evolutionary search,
while \ToolTeralizer{} thoroughly tests within those paths through property-based exploration.
This suggests a two-phase strategy:
quick test generation for path discovery,
followed by test generalization for thorough validation.

Randoop~\cite{pacheco_2007_randoop} generates tests through feedback-directed random testing,
creating method call sequences and checking for contract violations.
Randoop explores the input space without path-specific constraints,
while \ToolTeralizer{} extracts exact path conditions from existing tests
to generate inputs within specific execution partitions.

Fuzzing~\cite{manes_2021_fuzzing} generates inputs to trigger crashes or violations.
Unlike property-based tests that encode specifications,
fuzzing targets the current implementation and must be re-executed after changes.
Fuzzing primarily detects crashes and security vulnerabilities,
while test generalization preserves developer-written assertions
to validate functional correctness.

\subsection{Property-Based Testing and Parameterized Testing}

QuickCheck~\cite{claessen_2000_quickcheck} pioneered property-based testing,
automatically generating test inputs from developer-written specifications.
Subsequent frameworks like ScalaCheck~\cite{nilsson_2014_scalacheck}
and jqwik~\cite{link_2022_jqwik} brought these capabilities to other languages.
Empirical studies show property-based testing finds more bugs than traditional testing~\cite{hughes_2016_experiences},
but adoption remains limited due to the expertise required
to write properties and generators~\cite{goldstein_2024_pbt_practice}.
\ToolTeralizer{} addresses this barrier through automation,
extracting properties from existing tests rather than requiring manual specification.

JARVIS~\cite{peleg_2018_jarvis} represents the closest prior work to our approach.
JARVIS automatically identifies repetitive test scenarios,
separates positive and negative examples,
and applies Safe Generalization to synthesize ScalaCheck property-based tests.
It operates over a predefined library of abstraction templates
(e.g., intervals \texttt{x $\in$ [a,b]}, equalities \texttt{x = c}, linear relations \texttt{|x-y| $\leq$ z})
that it instantiates and ranks based on observed test examples.
This black-box approach preserves the original test oracles
and captures tester-encoded nuances,
but is limited by its template vocabulary.
When suitable templates are missing,
JARVIS produces conservative over-approximations
or requires manual extension of the template library.
For example, given \texttt{abs(5) = 5} and template \texttt{x > \$},
JARVIS might infer \texttt{x > 0 â†’ abs(x) = x},
missing the boundary case where \texttt{x = 0}.
\ToolTeralizer{} takes a fundamentally different approach:
instead of generalizing from examples using templates,
we extract exact path conditions (e.g., \texttt{x >= 0}) directly from the implementation
through symbolic execution.
This shift from example-driven template instantiation to semantics-based extraction
ensures generated properties are path-exact for covered execution paths,
eliminating template dependence entirely.

Parameterized unit testing~\cite{tillmann_2005_parameterized}
separates test logic from test data,
allowing tests to run with multiple input values.
Tools like Pex~\cite{tillmann_2008_pex} demonstrated
how symbolic execution can automatically generate inputs
for developer-written parameterized tests.
Empirical studies show PUTs improve test effectiveness
but require significant developer effort to write~\cite{tillmann_2006_unit}.
Property-based testing differs from PUTs by emphasizing
generators that produce many samples at runtime
rather than solver-generated inputs computed once.
\ToolTeralizer{} bridges these approaches:
we use single-path symbolic analysis to extract specifications,
then generate property-based tests with runtime generators like QuickCheck,
automating the entire transformation from concrete tests.

\subsection{Specification Extraction Approaches}

The ability to automatically extract specifications from programs
is fundamental to our approach.
Several techniques exist, each with different characteristics
that affect their suitability for test generalization.

\subsubsection{Symbolic Execution Tools}
\ToolSPF{}~\cite{pasareanu_2013_symbolic}, which we use in constraint collection mode,
avoids the path explosion problem of full symbolic execution.
While full symbolic execution explores all feasible paths
(exponential in the number of branches),
constraint collection follows only the single path executed by each test.
This design choice is deliberate:
existing tests provide validated oracles only for their specific execution paths.
Exploring additional paths would require synthesizing new oracles,
risking overfitting to implementation details.

Other Java symbolic execution tools face similar trade-offs.
JBSE~\cite{braione_2016_jbse} handles complex heap structures,
JDart~\cite{luckow_2016_jdart} and GDart~\cite{mues_2022_gdart} provide concolic execution,
and SWAT~\cite{mues_2024_swat} uses dynamic instrumentation.
All share the fundamental limitation to numeric and boolean types
where constraint solvers can provide precise solutions.
Despite these limitations, single-path analysis provides
a practical path to automated test generalization
by focusing on the subset of tests amenable to symbolic analysis.

\subsubsection{Alternative Specification Mining Techniques}

Other specification extraction approaches produce different types of specifications
unsuitable for test input generation.
Daikon~\cite{ernst_2007_daikon} infers likely invariants from program executions,
producing probabilistic specifications like \texttt{x > y} or \texttt{z = 2*x}.
These statistical invariants may hold for observed executions
but lack the path-specific precision needed for property-based test generation.
A Daikon-inferred invariant \texttt{x > 0} from limited observations
might miss the boundary case \texttt{x = 0} that our symbolic analysis captures exactly.

Infer~\cite{calcagno_2015_infer} uses abstract interpretation
to prove memory safety and resource properties.
While sound for its intended purpose,
abstract interpretation deliberately loses precision
to achieve tractable analysis---the opposite of what test generation requires.
Infer might prove that a method never dereferences null
but cannot provide the exact numeric constraints needed to generate test inputs.

QuickSpec~\cite{smallbone_2017_quickspec} and Speculate~\cite{braquehais_2017_speculate}
discover algebraic properties through testing.
They find universal laws like \texttt{reverse(reverse(xs)) = xs}
that hold for all inputs.
Test generalization requires path-specific properties:
not \texttt{abs(x) >= 0} for all \texttt{x},
but \texttt{abs(x) = x} when \texttt{x >= 0}.

\subsection{Metamorphic Testing}

Metamorphic testing~\cite{chen_1998_metamorphic,segura_2016_survey} addresses the oracle problem
by defining relations between multiple executions.
Rather than specifying expected outputs directly,
metamorphic relations express how outputs change when inputs are transformed
(e.g., \texttt{sin(x) = sin(x + 2$\pi$)}).

Recent work automates metamorphic relation inference.
MR-Scout~\cite{xu_2024_mrscout} synthesizes relations from existing test cases,
while GenMorph~\cite{ayerdi_2024_genmorph} uses genetic programming
to generate relations automatically.
Search-based approaches~\cite{zhang_2014_search} infer polynomial relations
from program executions.
These approaches serve different testing needs.
Metamorphic testing can only validate relational properties between executions.
For \texttt{abs}, it could verify \texttt{abs(abs(x)) = abs(x)} or \texttt{abs(-x) = abs(x)},
but cannot verify that \texttt{abs(-5) = 5}.
\ToolTeralizer{} preserves concrete assertions from existing tests,
generalizing from \texttt{abs(-5) = 5} to verify \texttt{abs(x) = -x} for all \texttt{x < 0}.
While metamorphic testing helps when expected outputs are unknown,
many critical software properties require validating specific computations
that only test generalization can address.

\subsection{Test Oracle and Assertion Enhancement}

Several approaches enhance test oracles and assertions
without generating entirely new tests.
EvoSuite's assertion generation~\cite{fraser_2011_evosuite} adds regression assertions
to capture observed behavior.
Recent work generates assertions from natural language~\cite{blasi_2018_translating}
or improves assertion quality through mutation testing~\cite{jahangirova_2016_test}.
These approaches add or refine assertions for specific execution scenarios.
\ToolTeralizer{} preserves existing assertions as specifications
and generalizes them to validate entire input partitions.

Test repair techniques~\cite{daniel_2011_automated}
fix broken tests after code changes.
While test repair adapts tests to match new implementations,
test generalization strengthens tests to detect future regressions.
Both preserve test intent but serve different maintenance phases.

\subsection{Evaluation Methodology in Testing Research}

Mutation testing~\cite{jia_2011_analysis,papadakis_2019_mutation}
provides systematic evaluation of test effectiveness
through controlled fault injection.
Tools like PIT~\cite{coles_2016_pit} have made mutation testing practical
for Java programs, enabling reproducible effectiveness measurements.
Our evaluation follows established practices~\cite{just_2014_mutants}:
using standard mutation operators,
measuring improvements across multiple projects,
and comparing variants to isolate specific effects.

Prior test amplification evaluations report varied improvements:
DSpot achieves 10--40\% mutation score increases~\cite{baudry_2015_dspot},
while assertion enhancement typically yields 5--15\% improvements~\cite{jahangirova_2016_test}.
Our 1--4 percentage point improvements reflect test generalization's specific scope:
strengthening existing paths rather than discovering new behaviors.
This establishes realistic expectations for the technique
and identifies where it complements existing approaches.
