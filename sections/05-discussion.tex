\section{Discussion}
\label{sec:discussion}

Our evaluation demonstrates that
semantics-based test generalization via symbolic analysis
is viable but currently constrained to
specific application environments and test architectures.
Under controlled conditions that match current tool capabilities,
\ToolTeralizer{} successfully generalizes 40.1\% of assertions (RQ5)
and improves mutation detection by 1--4 percentage points (RQ1).
As a post-processing step for generated tests,
generalization offers competitive efficiency.
For example, combining 1-second \ToolEvoSuite{} generation with \ToolTeralizer{}'s generalization
achieves comparable mutation detection to 60-second generation alone
while reducing total processing time by 31.9\% (RQ4).

However, fully automated generalization of real-world projects
faces substantial barriers (RQ6):
only 0.6\% of assertions pass early analysis and filtering stages
(versus 47.8\% under controlled conditions),
only 0.2\% of assertions successfully generalize,
and only 1.7\% of real-world projects complete the processing pipeline.
This section discusses when and why generalization succeeds (Section~\ref{sec:when-it-works}),
when and why it fails (Section~\ref{sec:when-it-fails}),
and how the applicability, effectiveness, and efficiency
of semantics-based test generalization can be improved
through combined efforts of the scientific research community
as well as practitioners who are interested in applying
test generalization in their own projects.

\subsection{When and Why Generalization Succeeds}
\label{sec:when-it-works}

Generalization succeeds when implementation and test properties
of the target projects align
with both \ToolTeralizer{}'s current static analysis capabilities
as well as the capabilities of \ToolSPF{}'s symbolic analysis.
Implementation code that is amenable to generalization
is primarily focused on numeric computations
through pure deterministic functions without any side effects
(thus enabling symbolic analysis)
and is organized in standard project structures that facilitate detection
of required output artifacts such as compilation outputs
as well as (mutation) testing and coverage reports.
Tests amenable to generalization are single-assertion unit tests
without complex setup logic, loops, or interprocedural control flow.
Together, these structural properties enable
reliable static analysis for identifying tested methods
and extracting path-exact specifications through \ToolSPF{}'s single-path symbolic analysis.

When the described conditions are satisfied,
\ToolTeralizer{} achieves moderate mutation score improvements at reasonable runtime cost.
Generalization increases mutation scores by 1.2--3.9 percentage points in \DatasetsEqBenchEs{} projects
and by 0.82--1.33 percentage points in \DatasetsCommonsEs{} projects
compared to \ToolEvoSuite{}-generated baselines (Figure~\ref{fig:mutation-detection-results}).
Beyond effectiveness, generalization offers competitive efficiency when combined with test generation,
where \ToolEvoSuite{} achieves breadth through coverage-guided exploration
and \ToolTeralizer{} achieves depth through partition-based input generation within already-covered test execution paths.
For example, 1-second \ToolEvoSuite{} generation combined with \VariantNaiveB{} generalization
achieves 51.7\% mutation detection rate in 37,532 seconds, thus outperforming 60-second generation alone
which achieves 51.6\% mutation detection rate in 55,075 seconds (Figure~\ref{fig:teralizer-efficiency}).

Outcomes vary based on constraint complexity and
original test suite effectiveness of the target projects.
More complex constraints hinder mutation score improvements
because they increase the number of \texttt{Too\-Many\-Filter\-Misses\-Exceptions}.
Similarly, stronger original test suites leave less room for mutation score improvements.
For example, \VariantNaive{} and \VariantImproved{}
both show larger mutation score improvements
for \DatasetsEqBenchEs{} than for \DatasetsCommonsEs{}
because of the simpler constraints in \DatasetsEqBenchEs{}
(137--231 vs. 290--507 average operation counts, Table~\ref{tab:mutation-detection-comparison}),
and larger mutation score improvements for \DatasetsCommonsEs{} than for \DatasetCommonsDev{}
because of \DatasetCommonsDev{}'s stronger original tests
(56.77--58.12\% vs. 80.35\% \VariantInitial{} mutation detection rate, Figure~\ref{fig:mutation-detection-results}).

\VariantNaive{} is more effective than \VariantImproved{} for simpler constraints
(Figure~\ref{fig:mutation-detection-results}, rows 1--3),
whereas \VariantImproved{} is more effective than \VariantNaive{} for more complex constraints
(Figure~\ref{fig:mutation-detection-results}, rows 4--6).
There are two primary factors that explain these results (Section~\ref{sec:constraint-complexity-eval}).
First, simpler constraints are easier to satisfy by chance.
Consequently, the difference in observed \texttt{TooManyFilterMissesExceptions}
between \VariantNaive{} and \VariantImproved{} is smaller in such cases
than for more complex constraints.
Second, simpler constraints enable \VariantImproved{}
to more reliably encode input partition boundaries.
As a result, it spends more \tries{} on boundary testing
but neglects non-boundary testing,
thus limiting overall mutation detection improvements.
This effect is particularly pronounced at lower \tries{} settings
where \VariantImprovedA{} noticeably underperforms
all other generalization strategies
(Figure~\ref{fig:mutation-detection-results}, rows 1--3).

\subsection{When and Why Generalization Fails}
\label{sec:when-it-fails}

Fully automated generalization fails
for the vast majority of evaluated real-world projects.
Overall, only 1.7\% of these projects (11 of 632)
successfully pass the full processing pipeline
(Table~\ref{tab:processing-failures})
and only 0.2\% of assertions (206 of 122,153)
are successfully generalized
(Table~\ref{tab:exclusions-breakdown-extended}).
Exclusions occur primarily during processing of
Stage~1~+~2 (project analysis, 79.4\% exclusion rate)
as well as Stage 5 (test suite reduction, 90.4\% exclusion rate).
In contrast, processing of Stage~3 (specification extraction) and 4 (generalized test creation)
succeeds in most cases, showing exclusion rates of only 10.0\% and 2.6\%, respectively.
These results indicate that the core generalization mechanism operates reliably 
when suitable generalization candidates are encountered.
However, early filtering for such candidates
and late-stage identification of generalizations that
improve overall test suite effectiveness
reflect significant barriers to practical adoption.

We identify three high-level causes that explain the high exclusion.
First, \ToolTeralizer{} is a research prototype,
which correspondingly limits the scope of its current capabilities.
Second, extracting accurate specifications for generalization
of test oracles is a fundamentally non-trivial problem,
even more so when moving beyond the domain of pure functions and numerical programs.
Third, factors such as execution errors in \ToolTeralizer{}'s dependencies,
timeouts enforced for evaluation purposes,
and test failures in original test suites
are beyond the direct control of the generalization mechanism itself,
but still increase the number of unsuccessful generalization attempts. 
Throughout the rest of this section,
we describe how these three high-level causes affect generalization outcomes.
This summary of failure causes then serves as the basis
for the discussion of future improvements in Section~\ref{sec:how-to-improve}.

\paragraph{Implementation Limitations}
\label{par:implementation-limitations}

There are four limiting factors in the current implementation of our prototype:
(i)~it only supports JUnit 4 and JUnit 5 tests and assertions,
(ii)~it only supports generalization of tests that contain at least one assertion,
(iii)~it only performs intraprocedural static analysis within test methods to detect assertions,
and (iv)~it only supports projects that use default output directories and formats
for compilation outputs, JUnit reports, JaCoCo reports, and PIT reports.
Limitation (iv) directly causes
95 project-level exclusions (15.0\% of projects)
due to output detection and processing failures
(Table~\ref{tab:processing-failures}, rows \#4, \#5, \#12, \#15, and \#16).
Limitations (i)--(iii) contribute to the exclusion of
129 projects (20.4\%) for which all tests are excluded
(Table~\ref{tab:processing-failures}, row~\#2)
and 266 projects (42.1\%) for which all assertions are excluded
(Table~\ref{tab:processing-failures}, rows \#1 and \#8).

While the exact impact
on the 129 test- and 266 assertion-related exclusions
is difficult to quantify precisely
(because both are also affected by the other two high-level factors),
filter rejections provide at least an approximate measure.
Limitation (i) causes all 12.5\% of \texttt{TestType} rejections
(Table~\ref{tab:exclusions-filtering-extended})
because these tests use JUnit~3.
Furthermore, limitation (ii) causes all true positive \texttt{NoAssertions} rejections
and (i)+(iii) cause all false positive \texttt{NoAssertions} rejections,
together accounting for the exclusion of 41.3\% of tests
(Table~\ref{tab:exclusions-filtering-extended}).
Thus, limitations (i)--(iii) have comparatively high impact
on the 129 test-related exclusions
--- the only other test-excluding factor is 11.8\% \texttt{NonPassingTest} rejections.
In contrast, their impact on the 266 assertion-related exclusions is comparatively low,
contributing only to 16.9\% \texttt{ExcludedTest} rejections
--- the lowest rate among all assertion-level filters
(Table~\ref{tab:exclusions-filtering-extended}).

\paragraph{Specification Extraction Challenges}
\label{par:specification-extraction-challenges}

Whereas implementation limitations primarily cause
direct project-level and test-related exclusions,
specification extraction challenges account for the majority
of the 266 assertion-related exclusions (42.1\% of projects)
as well as all 3 (0.5\%) generalization-related exclusions
(Table~\ref{tab:processing-failures}, rows \#1, \#8, and \#11).
The largest portion of these exclusions are due to 
type limitations of the underlying symbolic analysis performed by \ToolSPF{}
(discussed in Section~\ref{sec:symbolic-analysis}),
which is used by \ToolTeralizer{} to extract specifications for oracle generalization
(Sections~\ref{sec:specification-extraction} and \ref{sec:generalized-test-creation}).
A smaller portion is due to the assumption that
each assertion can be generalized by extracting the input-output specification
of the last method call that was executed before the assertion
(Section~\ref{sec:tested-method-identification}).

To quantify the impact of type limitations, notice that they are responsible for 
the following assertion filter rejections listed in Table~\ref{tab:exclusions-filtering-extended}:
all \texttt{ParameterType} rejections (49.4\% rejection rate),
all \texttt{ReturnType} rejections (32.6\%),
most \texttt{AssertionType} rejections (23.9\%),
and many \texttt{MissingValue} rejections (57.9\%).
\texttt{AssertionType} is type-related because
many unsupported assertions are for non-primitive types
(\texttt{assert(Not)Null}, \texttt{assert(Not)Same}, \texttt{assertArrayEquals}, etc.).
\texttt{MissingValue} rejections are type-related
because they are a superset of \texttt{AssertionType} rejections.
\texttt{Parameter\-Type} and \texttt{ReturnType} rejections
are directly enforced due to type limitations.
Furthermore, the exclusion rates relative to the subset of cases for which type information
is available are even higher than overall rejection rates suggest. Of 65,676 cases with parameter type information,
60,283 (91.8\%) are rejected by the \texttt{ParameterType} filter.
Similarly, 39,780 of 51,425 cases (77.4\%) with return type information
are rejected by the \texttt{ReturnType} filter.

Exclusions due to the 1:1 assertion-to-MUT mapping assumption
show a smaller impact on overall exclusion rates.
Specifically, this assumption causes the subset of \texttt{MissingValue} rejections
that occur when no MUT can be identified for a given assertion.
As explained in Section~\ref{par:real-world-assertion-exclusions},
this subset accounts for 26\% of \texttt{MissingValue} rejections,
which in turn reject 57.9\% of identified assertions ---
thus contributing rejection votes for approximately 15\% of all assertions.
However, this figure likely understates the assumption's true impact:
since assertion-to-MUT mapping is only attempted for supported assertions,
other rejection causes such as type limitations shadow
an unknown portion of mapping failures
that would be revealed if type and assertion support were improved.

\paragraph{Dependencies and Environment}
\label{par:environmental-issues}

Beyond implementation limitations and specification extraction challenges,
generalization success rates are also affected by
execution errors in \ToolTeralizer{}'s dependencies
as well as environmental factors such as enforced resource limits.
These factors are beyond the direct control of the generalization approach itself,
but nevertheless account for 128 direct project-level exclusions (20.3\% of projects).
Furthermore, they contribute to 
129 test-related exclusions (20.4\%)
and 266 assertion-related exclusions (42.1\%),
albeit to a comparatively small degree.
The 128 project-level exclusions are caused by 89 timeouts
(Table~\ref{tab:processing-failures}, rows \#3, \#10, and \#13)
and 39 dependency errors
(Table~\ref{tab:processing-failures}, rows \#6, \#7, \#9, \#12, and \#14).
The 129 test-related exclusions are affected by
\texttt{NonPassingTest} rejections
(11.8\% of tests, Table~\ref{tab:exclusions-filtering-extended}),
and the 266 assertion-related exclusions are affected by
\texttt{ExcludedTest} rejections (16.9\% of assertions),
\texttt{MissingValue} rejections (57.9\%,
33\% of which are cases where Spoon
is unable to resolve the declaration of an identified MUT,
as explained in Section~\ref{par:real-world-assertion-exclusions}),
and failures during \ToolSPF{} execution (0.3\%).
\ToolSPF{} failures are underrepresented
because filtering excludes most assertions before processing
reaches the specification extraction stage.
Among the 1093 assertions that reach \ToolSPF{},
382 (34.9\%) fail due to \ToolSPF{} errors
and enforced resource limits
(Table~\ref{tab:exclusions-breakdown-extended}).

\subsection{Directions for Future Improvements}
\label{sec:how-to-improve}

Based on our findings, three complementary directions emerge for future improvements of semantics-based test generalization:
(i) expanding applicability to handle more projects, tests, and assertions,
(ii) improving effectiveness when generalization does apply,
and (iii) improving efficiency in terms of generalization runtime
as well as size and runtime of the generalized test suite.
The following subsections discuss specific opportunities in each direction,
distinguishing between engineering improvements
that can can be achieved through additional implementation effort
and research challenges requiring advances in underlying techniques.

\subsubsection{Improving Applicability}

The primary barrier to real-world adoption is limited applicability:
99.4\% of real-world assertions are excluded before reaching generalized test creation
(Table~\ref{tab:exclusions-breakdown-extended}).
Three categories of improvements could noticeably expand
the subset of projects, tests, and assertions that are amenable to generalization.

\paragraph{Type Support}

Type limitations cause the largest portion of assertion-level exclusions.
Among assertions where type information is available,
type-based rejection rates reach
91.8\% for cases with known parameter types
and 77.4\% for cases with known return types
(Section~\ref{par:specification-extraction-challenges}).
Expanding type support remains a fundamental research
challenge~\cite{amadini_2021_string_survey,zhong_2021_arrays,chen_2024_z3noodler,chocholaty_2025_z3noodler,sun_2024_cgs}:
precise constraint modeling for strings, arrays, and objects
requires advances in symbolic analysis
that go beyond the capabilities of current tools and approaches.
Furthermore, type limitations shadow other issues.
Thus, as type support improves,
additional limitations in assertion-to-MUT mapping and general assertion support
would become visible, enabling more in-depth analysis and targeted improvement of these causes.

\paragraph{Static Analysis}

The \texttt{NoAssertions} and \texttt{TestType} filters
reject 41.3\% and 12.5\% of tests, respectively
(Section~\ref{par:implementation-limitations}).
Both are addressable through engineering improvements:
interprocedural analysis that tracks assertion calls through the call graph
would recover tests where assertions exist in helper methods.
Tests that genuinely lack assertions could be modeled as implicit ``does not throw'' checks,
thus eliminating the current need to exclude tests without assertions
due to a lack of validated oracles.
Adding support for JUnit~3, TestNG,
and assertion libraries such as AssertJ, Hamcrest, and Truth
would recover further rejections by the \texttt{TestType} and \texttt{NoAssertions} filters.

\paragraph{Project Structure and Environment}

Output detection failures exclude 14.7\% of projects
(Table~\ref{tab:processing-failures}).
Configurable output paths or improved search heuristics
would recover these projects without changing the core approach.
%
Timeouts exclude 14.1\% of projects across processing stages
(Table~\ref{tab:processing-failures}).
While increased limits could reduce these exclusions,
diminishing returns are apparent: 
doubling of all timeouts
recovered only 2 of 89 timed-out projects in our internal testing,
increasing the number of successfully processed real-world projects from 11 to 13.
\ToolSPF{} execution errors account for 51.4\% of specification extraction failures
under controlled settings (Section~\ref{sec:limitations-eval}),
many due to missing models for native methods.
Contributing such models to \ToolSPF{} would reduce these failures
without any other changes in the approach.

\subsubsection{Improving Effectiveness}

When generalization does apply, effectiveness depends on
the generation of inputs that thoroughly cover the valid input space.
Two factors influence this:
(i)~the generation strategy,
which determines whether inputs are sampled randomly (\VariantNaive{})
or specifically target input partition boundaries (\VariantImproved{}),
and (ii)~constraint encoding,
which determines how much of the extracted specification can be used to guide generation.

\paragraph{Generation Strategies}

Constraint-aware input generation used by \VariantImproved{}
increases detection rate improvements of boundary-related mutations
such as \texttt{ConditionalsBoundary} compared to \VariantNaive{} (+2.55pp vs.\ +1.21pp,
Section~\ref{sec:detection-rates-per-mutator}).
However, focusing too much on boundaries limits arithmetic diversity
within available \tries{}.
This negatively affects detection rate improvements of \texttt{Math} mutations,
particularly at lower \tries{} settings (Section~\ref{sec:constraint-complexity-eval}).
To improve detection rates without increasing \tries{},
more balanced generation strategies can be developed
that use heuristics based on constraint complexity or other source code properties
to better balance boundary vs.\ non-boundary testing,
thus utilizing the benefits of both random and boundary-focused
generation where each provides the largest benefit.

\paragraph{Constraint Encoding}

Average constraint utilization per project ranges from 11\% to 69\%
in controlled settings
(Table~\ref{tab:mutation-detection-comparison}).
This is because \ToolTeralizer{}'s \VariantImproved{} generalization strategy
only encodes simple in-/equalities on variables and constants,
whereas compound terms such as \texttt{a == (b + 1)}
are enforced through filtering (Section~\ref{sec:constraint-encoding}).
Extending constraint encoding support would enable more precise boundary testing
while reducing \texttt{TooManyFilterMissesException}s
that affect 5.7--6.0\% of \DatasetsEqBenchEs{} generalizations,
14.9--15.7\% of \DatasetsCommonsEs{} generalizations,
and 11.5--14.2\% of \DatasetCommonsDev{} generalizations
(Section~\ref{sec:limitations-eval}).
However, as constraint complexity increases,
generating valid inputs becomes increasingly difficult.
Techniques such as SMT-based constraint solving~\cite{de_moura_2008_z3,ringer_2017_iorek},
targeted property-based testing~\cite{loscher_2017_targeted},
or coverage-guided property-based testing~\cite{lampropoulos_2019_fuzzchick}
could more effectively generate inputs satisfying complex constraints
than \ToolJqwik{}'s primarily random generation,
albeit at increased computational cost.

\subsubsection{Improving Efficiency}

Efficiency improvements could be implemented along the following three dimensions:
(i)~tool runtime, which determines processing cost during generalization,
(ii)~test suite runtime, which determines execution cost after generalization,
and (iii)~test suite size, which primarily affects maintenance overhead.

\paragraph{Tool Runtime}

Processing costs are dominated by mutation testing,
which consumes 59.1--95.7\% of total pipeline runtime
(Figure~\ref{fig:teralizer-runtimes}).
However, not all mutation operators benefit equally from generalization:
\texttt{Math}, \texttt{ConditionalsBoundary}, and \texttt{RemoveConditionalEqualElse}
show the largest detection improvements,
while return value mutations show near-zero improvement
from their high baseline detection rates
(Table~\ref{tab:detections-per-mutator}).
Focusing mutation testing on operators where generalization provides
the most benefit would reduce runtime costs.
Furthermore, lightweight heuristics based on constraint complexity
or assertion patterns could
identify unlikely-beneficial candidates before mutation testing,
reducing processing time at the cost of potentially missing some improvements.
Incremental processing that targets only newly-added or modified tests
would avoid repeated analysis of stable code in continuous integration settings.
Arcmutate~\cite{arcmutate_2024}, a commercial extension of \ToolPit{},
offers an accelerator plugin that could further reduce processing costs.

\paragraph{Test Suite Runtime}

Test suite runtime increases caused by generalization stem primarily from \ToolJqwik{} framework overhead
(approximately 150ms per test, Figure~\ref{fig:test-runtime-differences})
and filter-and-regenerate cycles when inputs violate constraints.
\VariantImproved{} reduces filter miss rates by 46.5--58.4\%
compared to \VariantNaive{} (Section~\ref{sec:limitations-eval}),
which reduces execution cost per successfully generated test input from 28.61ms to 18.92ms at 10~\tries{}
and from 5.68ms to 1.98ms at 200~\tries{} (Figure~\ref{fig:test-runtime-differences}).
Better constraint encoding would further reduce filter-and-regenerate cycles
by enabling more inputs to be generated directly rather than requiring filtering.
Additionally, \ToolJqwik{}~2 plans parallelization support~\cite{link_2024_jqwik2},
which could reduce test suite execution time
by distributing property-based test execution across multiple cores.

\paragraph{Test Suite Size}

Observed LOC increases of 4.9--58.7\% across projects
(Table~\ref{tab:lines-per-project})
have two primary causes:
explicit constraint encoding in generalized tests
and structural duplication from test isolation
(Section~\ref{sec:test-suite-line-count}).
Abstracting constraint encoding in a library
could reduce this overhead,
and tighter integration of generalized tests into original test classes
would avoid duplication from copied imports and helper methods.
Test suite reduction could also be extended
to replace multiple original tests that cover the same partition
with a single property-based test,
thus reducing test count instead of only compensating for added tests.
This mirrors the idea of test suite reduction via
parameterization~\cite{tsukamoto_2018_autoput,azamnouri_2021_compressing},
but would use semantics-based analysis rather than syntactic clone detection
to identify mergeable tests.

\subsubsection{Deployment Strategies}

Our results position semantics-based test generalization
for targeted deployment during new unit test development
or as a post-processing step for generated tests in numeric-heavy domains.
Because generalization success depends not only on domain characteristics
but also on program and test architecture,
developers who are interested in adopting automated test generalization tools 
such as \ToolTeralizer{} can improve generalization outcomes
through their implementation choices independent of further test generalization advances:

\begin{enumerate}
  \item Following a more functional programming style that emphasizes pure functions
    reduces assertion-to-MUT mapping failures
    (57.9\% \texttt{MissingValue} rejections, Table~\ref{tab:exclusions-filtering-extended}).
  \item Placing assertions directly in test methods rather than delegating to helper methods
    reduces assertion detection failures due to current interprocedural analysis
    (41.3\% \texttt{NoAssertions} rejections, Table~\ref{tab:exclusions-filtering-extended}).
  \item Favoring supported assertions such as \texttt{assertEquals}
    over unsupported ones such as \texttt{assertThat} where this is feasible
    reduces assertion type exclusions
    (23.9\% \texttt{AssertionType} rejections, Table~\ref{tab:exclusions-filtering-extended}).
  \item Ensuring a green original test suite by addressing flaky tests and missing dependencies
    avoids exclusions due to failing tests
    (11.8\% \texttt{NonPassingTest} rejections, Table~\ref{tab:exclusions-filtering-extended}).
  \item Using standard project structures and build output locations for test reports and coverage data
    reduces output detection failures
    (14.7\% project exclusions, Table~\ref{tab:processing-failures}).
\end{enumerate}

Beyond these factors,
test smells~\cite{garousi_2018_test_smells,van_deursen_2001_refactoring,meszaros_2007_xunit}
represent another dimension that affects generalizability.
For example, \emph{Eager Test}
(where tests invoke multiple production methods)
complicates assertion-to-MUT mapping
(Section~\ref{sec:tested-method-identification})
and \emph{Conditional Test Logic}
(where tests contain loops or other conditionals)
can lead to inaccurate specifications
due to \ToolTeralizer{}'s limited loop handling
(Section~\ref{par:generalization-level-exclusions}).
Thus, another direction for future work
is developing automated transformation approaches
that refactor test code to improve generalizability
before applying tools like \ToolTeralizer{}.
Such transformations could build on
testability transformation techniques~\cite{harman_2004_testability},
which modify programs to improve amenability to test generation,
and recent advances in automated test smell detection~\cite{peruma_2020_tsdetect}
and refactoring~\cite{martins_2024_catalog,xuan_2016_brefactoring}.

\subsection{Threats to Validity}
\label{sec:threats-to-validity}

\paragraph{Construct Validity}

We use mutation score as a proxy for fault detection capability.
Fundamentally, this use of mutation testing
rests on two hypotheses:
the competent programmer hypothesis,
which assumes that real faults are often
only small deviations from correct programs,
and the coupling effect,
which suggests that tests which detect simple faults
will also generally detect more complex faults~\cite{demillo_1978_hints}.
Empirical studies validated the coupling effect~\cite{offutt_1992_coupling},
and subsequent work demonstrated that mutation scores correlate
with real fault detection~\cite{just_2014_mutants,papadakis_2018_correlation}.
Furthermore, surveys confirm the use
of mutation testing as a standard evaluation technique
in software testing research~\cite{jia_2011_analysis,papadakis_2019_mutation}.
%
While our use of \ToolPit{}'s \texttt{DEFAULTS} set of mutation operators
may not represent all fault types,
\texttt{DEFAULTS} is explicitly recommended by
\ToolPit{} as a stable set of operators
that minimizes equivalent mutants and avoids subsumption
~\cite{coles_2021_less_is_more, coles_pit_mutators}.

\paragraph{Internal Validity}

Our experiments use single runs per configuration.
While additional runs would produce more robust results,
we already observe consistent effectiveness and efficiency trends across projects
and configuration settings with our current setup,
even though precise magnitudes of effects vary slightly.
%
Evaluation of higher \tries{} settings
and longer timeouts could provide further evidence of observed scaling behaviors.
However, we empirically determined
these settings to provide a reasonable trade-off between
resource requirements and result quality. Scaling trends
and diminishing returns are already apparent throughout the evaluation,
and doubled timeout settings recovered only 2 of 89 timed-out
projects in our internal testing.

\paragraph{External Validity}
Our implementation targets Java~5--8 with JUnit~4/5 and Maven or Gradle builds.
While the core approach is programming language-agnostic and could be
implemented for other languages and ecosystems
(e.g., using KLEE~\cite{cadar_2008_klee}
with RapidCheck~\cite{rapidcheck_2025} for~C/C++,
or CrossHair~\cite{crosshair_2025}
with Hypothesis~\cite{maciver_2019_hypothesis} for Python),
observed results might differ
due to inherent language differences
and different maturity of available tools.
%
Our evaluation of potential benefits
emphasizes projects that match current
symbolic analysis capabilities,
particularly regarding type support and corresponding
limitations of generalizable assertion types.
As type support improves,
applicability of semantics-based test generalization
would directly benefit, though results
that can be achieved for non-numeric types might
differ from those we observed during generalization
of primarily numerical programs.
