\section{Discussion}
\label{sec:discussion}

Our evaluation demonstrates that
semantics-based test generalization via symbolic analysis
is viable but currently constrained to
specific application environments and test architectures.
Under controlled conditions that match current tool capabilities,
\ToolTeralizer{} successfully generalizes 40.1\% of assertions (RQ5)
and improves mutation detection by 1--4 percentage points (RQ1).
As a post-processing step for generated tests,
generalization offers competitive efficiency.
For example, combining 1-second \ToolEvoSuite{} generation with \ToolTeralizer{}'s generalization
achieves comparable mutation detection to 60-second generation alone
while reducing total processing time by 31.9\% (RQ4).

However, fully automated generalization of real-world projects
faces substantial barriers (RQ6):
only 0.6\% of assertions pass early analysis and filtering stages
(versus 47.8\% under controlled conditions),
only 0.2\% of assertions successfully generalize,
and only 1.7\% of real-world projects complete the processing pipeline.
This section discusses when and why generalization succeeds (Section~\ref{sec:when-it-works}),
when and why it fails (Section~\ref{sec:when-it-fails}),
and how the applicability, effectiveness, and efficiency
of semantics-based test generalization can be improved
through combined efforts of the scientific research community
as well as practitioners who are interested in applying
test generalization in their own projects.

\subsection{When and Why Generalization Succeeds}
\label{sec:when-it-works}

Generalization succeeds when implementation and test properties
of the target projects align
with both \ToolTeralizer{}'s current static analysis capabilities
as well as the capabilities of \ToolSPF{}'s symbolic analysis.
Implementation code that is amenable to generalization
is primarily focused on numeric computations
through pure deterministic functions without any side effects
(thus enabling symbolic analysis)
and is organized in standard project structures that facilitate detection
of required output artifacts such as compilation outputs
as well as (mutation) testing and coverage reports.
Tests amenable to generalization are single-assertion unit tests
without complex setup logic, loops, or interprocedural control flow.
Together, these structural properties enable
reliable static analysis for identifying tested methods
and extracting path-exact specifications through \ToolSPF{}'s single-path symbolic analysis.

When the described conditions are satisfied,
\ToolTeralizer{} achieves moderate mutation score improvements at reasonable runtime cost.
Generalization increases mutation scores by 1.2--3.9 percentage points in \DatasetsEqBenchEs{} projects
and by 0.82--1.33 percentage points in \DatasetsCommonsEs{} projects
compared to \ToolEvoSuite{}-generated baselines (Figure~\ref{fig:mutation-detection-results}).
Beyond effectiveness, generalization offers competitive efficiency when combined with test generation,
optimizing complementary dimensions where \ToolEvoSuite{} achieves breadth through coverage-guided exploration
and \ToolTeralizer{} achieves depth through partition-based input generation within already-covered test execution paths.
For example, 1-second \ToolEvoSuite{} generation combined with \VariantNaiveB{} generalization
achieves 51.7\% detection in 37,532 seconds, thus outperforming 60-second generation alone
which achieves 51.6\% detection in 55,075 seconds (Figure~\ref{fig:teralizer-efficiency}).

Outcomes vary based on constraint complexity and
original test suite effectiveness of the target projects.
More complex constraints hinder mutation score improvements
because they increase the number of \texttt{Too\-Many\-Filter\-Misses\-Exceptions}.
Similarly, stronger original test suites leave less room for mutation score improvements.
For example, \VariantNaive{} and \VariantImproved{}
both show larger mutation score improvements
for \DatasetsEqBenchEs{} than for \DatasetsCommonsEs{}
because of the simpler constraints in \DatasetsEqBenchEs{}
(137--231 vs. 290--507 average operation counts, Table~\ref{tab:mutation-detection-comparison}),
and larger mutation score improvements for \DatasetsCommonsEs{} than for \DatasetCommonsDev{}
because of \DatasetCommonsDev{}'s stronger original tests
(56.77--58.12\% vs. 80.35\% \VariantInitial{} mutation detection rate, Figure~\ref{fig:mutation-detection-results}).

\VariantNaive{} is more effective than \VariantImproved{} for simpler constraints
(Figure~\ref{fig:mutation-detection-results}, rows 1--3),
whereas \VariantImproved{} is more effective than \VariantNaive{} for more complex constraints
(Figure~\ref{fig:mutation-detection-results}, rows 4--6).
There are two primary factors that explain these results (Section~\ref{sec:constraint-complexity-eval}).
First, simpler constraints are easier to satisfy by chance.
Consequently, the difference in observed \texttt{TooManyFilterMissesExceptions}
between \VariantNaive{} and \VariantImproved{} is smaller in such cases
than for more complex constraints.
Second, simpler constraints enable \VariantImproved{}
to more reliably encode input partition boundaries.
As a result, it spends more \tries{} on boundary testing
but neglects non-boundary testing,
thus limiting overall mutation detection improvements.
This effect is particularly pronounced at lower \tries{} settings
where \VariantImprovedA{} noticeably underperforms
all other generalization strategies
(Figure~\ref{fig:mutation-detection-results}, rows 1--3).

\subsection{When and Why Generalization Fails}
\label{sec:when-it-fails}

Fully automated generalization fails
for the vast majority of evaluated real-world projects.
Overall, only 1.7\% of these projects (11 of 632)
successfully pass the full processing pipeline
(Table~\ref{tab:processing-failures})
and only 0.2\% of assertions (206 of 122,153)
are successfully generalized
(Table~\ref{tab:exclusions-breakdown-extended}).
Exclusions occur primarily during processing of
Stage~1~+~2 (project analysis, 79.4\% exclusion rate)
as well as Stage 5 (test suite reduction, 90.4\% exclusion rate).
In contrast, processing of Stage~3 (specification extraction) and 4 (generalized test creation)
succeeds in most cases, showing exclusion rates of only 10.0\% and 2.6\%, respectively.
These results indicate that the core generalization mechanism operates reliably 
when suitable generalization candidates are encountered.
However, early filtering for such candidates
and late-stage identification of generalizations that
improve overall test suite effectiveness
reflect significant barriers to practical adoption.

Broadly speaking, there are three high-level causes
that explain the high exclusion rates we observe
when evaluating fully automated test generalization via \ToolTeralizer{} in real-world projects.
First, \ToolTeralizer{} is a research prototype,
which correspondingly limits the scope of its current capabilities.
Second, extracting accurate specifications for generalization
of test oracles is a fundamentally non-trivial problem,
even more so when moving beyond the domain of pure functions and numeric-focused programs.
Third, factors such as execution errors in \ToolTeralizer{}'s dependencies,
timeouts enforced for evaluation purposes,
and test failures in original test suites
are beyond the direct control of the generalization mechanism itself,
but still increase the number of unsuccessful generalization attempts. 
Throughout the rest of this section,
we describe how these three high-level causes affect generalization outcomes.
This summary of failure causes then serves as the basis for discussion of future improvements
in Section~\ref{sec:how-to-improve}.

\paragraph{Implementation Limitations}

There are four limiting factors in our current prototype
that are implementation specific rather than fundamental:
(i)~it only supports JUnit 4 and JUnit 5 tests and assertions,
(ii)~it only supports generalization of tests that contain at least one assertion,
(iii)~it only performs intraprocedural static analysis within test methods to detect assertions,
and (iv)~it only supports projects that use default output directories and formats
for compilation outputs, JUnit reports, JaCoCo reports, and PIT reports.
Limitation (iv) directly causes
95 project-level exclusions (15.0\% of projects)
due to output detection and processing failures
(Table~\ref{tab:processing-failures}, rows \#4, \#5, \#12, \#15, and \#16).
Limitations (i)--(iii) contribute to the exclusion of
129 projects (20.4\%) for which all tests are excluded
(Table~\ref{tab:processing-failures}, row~\#2)
and 266 projects (42.1\%) for which all assertions are excluded
(Table~\ref{tab:processing-failures}, rows \#1 and \#8).

While the exact degree of influence
on the 129 test- and 266 assertion-related exclusions
is difficult to quantify precisely
(because both are also affected by the other two high-level factors),
filter rejections provide at least an approximate measure.
Limitation (i) causes all 12.5\% of \texttt{TestType} rejections
(Table~\ref{tab:exclusions-filtering-extended})
because these tests use JUnit~3.
Furthermore, limitation (ii) causes all true positive \texttt{NoAssertions} rejections
and (i)+(iii) cause all false positive \texttt{NoAssertions} rejections,
together accounting for the exclusion of 41.3\% of tests
(Table~\ref{tab:exclusions-filtering-extended}).
Thus, limitations (i)--(iii) have comparatively high impact
on the 129 test-related exclusions
--- the only other test-excluding factor is 11.8\% \texttt{NonPassingTest} rejections.
In contrast, their influence on the 266 assertion-related exclusions is comparatively low,
contributing only to 16.9\% \texttt{ExcludedTest} rejections
--- the lowest rate among all assertion-level filters
(Table~\ref{tab:exclusions-filtering-extended}).

\paragraph{Specification Extraction Challenges}

Whereas implementation limitations primarily cause
direct project-level and test-related exclusions,
specification extraction challenges account for the majority
of the 266 assertion-related exclusions (42.1\% of projects)
as well as all 3 (0.5\%) generalization-related exclusions
(Table~\ref{tab:processing-failures}, rows \#1, \#8, and \#11).
The largest portion of these exclusions are due to 
type limitations of the underlying symbolic analysis performed by \ToolSPF{}
(discussed in Section~\ref{sec:symbolic-analysis}),
which is used by \ToolTeralizer{} to extract specifications for oracle generalization
(Sections~\ref{sec:specification-extraction} and \ref{sec:generalized-test-creation}).
A smaller portion is due to the simplifying assumption that
each assertion can be generalized by extracting the input-output specification
of the last method call that was executed before the assertion
(Section~\ref{sec:tested-method-identification}).

To quantify the impact of type limitations, notice that they are responsible for 
the following assertion filter rejections listed in Table~\ref{tab:exclusions-filtering-extended}:
all \texttt{ParameterType} rejections (49.4\% rejection rate),
all \texttt{ReturnType} rejections (32.6\%),
most \texttt{AssertionType} rejections (23.9\%),
and many \texttt{MissingValue} rejections (57.9\%).
\texttt{AssertionType} is type-related because
many unsupported assertions are for non-primitive types
(\texttt{assert(Not)Null}, \texttt{assert(Not)Same}, \texttt{assertArrayEquals}, etc.).
\texttt{MissingValue} rejections are type-related
because they are a superset of \texttt{AssertionType} rejections.
\texttt{Parameter\-Type} and \texttt{ReturnType} rejections
are directly enforced due to type limitations.
Furthermore, the exclusion rates relative to the subset of cases for which type information
is available are even higher than overall rejection rates suggest. Of 65,676 cases with parameter type information,
60,283 (91.8\%) are rejected by the \texttt{ParameterType} filter.
Similarly, 39,780 of 51,425 cases (77.4\%) with return type information
are rejected by the \texttt{ReturnType} filter.

Exclusions due to the 1:1 assertion-to-MUT mapping assumption
show a smaller impact on overall exclusion rates.
Specifically, this assumption causes the subset of \texttt{MissingValue} rejections
that occur when no MUT can be identified for a given assertion.
As explained in Section~\ref{par:real-world-assertion-exclusions},
this subset accounts for 26\% of \texttt{MissingValue} rejections,
which in turn reject 57.9\% of identified assertions ---
thus contributing rejection votes for approximately 15\% of all assertions.
However, this figure likely understates the assumption's true impact:
since assertion-to-MUT mapping is only attempted for supported assertions,
other rejection causes such as type limitations shadow
an unknown portion of mapping failures
that would be revealed if type and assertion support were improved.

\paragraph{Dependencies and Environment}

Beyond implementation limitations and specification extraction challenges,
generalization success rates are also affected by
execution errors in \ToolTeralizer{}'s dependencies
as well as environmental factors such as enforced resource limits.
These factors are beyond the direct control of the generalization approach itself,
but nevertheless account for 128 direct project-level exclusions (20.3\% of projects).
Furthermore, they contribute to 
129 test-related exclusions (20.4\%)
and 266 assertion-related exclusions (42.1\%),
albeit to a comparatively small degree.
The 128 project-level exclusions are caused by 89 timeouts
(Table~\ref{tab:processing-failures}, rows \#3, \#10, and \#13)
and 39 dependency errors
(Table~\ref{tab:processing-failures}, rows \#6, \#7, \#9, \#12, and \#14).
The 129 test-related exclusions are affected by
\texttt{NonPassingTest} rejections
(11.8\% of tests, Table~\ref{tab:exclusions-filtering-extended}),
and the 266 assertion-related exclusions are affected by
\texttt{ExcludedTest} rejections (16.9\% of assertions),
\texttt{MissingValue} rejections (57.9\%,
33\% of which are cases where Spoon
is unable to resolve the declaration of an identified MUT,
as explained in Section~\ref{par:real-world-assertion-exclusions}),
and failures during \ToolSPF{} execution (0.3\%).
\ToolSPF{} failures are underrepresented
because filtering excludes most assertions before processing
reaches the specification extraction stage.
Among the 1093 assertions that reach \ToolSPF{},
382 (34.9\%) fail due to \ToolSPF{} errors
and enforced resource limits
(Table~\ref{tab:exclusions-breakdown-extended}).

\subsection{Directions for Future Improvements}
\label{sec:how-to-improve}

Based on our findings, three complementary directions emerge for future improvements of semantics-based test generalization:
(i) expanding applicability to handle more projects, tests, and assertions,
(ii) improving effectiveness when generalization does apply,
and (iii) improving efficiency in terms of generalization runtime
as well as size and runtime of the generalized test suite.
The following subsections discuss specific opportunities in each direction,
distinguishing between engineering improvements
that can can be achieved through additional implementation effort
and research challenges requiring advances in underlying techniques.

\subsubsection{Improving Applicability}

The primary barrier to real-world adoption is limited applicability:
99.4\% of real-world assertions are excluded before reaching generalized test creation
(Table~\ref{tab:exclusions-breakdown-extended}).
Three categories of improvements could noticeably expand
the subset of projects, tests, and assertions that are amenable to generalization.

\paragraph{Type Support}

Type limitations cause the largest portion of assertion-level exclusions.
Among assertions where type information is available,
type-based rejection rates reach
91.8\% for cases with known parameter types
and 77.4\% for cases with known return types
(Section~\ref{par:specification-extraction-challenges}).
Expanding type support remains a fundamental research
challenge~\cite{amadini_2021_string_survey,zhong_2021_arrays,chen_2024_z3noodler,chocholaty_2025_z3noodler,sun_2024_cgs}:
precise constraint modeling for strings, collections, and custom objects
requires advances in symbolic analysis beyond current capabilities.
Furthermore, type limitations shadow other issues:
as type support improves,
additional limitations in assertion-to-MUT mapping and general assertion support
would become visible, enabling more in-depth analysis and targeted improvement of these causes.

\paragraph{Static Analysis}

The \texttt{NoAssertions} and \texttt{TestType} filters
reject 41.3\% and 12.5\% of tests, respectively
(Section~\ref{par:implementation-limitations}).
Both are addressable through engineering improvements:
interprocedural analysis that tracks assertion calls through the call graph
would recover tests where assertions exist in helper methods.
Tests that genuinely lack assertions could be modeled as implicit ``does not throw'' checks,
thus eliminating the current need to exclude tests without assertions
due to a lack of validated oracles.
Adding support for JUnit~3, TestNG,
and assertion libraries such as AssertJ, Hamcrest, and Truth
would recover further rejections by the \texttt{TestType} and \texttt{NoAssertions} filters.

\paragraph{Project Structure and Environment}

Output detection failures exclude 14.7\% of projects
(Table~\ref{tab:processing-failures}).
Configurable output paths or improved search heuristics
would recover these projects without changing the core approach.
%
Timeouts exclude 14.1\% of projects across processing stages
(Table~\ref{tab:processing-failures}).
While increased limits could reduce these exclusions,
diminishing returns are apparent: 
doubling of all timeouts
recovered only 2 of 89 timed-out projects in our internal testing,
increasing the number of successfully processed real-world projects from 11 to 13.
\ToolSPF{} execution errors account for 51.4\% of specification extraction failures
under controlled settings (Section~\ref{sec:limitations-eval}),
many due to missing models for native methods.
Contributing such models to \ToolSPF{} would reduce these failures
without any other changes in the approach.

\subsubsection{Improving Effectiveness}

When generalization does apply, effectiveness depends on
the generation of inputs that thoroughly cover the valid input space.
Two factors influence this:
(i)~the generation strategy,
which determines whether inputs are sampled randomly (\VariantNaive{})
or specifically target input partition boundaries (\VariantImproved{}),
and (ii)~constraint encoding,
which determines how much of the extracted specification can be used to guide generation.

\paragraph{Generation Strategies}

Constraint-aware input generation used by \VariantImproved{}
increases detection rate improvements of boundary-related mutations
such as \texttt{ConditionalsBoundary} compared to \VariantNaive{} (+2.55pp vs.\ +1.21pp,
Section~\ref{sec:detection-rates-per-mutator}).
However, focusing too much on boundaries limits arithmetic diversity
within available \tries{}.
This negatively affects detection rate improvements of \texttt{Math} mutations,
particularly at lower \tries{} settings (Section~\ref{sec:constraint-complexity-eval}).
To improve detection rates without increasing \tries{},
more balanced generation strategies can be developed
that use heuristics based on constraint complexity or other source code properties
to better balance boundary vs.\ non-boundary testing,
thus utilizing the benefits of both random and boundary-focused
generation where each provides the largest benefit.

\paragraph{Constraint Encoding}

Average constraint utilization per project ranges from 11\% to 69\%
in controlled settings
(Table~\ref{tab:mutation-detection-comparison}).
This is because \ToolTeralizer{}'s \VariantImproved{} generalization strategy
only encodes simple in-/equalities on variables and constants,
whereas compound terms such as \texttt{a == (b + 1)}
are enforced through filtering (Section~\ref{sec:constraint-encoding}).
Extending constraint encoding support would enable more precise boundary testing
while reducing \texttt{TooManyFilterMissesException}s
that affect 5.7--6.0\% of \DatasetsEqBenchEs{} generalizations,
14.9--15.7\% of \DatasetsCommonsEs{} generalizations,
and 11.5--14.2\% of \DatasetCommonsDev{} generalizations
(Section~\ref{sec:limitations-eval}).
However, as constraint complexity increases,
generating valid inputs becomes increasingly difficult.
Techniques such as SMT-based constraint solving~\cite{de_moura_2008_z3,ringer_2017_iorek},
targeted property-based testing~\cite{loscher_2017_targeted},
or coverage-guided property-based testing~\cite{lampropoulos_2019_fuzzchick}
could more effectively generate inputs satisfying complex constraints
than \ToolJqwik{}'s primarily random generation,
albeit at increased computational cost.
