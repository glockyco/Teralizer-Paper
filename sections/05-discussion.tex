\section{Discussion}
\label{sec:discussion}

Our evaluation demonstrates that
semantics-based test generalization via symbolic analysis
is viable but currently constrained to
specific application environments and test architectures.
Under controlled conditions that match current tool capabilities,
\ToolTeralizer{} successfully generalizes 40.1\% of assertions (RQ5)
and improves mutation detection by 1--4 percentage points (RQ1).
As a post-processing step for generated tests,
generalization offers competitive efficiency.
For example, combining 1-second \ToolEvoSuite{} generation with \ToolTeralizer{}'s generalization
achieves comparable mutation detection to 60-second generation alone
while reducing total processing time by 31.9\% (RQ4).

However, fully automated generalization of real-world projects
faces substantial barriers (RQ6):
only 0.6\% of assertions pass early analysis and filtering stages
(versus 47.8\% under controlled conditions),
only 0.2\% of assertions successfully generalize,
and only 1.7\% of real-world projects complete the processing pipeline.
This section discusses when and why generalization succeeds (Section~\ref{sec:when-it-works}),
when and why it fails (Section~\ref{sec:when-it-fails}),
and how the effectiveness, efficiency, and applicability
of semantics-based test generalization can be improved
through combined efforts of the scientific research community
as well as practitioners who are interested in applying
test generalization in their own projects.

\subsection{When and Why Generalization Succeeds}
\label{sec:when-it-works}

Generalization succeeds when implementation and test properties
of the target projects align
with both \ToolTeralizer{}'s current static analysis capabilities
as well as the capabilities of \ToolSPF{}'s symbolic analysis.
Implementation code that is amenable to generalization
is primarily focused on numeric computations
through pure deterministic functions without any side effects
(thus enabling symbolic analysis)
and is organized in standard project structures that facilitate detection
of required output artifacts such as compilation outputs
as well as (mutation) testing and coverage reports.
Tests amenable to generalization are single-assertion unit tests
without complex setup logic, loops, or interprocedural control flow.
Together, these structural properties enable
reliable static analysis for identifying tested methods
and extracting path-exact specifications through \ToolSPF{}'s single-path symbolic analysis.

When the described conditions are satisfied,
\ToolTeralizer{} achieves moderate mutation score improvements at reasonable runtime cost.
Generalization increases mutation scores by 1.2--3.9 percentage points in \DatasetsEqBenchEs{} projects
and by 0.82--1.33 percentage points in \DatasetsCommonsEs{} projects
compared to \ToolEvoSuite{}-generated baselines (Figure~\ref{fig:mutation-detection-results}).
Beyond effectiveness, generalization offers competitive efficiency when combined with test generation,
optimizing complementary dimensions where \ToolEvoSuite{} achieves breadth through coverage-guided exploration
and \ToolTeralizer{} achieves depth through partition-based input generation within already-covered test execution paths.
For example, 1-second \ToolEvoSuite{} generation combined with \VariantNaiveB{} generalization
achieves 51.7\% detection in 37,532 seconds, thus outperforming 60-second generation alone
which achieves 51.6\% detection in 55,075 seconds (Figure~\ref{fig:teralizer-efficiency}).

Outcomes vary based on constraint complexity and
original test suite effectiveness of the target projects.
More complex constraints hinder mutation score improvements
because they increase the number of \texttt{Too\-Many\-Filter\-Misses\-Exceptions}.
Similarly, stronger original test suites leave less room for mutation score improvements.
For example, \VariantNaive{} and \VariantImproved{}
both show larger mutation score improvements
for \DatasetsEqBenchEs{} than for \DatasetsCommonsEs{}
because of the simpler constraints in \DatasetsEqBenchEs{}
(137--231 vs. 290--507 average operation counts, Table~\ref{tab:mutation-detection-comparison}),
and larger mutation score improvements for \DatasetsCommonsEs{} than for \DatasetCommonsDev{}
because of \DatasetCommonsDev{}'s stronger original tests
(56.77--58.12\% vs. 80.35\% \VariantInitial{} mutation detection rate, Figure~\ref{fig:mutation-detection-results}).

\VariantNaive{} is more effective than \VariantImproved{} for simpler constraints
(Figure~\ref{fig:mutation-detection-results}, rows 1--3),
whereas \VariantImproved{} is more effective than \VariantNaive{} for more complex constraints
(Figure~\ref{fig:mutation-detection-results}, rows 4--6).
There are two primary factors that explain these results (Section~\ref{sec:constraint-complexity-eval}).
First, simpler constraints are easier to satisfy by chance.
Consequently, the difference in observed \texttt{TooManyFilterMissesExceptions}
between \VariantNaive{} and \VariantImproved{} is smaller in such cases
than for more complex constraints.
Second, simpler constraints enable \VariantImproved{}
to more reliably encode input partition boundaries.
As a result, it spends more \tries{} on boundary testing
but neglects non-boundary testing,
thus limiting overall mutation detection improvements.
This effect is particularly pronounced at lower \tries{} settings
where \VariantImprovedA{} noticeably underperforms
all other generalization strategies
(Figure~\ref{fig:mutation-detection-results}, rows 1--3).

\subsection{When and Why Generalization Fails}
\label{sec:when-it-fails}

Fully automated generalization fails
for the vast majority of evaluated real-world projects.
Overall, only 1.7\% of these projects (11 of 632)
successfully pass the full processing pipeline
(Table~\ref{tab:processing-failures})
and only 0.2\% of assertions (206 of 122,153)
are successfully generalized
(Table~\ref{tab:exclusions-breakdown-extended}).
Exclusions occur primarily during processing of
Stage~1~+~2 (project analysis, 79.4\% exclusion rate)
as well as Stage 5 (test suite reduction, 90.4\% exclusion rate).
In contrast, processing of Stage~3 (specification extraction) and 4 (generalized test creation)
succeeds in most cases, showing exclusion rates of only 10.0\% and 2.6\%, respectively.
These results indicate that the core generalization mechanism operates reliably 
when suitable generalization candidates are encountered.
However, early filtering for such candidates
and late-stage identification of generalizations that
improve overall test suite effectiveness
reflect significant barriers to practical adoption.

Broadly speaking, there are three high-level causes
that explain the high exclusion rates we observe
when evaluating fully automated test generalization via \ToolTeralizer{} in real-world projects.
First, \ToolTeralizer{} is a research prototype,
which correspondingly limits the scope of its current capabilities.
Second, extracting accurate specifications for generalization
of test oracles is a fundamentally non-trivial problem,
even more so when moving beyond the domain of pure functions and numeric-focused programs.
Third, factors such as execution errors in \ToolTeralizer{}'s dependencies,
timeouts enforced for evaluation purposes,
and test failures in original test suites
are beyond the direct control of the generalization mechanism itself,
but still increase the number of unsuccessful generalization attempts. 
Throughout the rest of this section,
we describe how these three high-level causes affect generalization outcomes.
This summary of failure causes then serves as the basis for discussion of future improvements
in Section~\ref{sec:how-to-improve}.

\paragraph{Implementation Limitations}

There are four limiting factors in our current prototype
that are implementation specific rather than fundamental:
(i)~it only supports JUnit 4 and JUnit 5 tests and assertions,
(ii)~it only supports generalization of tests that contain at least one assertion,
(iii)~it only performs intraprocedural static analysis within test methods to detect assertions,
and (iv)~it only supports projects that use default output directories and formats
for compilation outputs, JUnit reports, JaCoCo reports, and PIT reports.
Limitation (iv) directly causes
95 project-level exclusions (15.0\% of projects)
due to output detection and processing failures
(Table~\ref{tab:processing-failures}, rows \#4, \#5, \#15, \#16, and \#17).
Limitations (i)--(iii) contribute to the exclusion of
129 projects (20.4\%) for which all tests are excluded
(Table~\ref{tab:processing-failures}, row~\#2)
and 266 projects (42.1\%) for which all assertions are excluded
(Table~\ref{tab:processing-failures}, rows \#1 and \#8).

While the exact degree of influence
on the 129 test- and 266 assertion-related exclusions
is difficult to quantify precisely
(because both are also affected by the other two high-level factors),
filter rejections provide at least an approximate measure.
Limitation (i) causes all 12.5\% of \texttt{TestType} rejections
(Table~\ref{tab:exclusions-filtering-extended})
because these tests use JUnit~3.
Furthermore, limitation (ii) causes all true positive \texttt{NoAssertions} rejections
and (i)+(iii) cause all false positive \texttt{NoAssertions} rejections,
together accounting for the exclusion of 41.3\% of tests
(Table~\ref{tab:exclusions-filtering-extended}).
Thus, limitations (i)--(iii) have comparatively high impact
on the 129 test-related exclusions
--- the only other test-excluding factor is 11.8\% \texttt{NonPassingTest} rejections.
In contrast, their influence on the 266 assertion-related exclusions is comparatively low,
contributing only to 16.9\% \texttt{ExcludedTest} rejections
--- the lowest rate among all assertion-level filters
(Table~\ref{tab:exclusions-filtering-extended}).
