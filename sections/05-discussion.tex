\section{Discussion}
\label{sec:discussion}

Our evaluation demonstrates that
semantics-based test generalization via symbolic analysis
is viable but currently constrained to
specific application environments and test architectures.
Under controlled conditions that match current tool capabilities,
\ToolTeralizer{} successfully generalizes 40.1\% of assertions (RQ5)
and improves mutation detection by 1--4 percentage points (RQ1).
As a post-processing step for generated tests,
generalization offers competitive efficiency.
For example, combining 1-second \ToolEvoSuite{} generation with \ToolTeralizer{}'s generalization
achieves comparable mutation detection to 60-second generation alone
while reducing total processing time by 31.9\% (RQ4).

However, fully automated generalization of real-world projects
faces substantial barriers (RQ6):
only 0.6\% of assertions pass early analysis and filtering stages
(versus 47.8\% under controlled conditions),
only 0.2\% of assertions successfully generalize,
and only 1.7\% of real-world projects complete the processing pipeline.
This section discusses when and why generalization succeeds (Section~\ref{sec:when-it-works}),
when and why it fails (Section~\ref{sec:when-it-fails}),
and how the effectiveness, efficiency, and applicability
of semantics-based test generalization can be improved
through combined efforts of the scientific research community
as well as practitioners who are interested in applying
test generalization in their own projects.

\subsection{When and Why Generalization Succeeds}
\label{sec:when-it-works}

Generalization succeeds when implementation and test properties
of the target projects align
with both \ToolTeralizer{}'s current static analysis capabilities
as well as the capabilities of \ToolSPF{}'s symbolic analysis.
Implementation code that is amenable to generalization
is primarily focused on numeric computations
through pure deterministic functions without any side effects
(thus enabling symbolic analysis)
and is organized in standard project structures that facilitate detection
of required output artifacts such as compilation outputs
as well as (mutation) testing and coverage reports.
Tests amenable to generalization are single-assertion unit tests
without complex setup logic, loops, or interprocedural control flow.
Together, these structural properties enable
reliable static analysis for identifying tested methods
and extracting path-exact specifications through \ToolSPF{}'s single-path symbolic analysis.

When the described conditions are satisfied,
\ToolTeralizer{} achieves moderate mutation score improvements at reasonable runtime cost.
Generalization increases mutation scores by 1.2--3.9 percentage points in \DatasetsEqBenchEs{} projects
and by 0.82--1.33 percentage points in \DatasetsCommonsEs{} projects
compared to \ToolEvoSuite{}-generated baselines (Figure~\ref{fig:mutation-detection-results}).
Beyond effectiveness, generalization offers competitive efficiency when combined with test generation,
optimizing complementary dimensions where \ToolEvoSuite{} achieves breadth through coverage-guided exploration
and \ToolTeralizer{} achieves depth through partition-based input generation within already-covered test execution paths.
For example, 1-second \ToolEvoSuite{} generation combined with \VariantNaiveB{} generalization
achieves 51.7\% detection in 37,532 seconds, thus outperforming 60-second generation alone
which achieves 51.6\% detection in 55,075 seconds (Figure~\ref{fig:teralizer-efficiency}).

Outcomes vary based on constraint complexity and
original test suite effectiveness of the target projects.
More complex constraints hinder mutation score improvements
because they increase the number of \texttt{Too\-Many\-Filter\-Misses\-Exceptions}.
Similarly, stronger original test suites leave less room for mutation score improvements.
For example, \VariantNaive{} and \VariantImproved{}
both show larger mutation score improvements
for \DatasetsEqBenchEs{} than for \DatasetsCommonsEs{}
because of the simpler constraints in \DatasetsEqBenchEs{}
(137--231 vs. 290--507 average operation counts, Table~\ref{tab:mutation-detection-comparison}),
and larger mutation score improvements for \DatasetsCommonsEs{} than for \DatasetCommonsDev{}
because of \DatasetCommonsDev{}'s stronger original tests
(56.77--58.12\% vs. 80.35\% \VariantInitial{} mutation detection rate, Figure~\ref{fig:mutation-detection-results}).

\VariantNaive{} is more effective than \VariantImproved{} for simpler constraints
(Figure~\ref{fig:mutation-detection-results}, rows 1--3),
whereas \VariantImproved{} is more effective than \VariantNaive{} for more complex constraints
(Figure~\ref{fig:mutation-detection-results}, rows 4--6).
There are two primary factors that explain these results (Section~\ref{sec:constraint-complexity-eval}).
First, simpler constraints are easier to satisfy by chance.
Consequently, the difference in observed \texttt{TooManyFilterMissesExceptions}
between \VariantNaive{} and \VariantImproved{} is smaller in such cases
than for more complex constraints.
Second, simpler constraints enable \VariantImproved{}
to more reliably encode input partition boundaries.
As a result, it spends more \tries{} on boundary testing
but neglects non-boundary testing,
thus limiting overall mutation detection improvements.
This effect is particularly pronounced at lower \tries{} settings
where \VariantImprovedA{} noticeably underperforms
all other generalization strategies
(Figure~\ref{fig:mutation-detection-results}, rows 1--3).
