\subsection{RQ2: How does constraint complexity affect random versus constraint-aware input generation?}
\label{sec:constraint-complexity-eval}

As discussed in Section~\ref{sec:overall-detection-rates},
\VariantNaive{} outperforms \VariantImproved{} on \DatasetsEqBenchEs{} projects.
However, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 cases.
To better understand these contrasting results, RQ2 examines
how constraint complexity differs across projects and
how it affects \VariantNaive{} versus \VariantImproved{}.
As described in Section~\ref{sec:constraint-encoding},
\VariantImproved{} tests
encode simple in-/equalities on numeric and boolean variables or constants
during input value generation.
More complex constraints are not encoded
during \VariantImproved{} input generation
--- and no constraints are encoded by \VariantNaive{} ---
but are still enforced during input filtering
which takes place after input generation.

Table~\ref{tab:mutation-detection-comparison} shows the model properties
of mutants that are (not) detected by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
Models represent the constraints that inputs must satisfy
to reach each mutant along a specific execution path.
We measure model complexity through operation count (total operators)
and constraint count (individual boolean conditions),
while tracking which percentage of constraints \VariantImproved{} can encode
during input generation versus enforce through post-generation filtering.
For instance, consider \texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains three constraints:
\texttt{a < 0}, \texttt{a == (b + 1)}, and~\texttt{c}.
\VariantImproved{} encodes the simple comparison \texttt{a < 0} and the boolean variable \texttt{c}
in the created input value generation code,
but encodes \texttt{a~==~(b~+~1)} only in the input value filtering code
because it contains the compound term \texttt{b + 1}.
Thus, \VariantImproved{} uses 2 of 3 total constraints for input value generation (66.7\% utilization),
and the model contains 5 operators:
\texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and another \texttt{\&\&}.

\input{tables/tab-mutation-detection-comparison}

Undetected mutants have more complex models than detected ones
across all evaluated projects.
Operation counts for undetected mutants are 1.2--3$\times$ higher:
\DatasetsEqBenchEs{} projects show mean counts of
218--231 operations for undetected mutants
versus 138--147 operations for detected mutants,
while \DatasetsCommonsEs{} show even larger gaps with 389--507 versus 290--468 operations.
Constraint counts follow similar patterns,
with undetected mutants having 1.0--2.5$\times$ more constraints.
Even though both \VariantNaive{} and \VariantImproved{}
achieve better generalization outcomes for simpler constraints,
more complex constraints have a stronger detrimental effect on \VariantNaive{},
which produces 2-2.5$\times$ as many \texttt{Too\-Many\-Filter\-Misses\-Exceptions} as \VariantImproved{},
as discussed in more detail in Section~\ref{sec:limitations-eval}.

Constraint utilization rates show large differences across project types.
\DatasetsEqBenchEs{} achieve 47--70\% mean constraint utilization for detected mutants,
while \DatasetsCommonsEs{} achieve only 25--47\% mean utilization.
The higher utilization in \DatasetsEqBenchEs{} reflects their simpler constraint structures:
these projects primarily use basic numeric comparisons
that match \VariantImproved{}'s encoding capabilities.
\DatasetsCommonsEs{} projects contain more compound terms
and mathematical functions that are not modeled by \ToolTeralizer{},
reducing the percentage of constraints that can guide input generation.

These utilization differences explain the contrasting detection results.
In \DatasetsEqBenchEs{}, simple constraints enable effective boundary targeting for \VariantImproved{},
yet these same simple constraints make \VariantNaive{}'s random generation viable.
The higher constraint utilization even has detrimental effects
on \VariantImproved{} detection rates
because the focus on boundary testing detracts from testing of intermediate values.
As a result, detection rates for the very common \texttt{Math} mutations decrease,
causing overall detection rates to go down despite detection rates for most other mutants increasing.

\DatasetsCommonsEs{} projects present a different scenario.
Complex constraints reduce \VariantImproved{}'s constraint utilization to 25--47\%,
causing generalized tests to generate inputs from broader ranges
that overapproximate the true partition boundaries.
As a result, fewer partition boundaries are accurately identified,
and the number of generated inputs that need to be excluded during filtering increases.
Nevertheless, constraint utilization still reduces \texttt{Too\-Many\-Filter\-Misses\-Exception} failures
relative to \VariantNaive{} (Section~\ref{sec:limitations-eval}), which
enables \VariantImproved{} variants to achieve higher mutation detection rates
than \VariantNaive{} in 7 of 9 cases
despite its \texttt{Math} mutation detection disadvantage.

Three paths emerge to further enhance \VariantImproved{}'s effectiveness.
First, the \texttt{Math} mutation trade-off can be addressed through
higher \tries{} settings or balanced generation strategies
that maintain boundary detection advantages while improving arithmetic coverage.
Second, extending \ToolTeralizer{}'s constraint encoding support
to handle more complex constraints
would further increase utilization rates,
thus enabling more effective constraint-aware input generation.
However, encoding of non-boundary constraints would require custom input generators
that are more capable than those provided by \ToolJqwik{}.
Third, adaptive strategies could select generation approaches based on
measured constraint complexity and mutation distribution,
applying constraint-aware generation where it provides the largest benefit.

\rqanswerbox{2}{
  Both input generation strategies perform better on simpler constraints,
  but \VariantNaive{}'s effectiveness degrades more strongly as constraint complexity increases.
  On \DatasetsEqBenchEs{} projects with simpler constraints,
  \VariantNaive{} outperforms \VariantImproved{}
  because random generation satisfies many constraints by chance,
  while \VariantImproved{}'s boundary focus limits arithmetic diversity within the available \tries{},
  thus reducing \texttt{Math} mutation detection rates.
  On \DatasetsCommonsEs{} projects with more complex constraints,
  \VariantNaive{} generates substantially more inputs that violate constraints,
  causing more \texttt{Too\-Many\-Filter\-Misses\-Exception} failures.
  \VariantImproved{}'s constraint-aware generation reduces these failures,
  enabling it to outperform \VariantNaive{} in 7 of 9 cases despite its \texttt{Math} mutation disadvantage.
  Balancing boundary and non-boundary testing could combine the advantages of both strategies.
}
