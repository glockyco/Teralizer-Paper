\subsection{RQ2: Examining Constraint Complexity}
\label{sec:constraint-complexity-eval}

Mutation testing provides a rigorous measure of test effectiveness
by evaluating a test suite's ability to detect deliberately introduced faults.
For \ToolTeralizer{}, mutation scores reveal whether testing additional inputs
within existing execution paths achieves the intended improvement in fault detection capabilities.
Section~\ref{sec:included-mutants} first establishes which tests and mutants are included in the evaluation,
revealing systematic differences between generated and developer-written test suites that influence generalization outcomes.
Section~\ref{sec:overall-detection-rates} quantifies detection improvements
across projects and variants,
demonstrating that generalization improves mutation scores
for \DatasetsEqBenchEs{} and \DatasetsCommons{} projects, albeit with varying improvement magnitudes.
Section~\ref{sec:detection-rates-per-mutator} dissects these improvements by mutation operator,
uncovering how much detection of different mutants benefits from generalization.
Finally, Section~\ref{sec:boundary-detection-effectiveness} provides a
detailed analysis of the \VariantImproved{} generalization variant,
demonstrating that its effectiveness correlates with constraint complexity,
i.e., projects with more complex input constraints show greater improvements
from constraint-aware input generation.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

As discussed in Section~\ref{sec:overall-detection-rates},
\VariantNaive{} variants
outperformed \VariantImproved{}
on \DatasetsEqBenchEs{} projects.
However, \DatasetsCommonsEs{} projects show \VariantImproved{}
outperforming \VariantNaive{} in 7 of 9 cases.
To provide a more thorough understanding of these contrasting results, we examine
how constraint complexity differs between project types and
what the underlying mechanisms are that explain how complexity affects mutation detection differences.
Table~\ref{tab:mutation-detection-comparison} shows the constraint characteristics
of mutants that are (not) detected by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
Models represent the constraints that inputs must satisfy
to reach each mutant along a specific execution path.
We measure model complexity through operation count (total operators)
and constraint count (individual boolean conditions),
while tracking which percentage of constraints \VariantImproved{} can encode
for use during input generation versus enforce through post-generation filtering.

\input{tables/tab-mutation-detection-comparison}

As described in more detail in Section~\ref{sec:improved-generalization},
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants during input value generation.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during input value filtering,
which takes place after initial input value generation.
For instance, consider the model \texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains three constraints:
\texttt{a < 0}, \texttt{a == (b + 1)}, and~\texttt{c}.
\VariantImproved{} encodes the simple comparison \texttt{a < 0} and the boolean variable \texttt{c}
in the created input value generation code,
but encodes \texttt{a~==~(b~+~1)} only in the input value filtering code
because it contains the compound term \texttt{b + 1}.
Thus, \VariantImproved{} uses 2 of 3 total constraints for input value generation (66.7\% utilization),
and the model contains 5 operators:
\texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and another \texttt{\&\&}.
With these metrics established, we are ready to examine the observed results.

Undetected mutants have more complex models than detected ones
across all evaluated projects.
Operation counts for undetected mutants are 1.2--3$\times$ higher:
\DatasetsEqBenchEs{} projects show mean counts of 218--231 versus 138--147 for detected mutants,
while \DatasetsCommonsEs{} exhibit even larger gaps with 389--507 versus 290--468 operations.
Constraint counts follow similar patterns,
with undetected mutants having 1.0--2.5$\times$ more constraints.
Even though both \VariantNaive{} and \VariantImproved{}
achieve better generalization outcomes for simpler constraints,
more complex constraints have a stronger detrimental effect on \VariantNaive{},
which produces 2-2.5$\times$ as many \texttt{TooManyFilterMissesExceptions} as \VariantImproved{},
as discussed in more detail in Section~\ref{sec:filtering-eval}.

Constraint utilization rates show large differences across project types.
\DatasetsEqBenchEs{} achieve 47--70\% mean constraint utilization for detected mutants,
while \DatasetsCommonsEs{} achieve only 25--47\% mean utilization.
The higher utilization in \DatasetsEqBenchEs{} reflects their simpler constraint structures:
these projects primarily use basic numeric comparisons
that match \VariantImproved{}'s encoding capabilities.
\DatasetsCommonsEs{} projects contain more compound terms
and mathematical functions that are not modeled by \ToolSPF{},
reducing the percentage of constraints that can guide input generation.

These utilization differences explain the contrasting performance between project types.
In the \DatasetsEqBenchEs{} projects, simple constraints enable effective boundary targeting for \VariantImproved{} variants,
yet these same simple constraints make random generation followed by \VariantNaive{} variants viable.
In fact, the higher constraint utilization even has detrimental effects
on the overall detection rates of \VariantImproved{} variants for these projects
because the focus on boundary testing detracts from testing of intermediate values.
As a result, detection rates for the very common \texttt{Math} mutations decrease,
causing overall detection rates to go down despite detection rates for most other mutants increasing.

\DatasetsCommonsEs{} present a different scenario.
Complex constraints reduce utilization to 25--47\%,
causing \VariantImproved{} variants to generate inputs from broader ranges
that overapproximate the actual partition boundaries.
As a result, fewer partition boundaries are accurately identified,
and the number of generated inputs that need to be excluded during filtering increases.
Nevertheless, constraint utilization still reduces \texttt{TooManyFilterMissesException} failures
relative to \VariantNaive{} (Section~\ref{sec:filtering-eval}), which
enables \VariantImproved{} variants to achieve overall higher mutation detection rates
than \VariantNaive{} in 7 of 9 cases
despite its current \texttt{Math} mutation detection disadvantage.

Three paths emerge to further enhance \VariantImproved{}'s effectiveness.
First, the \texttt{Math} mutation trade-off can be addressed through
higher \tries{} settings or balanced generation strategies
that maintain boundary detection advantages while improving arithmetic coverage.
Second, extending constraint support to handle more complex constraints
would further increase utilization rates,
thus enabling more effective constraint-aware input generation.
Third, adaptive strategies could select generation approaches based on
measured constraint complexity and mutation distribution,
applying constraint-aware generation where it provides the largest benefit.

TODO: Answer to RQ1.

% \begin{center}
% \fbox{\begin{minipage}{0.98\textwidth}
% \paragraph{Answer to RQ1:}
% \ToolTeralizer{} improves mutation detection rates across all evaluated datasets,
% with effectiveness strongly influenced by initial test suite quality and constraint complexity.
% EvoSuite-generated test suites achieve the largest improvements:
% 1.2--3.9 percentage points (2.4--8.2\% relative) on \DatasetsEqBenchEs{}
% and 0.82--1.33 percentage points (1.4--2.3\% relative) on \DatasetsCommonsEs{}.
% Developer-written Apache Commons tests, already achieving 80.4\% mutation score,
% show minimal improvement (0.05--0.07 percentage points),
% demonstrating diminishing returns for mature test suites.
% \VariantNaive{} and \VariantImproved{} variants show complementary strengths:
% \VariantNaive{} excels at detecting \texttt{Math} mutations through diverse numeric inputs,
% while \VariantImproved{} better detects \texttt{ConditionalsBoundary} mutations
% through constraint-aware generation.
% Higher \tries{} settings consistently improve detection with diminishing returns,
% though \VariantImproved{} requires sufficient attempts to balance boundary and non-boundary testing.
% \end{minipage}}
% \end{center}
