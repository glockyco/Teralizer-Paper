\subsection{RQ2: Effects on Test Suite Size and Runtime}
\label{sec:ancillary-effects-eval}

Test generalization transforms conventional JUnit tests into property-based \ToolJqwik{} tests.
While this transformation improves mutation detection (Section~\ref{sec:primary-effects-eval}),
it also affects test suite characteristics in several ways:

\begin{enumerate}
  \item the number of tests in the test suite (Section~\ref{sec:test-suite-test-count}),
  \item the number of lines of code in the test suite (Section~\ref{sec:test-suite-line-count}),
  \item the execution time of the test suite (Section~\ref{sec:test-suite-execution-time}).
\end{enumerate}

Section~\ref{sec:test-suite-test-count} reveals how test architecture determines
whether added generalized tests can be compensated by original test removals,
explaining why \ToolEvoSuite{}-generated test suites achieve near-complete compensation
while the developer-written test suite in \DatasetCommonsDev{} does not allow for any test removals at all.
Section~\ref{sec:test-suite-line-count} documents
how current constraint encoding and test isolation implementations increase lines of code in the test suite,
suggesting that the introduction of constraint abstraction libraries
as well as tighter integration of generalized tests into the original test suite
could reduce this overhead.
Section~\ref{sec:test-suite-execution-time} analyzes runtime patterns,
showing that costs stem primarily from property-based testing overhead and \tries{} repetition,
but could be at least partially alleviated
through test suite reduction and improved support for more complex input constraints.

Throughout the result presentation, we focus primarily on the results
of the \VariantNaiveC{} and \VariantImprovedC{} generalization variants
as they best represent the current capabilities of \ToolTeralizer{}.
The results for variants with fewer \tries{} follow similar trends
albeit with correspondingly smaller effects
on the measured test suite characteristics.
Full results for all generalization variants
are available in our replication package~\cite{replicationpackage}.

\subsubsection{Number of Tests in the Test Suite}
\label{sec:test-suite-test-count}

Generalization creates new property-based tests
for each successfully transformed unit test,
yet the net effect on test counts can vary considerably
depending on test architecture and mutation detection capabilities
of the \VariantOriginal{} test suite.
Table~\ref{tab:tests-per-project} quantifies the observed changes
for the \VariantNaiveC{} and \VariantImprovedC{} generalization variants.
The number of added tests is between 174--211
(3.5--4.3\% of \VariantOriginal{} test suite size)
for the \DatasetsEqBenchEs{} projects,
between 60--75 (2.2--2.9\%)
for the \DatasetsCommonsEs{} projects,
and 3 (0.4\%) for the \DatasetCommonsDev{} project.

\input{tables/tab-tests-per-project}

\VariantImprovedC{} generally adds a larger number of tests than \VariantNaiveC{} 
because more of the tests that are created by \VariantNaiveC{}
are excluded due to \texttt{Too\-Many\-Filter\-Misses\-Exception}s
(as described in Section~\ref{sec:naive-generalization}
and evaluated in Section~\ref{sec:filtering-eval}).
Furthermore, the number of added tests is much smaller
than the total number of generalizations that are created and evaluated by \ToolTeralizer{}
because only tests that measurably increase the mutation score of the test suite are retained.
In total, \ToolTeralizer{} generates 65,633 candidate generalizations across all projects and variants,
but filtering retains only 4,240 (6.5\%) that demonstrably improve mutation detection results.

Even though \ToolTeralizer{} adds hundreds of tests to the generalized test suites,
net test count changes remain minimal.
Added tests are largely compensated by removed tests
in the \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects
which use \ToolEvoSuite{}-generated test suites.
As a result, total test suite size only increases by 0--1 test cases
(0--0.04\% of \VariantOriginal{} test suite size)
for these projects.
The \DatasetCommonsDev{} project sees less compensation success:
none of the tests for which generalizations are added can be removed.
This is because an \VariantOriginal{} test can only be removed
if generalized tests are created for all assertions in the test
(Section~\ref{sec:baseline-generalization}).
In projects with \ToolEvoSuite{}-generated tests,
this requirement is generally satisfied because most tests only contain a single assertion.
However, this requirement is more difficult to satisfy for \DatasetCommonsDev{} 
because the developer-written tests often contain multiple assertions.

The observed compensation patterns reveal how test architecture shapes generalization outcomes.
Projects containing mostly focused, single-assertion tests enable near-complete compensation,
while multi-assertion test suites resist such consolidation.
In future work, we plan to explore opportunities
for generalization to decrease overall test suite size.
This can be achieved
by replacing multiple \VariantOriginal{} tests
that all cover the same input partition
with a single property-based test that covers the same partition instead.
For further discussion and related work covering this idea, see Section~\ref{sec:test-suite-reduction}.

\subsubsection{Lines of Code in the Test Suite}
\label{sec:test-suite-line-count}

While test counts remain relatively stable through the test replacement mechanism,
lines of code (LOC) increase across all projects and generalization variants.
Table~\ref{tab:lines-per-project} shows increases of
31.5--58.7\% for \DatasetsEqBenchEs{} projects,
18.8--29.9\% for \DatasetsCommonsEs{} projects,
and 4.9--5.3\% for \DatasetCommonsDev{}.
These increases correlate with the number of added tests
rather than net test count changes.
For example, for the \DatasetEqBenchA{} project,
\VariantNaiveC{} adds 177 generalized tests and removes all 177 corresponding original tests.
However, the added tests increase test suite size by 11,780 LOC while the removed tests reduce LOC by only 1,127.
This asymmetry between added and removed LOC stems from
two characteristics inherent to \ToolTeralizer{}'s current implementation.

\input{tables/tab-lines-per-project}

First, \ToolTeralizer{} encodes constraints explicitly in the source code of the created property-based tests.
More specifically, each generalized test defines input ranges matching extracted constraints by
translating input specifications into Java code for a custom \ToolJqwik{} \texttt{Arbitrary}
(Section~\ref{sec:test-transformation}).
As a result, the LOC impact scales with both parameter count and constraint complexity.
\VariantImproved{} variants also show higher per-test LOC as a consequence of this mechanism 
because of their more sophisticated input generation logic.
For example, \DatasetCommonsDev{}'s three generalized tests require
36 additional LOC with \VariantImprovedC{} compared to \VariantNaiveC{}
(9,018 vs.\ 8,982 total LOC).
Methods with multiple parameters amplify this effect,
as each parameter requires its own constraint encoding logic in \VariantImproved{} tests.

Second, test isolation creates structural duplication.
\ToolTeralizer{} creates new test classes for each generalized method,
copying imports, setup/teardown methods, helper functions, class fields, etc.
from original test classes (Section~\ref{sec:test-transformation}).
This architectural choice prioritizes safety over LOC efficiency,
preventing unintended interactions between generalized and original tests
while avoiding potential side effects on developer-written code.

The duplication overhead varies significantly between \ToolEvoSuite{}-generated and developer-written tests.
\DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects show similar LOC overhead per generalization
(66-67 vs 62-63 LOC per added test, respectively),
reflecting the structural uniformity of \ToolEvoSuite{}-generated tests.
In contrast, \DatasetCommonsDev{} shows substantially higher duplication overhead
(140 LOC per added test).
This is because developer-written tests contain more shared setup code
as well as multi-assertion architectures that hinder compensation through test removals.
The higher percentage increases observed in \DatasetsEqBenchEs{} projects (31-59\%) compared to \DatasetsCommonsEs{} projects (19-30\%)
stem primarily from the retained generalized tests representing a larger fraction of the original test suite
in \DatasetsEqBenchEs{} (~3.5-3.8\%) compared to \DatasetsCommonsEs{} (~2.2-2.4\%).
The smaller difference between \VariantNaive{} and \VariantImproved{}
LOC increases in \DatasetsCommonsEs{} projects compared to \DatasetsEqBenchEs{} projects
likely reflects \VariantImproved{}'s lower constraint utilization on more complex specifications
(28.5\% for \DatasetsCommonsEs{} vs.\ 54.7\% for \DatasetsEqBenchEs{}).

Both overhead sources represent implementation choices rather than inherent limitations.
Integration of generalized methods into original test classes would eliminate duplication,
while constraint specification libraries could abstract input value generator details.
The primary reason \ToolTeralizer{} avoids in-place transformation of tests
is to keep changes unintrusive and isolated from original test suites,
preventing unintended adverse side-effects on developer-written code.
However, no technical limitation strictly requires separate test classes.
Similarly, constraint encoding logic could be extracted to a library
that abstracts implementation details.
In the test code of the target projects, only minor changes would remain then:
modified test annotations, parameterized inputs, and generalized assertions.
As a result, such optimizations could reduce the current LOC increases
while preserving generalization's mutation detection benefits.

\subsubsection{Execution Time of the Test Suite}
\label{sec:test-suite-execution-time}

As shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{} and \VariantImprovedC{}
increases overall test suite runtimes
for all \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
More specifically, test suite runtimes
show increases of 574.5--1210.0\% for the \DatasetsEqBenchEs{} projects,
444.1--2651.5\% for the \DatasetsCommonsEs{} projects,
and 9.4--57.1\% for the \DatasetCommonsDev{} project.
Runtime increases are primarily affected by the following factors:

\begin{enumerate}
  \item the number of tests added by the generalization, even if they are compensated for by removed tests,
  \item the number of \tries{} used during property-based testing,
  \item the used generalization approach, i.e., \VariantNaive{} vs. \VariantImproved{} generalization,
  \item the complexity of input partition constraints.
\end{enumerate}

In the following paragraphs,
we discuss how each of the above factors
influences overall test suite execution times
and how these factors interact with each other.

\input{tables/tab-runtime-per-project}

\paragraph{Added Tests}

Execution of conventional JUnit tests has a lower runtime cost
than execution of corresponding property-based jqwik tests.
More specifically, individual property-based tests
created with the \VariantBaseline{} generalization variant 
take, on average, 149.56 milliseconds (ms) longer to execute
than the corresponding \VariantOriginal{} tests
(as shown in the left plot of Figure~\ref{fig:test-runtime-differences})
which have a mean execution time of only 3.6~ms. % 0.0035570506476290337
% Exact number = 0.0035570506476290337 via:
% SELECT AVG(runtime)
% FROM junit_test_report jtr
% JOIN generalization g ON g.test_id = jtr.test_id
% WHERE stage = 'COLLECT_JUNIT_REPORTS_ORIGINAL'
% AND g.is_included;
Therefore, overall test suite execution time increases,
on average, by at least 149.56~ms per jqwik test
that is added during the generalization process,
even if no new test inputs are exercised
and the added tests are compensated for by removed JUnit tests.
Since these runtime increases are inherent to the use of jqwik,
they are orthogonal to our specific generalization approach.
Any manual or automated transformation of JUnit tests
to jqwik tests incurs the same runtime overhead,
and this overhead can only be reduced
if fewer jqwik tests are created
(e.g., through test suite reduction, as discussed in Section~\ref{sec:test-suite-reduction})
or if the runtime performance of jqwik is improved.

\paragraph{Number of \tries{}}

Increasing the number of \tries{}
directly increases the number of test inputs
that need to be generated and exercised
during property-based test execution.
For example, as shown in Figure~\ref{fig:test-runtime-differences},
execution time of \VariantNaive{} tests
is, on average, 286.13 milliseconds (ms) longer per test
than for \VariantOriginal{} tests when using 10 \tries{}
(a +91.3\% increase compared to the \VariantBaseline{} overhead of 149.56~ms),
348.56~ms (+133.0\%) longer with 50 \tries{},
and 1136.21~ms (+659.7\%) longer with 200 \tries{}.
Similarly, execution time of \VariantImproved{} tests
is 189.17~ms (+26.5\%) longer with 10 \tries{},
246.85~ms (+65.1\%) longer with 50 \tries{},
and 395.26~ms (+164.3\%) longer with 200 \tries{}.
While overall runtime increases
get larger as \texttt{tries} increase,
runtime increases per \texttt{try}
get smaller as \texttt{tries} increase.
More specifically, \VariantNaive{} variants
show per-\texttt{try} increases
of 28.61~ms / 6.97~ms / 5.68~ms at 10 / 50 / 200 \tries{}.
\VariantImproved{} variants show
per-\texttt{try} increases 
of 18.92~ms / 4.94~ms / 1.98~ms at 10 / 50 / 200 \tries{}.
However, to attribute these observed
improvements in per-\texttt{try} efficiency to any specific causes
would require a more thorough microbenchmarking setup
that properly accounts for confounding factors such as JVM warmup,
which is beyond the scope of this evaluation.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig_test_runtime_differences}
  \caption{Runtime Comparison: Test vs. Generalization.}
  %\Description{@TODO}
  \label{fig:test-runtime-differences}
\end{figure}

\paragraph{\VariantNaive{} vs.\ \VariantImproved{} Generalization}

Runtime increases are generally larger
for \VariantNaive{} than for \VariantImproved{}
when comparing the same tests.
For example, \VariantNaiveC{} and \VariantImprovedC{}
both generalize the same three tests of \DatasetCommonsDev{}
(see Table~\ref{tab:tests-per-project}).
Nevertheless, as shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{}
increases overall test suite runtime by 4.54 seconds
(+57.1\% compared to \VariantOriginal{}).
\VariantImprovedC{}, on the other hand,
only increases runtime by 0.74 seconds (+9.4\%).
The more pronounced runtime impact of \VariantNaive{} variants is caused by 
the input value generation approach used by these variants.
As described in Section~\ref{sec:naive-generalization},
\VariantNaive{} generalization selects inputs for test execution
by first randomly generating inputs
that match the parameter types of the tested method,
and then discarding any inputs that do not satisfy the input specification.
Especially for cases with restrictive input specifications (e.g., \texttt{a~==~b~\&\&~b~==~c}),
this causes a large runtime overhead
because many discard-and-regenerate cycles are required
until a valid set of inputs is identified
(or \ToolJqwik{} aborts the process due to \texttt{TooManyFilterMisses}).
\VariantImproved{} variants are less affected by this
because they already consider (some) input constraints
during initial input value generation
(as described in Section~\ref{sec:improved-generalization}).
As a result, fewer inputs need to be discarded and regenerated,
which lessens the overall runtime impact of \VariantImproved{} generalization
despite the more involved input value generation process.

\paragraph{Complexity of Constraints}

As complexity of input specifications increases,
required runtime also increases.
This is because more complex constraints
cause more filter-and-regenerate cycles to occur
during execution of property-based tests.
While \VariantNaive{} generalization
is more strongly affected by this than \VariantImproved{}
because it does not consider any constraints during value generation
(as discussed in the previous paragraph),
the issue also affects \VariantImproved{} generalization
in cases where less than 100\% of constraints can be used
(for further details on constraint use,
see Sections~\ref{sec:improved-generalization} and \ref{sec:boundary-detection-effectiveness}).
For example, as shown in Table~\ref{tab:tests-per-project},
\VariantImprovedC{} adds 3 times as many tests (206 vs.\ 69) to \DatasetEqBenchA{}
as it adds to \DatasetCommonsA{},
yet runtime increases are similar: +578.7\% vs.\ +680.5\%.
This is because input specifications in \DatasetEqBenchA{}
have fewer operations (mean: 159.1 vs.\ 208.0, median: 10 vs.\ 15)
and constraints (mean: 6.5 vs.\ 7.5, median: 2 vs.\ 5)
than in \DatasetCommonsA{}, 
and fewer of these constraints
can be used during input value generation
(mean: 42.9\% vs.\ 61.5\%, median: 80\% vs.\ 100\%).
This indicates that better support for more complex input constraints
not only holds the potential to increase detection rates
(as suggested in Section~\ref{sec:boundary-detection-effectiveness})
but could also reduce the runtime cost of generalized tests at the same time.

@TODO: Add answer for RQ2.
