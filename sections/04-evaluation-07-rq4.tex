\subsection{RQ4: How efficient is test generalization compared to extended test generation?}
\label{sec:runtime-eval}

We measured \ToolTeralizer{}'s runtime across seven generalization strategies
to assess viability compared to existing automated testing tools.
Since no existing tools perform automated test generalization from unit tests to property-based tests,
we compared efficiency against \ToolEvoSuite{}~\cite{fraser_2011_evosuite}, the established test generation tool.
%Runtime results were collected on a MacBook Air with M2 processor and 24~GB of memory.
\VariantShared{} processing stages were executed only once per project by \ToolTeralizer{}
and collected specifications were then reused across all seven generalization strategies.
Processing required a total time of 3.4 hours for \DatasetCommonsDev{},
8.2--9.8 hours for the three \DatasetsCommonsEs{} variants,
and 24.8--30.9 hours for the three \DatasetsEqBenchEs{} variants (Section~\ref{sec:execution-time}).
Despite the relatively high runtime cost,
Pareto analysis (Section~\ref{sec:execution-efficiency}) shows
that combining low search budget \ToolEvoSuite{} generation and \ToolTeralizer{} generalization
achieved better detection-to-runtime ratios than simply running \ToolEvoSuite{} with higher search budgets
in several configurations.

\subsubsection{Execution Time of \ToolTeralizer{}}
\label{sec:execution-time}

Runtime distribution across processing stages reveals
that validation stages dominate overall computational cost of \ToolTeralizer{}'s processing pipeline.
Figure~\ref{fig:teralizer-runtimes} shows
generalization validation consuming 1,110--35,538 seconds (40.5--78.5\% of total processing time)
while specification extraction requires 65--2,793 seconds (0.7--8.2\%)
and test transformation takes 5--173 seconds (0.1--0.5\%).
This resembles \ToolEvoSuite{}'s distribution,
where the JUnit checking phase,
which is most comparable to \ToolTeralizer{}'s
generalization validation stage,
consumes 86\%, 72\%, and 34\% of median runtime
for 1s, 10s, and 60s search budgets respectively.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_teralizer_runtimes}
  \caption{Teralizer runtimes per project, processing stage, and generalization strategy. Stages 1--3 are \VariantShared{} stages that are only executed once. Stages 4 and 5 are executed once per generalization strategy, i.e., \VariantBaseline{}, \VariantNaives{}, and \VariantImproveds{}.}
  \Description{Grouped bar chart showing runtime breakdown across processing stages for seven projects.
  Y-axis shows runtime in seconds (up to 40000).
  X-axis groups bars by processing stage:
  Original Validation, Specification Extraction, Initial Validation,
  Test Transformation, and Generalization Validation.
  Each group contains bars for different variants (SHARED, BASELINE, NAIVE with 10/50/200 tries,
  and IMPROVED with 10/50/200 tries) shown in different colors.
  Bar heights show runtime with values labeled above each bar.
  Generalization Validation stage shows the highest runtimes (up to 35.5k seconds),
  while Test Transformation shows minimal runtime (5-173 seconds).
  Pattern reveals validation stages dominate total runtime,
  particularly for higher tries settings.}
  \label{fig:teralizer-runtimes}
\end{figure}

\paragraph{Non-Validation Runtimes}

Non-validation stages complete efficiently
despite containing the core generalization logic.
Specification extraction (65--2,793 seconds) uses \ToolSPF{}
to concretely execute tests using single-path symbolic analysis,
extracting path conditions and symbolic outputs
without requiring any constraint solving calls
(Section~\ref{sec:specification-extraction}).
While this introduces some overhead
because the JVM implementation of \ToolJPF / \ToolSPF{}
is less optimized than production-ready JVM implementations
(\ToolJPF{} itself runs inside of a host JVM process \cite{pasareanu_2013_symbolic}),
the cost is comparatively small, representing a one-time cost
of ca.\ 100$\times$ the runtime of an original test suite execution.
Test transformation cost is even smaller at a one-time runtime cost
of only 5--173 seconds per project.
The reason for this low cost is that test transformation
only performs syntactic replacements, e.g.,
converting JUnit annotations to \ToolJqwik{} ones,
wrapping input constraint encodings in \texttt{TestParameters} classes,
and replacing expected values in assertions 
(Section~\ref{sec:generalized-test-creation}).
Consequently, both specification extraction as well as test transformation
offer comparatively little opportunity for runtime improvements.

\paragraph{Validation Runtimes}

Validation costs are substantially larger than non-validation costs
due to the high runtime cost of mutation testing,
which is by far the largest contributing factor in all validation stages.
For example, even on the \VariantOriginal{} test suite
without any added generalized tests,
execution of \VariantOriginal{} validation (701--3,898 seconds)
generally requires 1.5--10$\times$ as much runtime as specification extraction,
with mutation testing representing 88\% of the total runtime cost of this stage on average.
Execution times for \VariantInitial{} validation (652--4,131 seconds)
are generally slightly lower than for \VariantOriginal{} validation
because of the filtering that takes place
between the two stages (Sections~\ref{sec:test-assertion-analysis}--\ref{sec:generalized-test-creation}).
Validation of generalized test suites has higher runtime requirements (1,110--35,538 seconds)
than for the \VariantOriginal{} and \VariantInitial{} ones
for largely the same reasons that individual property-based \ToolJqwik{} tests
take longer to execute than conventional JUnit tests, i.e.,
\ToolJqwik{} overhead, larger number of tested inputs,
as well as filter-and-regenerate cycles
which occur more often for \VariantNaive{} variants
and in the presence of more complex input constraints (Section~\ref{sec:test-suite-execution-time}).

While it might seem tempting to forgo any validation,
this would result in considerable increases to the execution times of generalized test suites.
After all, one of the primary purpose of validation stages is
(i)~to provide a baseline for mutation detection comparisons
(\VariantOriginal{} / \VariantInitial{} Validation),
and (ii)~to identify which generalized tests do not
measurably increase mutation detection rates (Generalization Validation),
thereby enabling \ToolTeralizer{} to remove non-contributing tests from the final generalized test suite.
In our evaluation, this filtering reduces the number of retained generalized tests
from a total of 65,633 to 4,240 across all projects and test suite variants
(Section~\ref{sec:test-suite-test-count}),
thus demonstrating the impact
that validation-based filtering has on generalization outcomes.
Even though there is a non-negligible one-time cost associated with this,
that cost amortizes over time compared to a longer-running test suite
that incurs further cost with every repeated execution.
Future optimization efforts could focus on validation efficiency
through faster mutation testing approaches
or lightweight pre-filtering heuristics
that identify likely-beneficial candidates without full validation.

\subsubsection{Efficiency of \ToolTeralizer{} vs.\ \ToolEvoSuite{}}
\label{sec:execution-efficiency}

Combining \ToolEvoSuite{}'s test generation with \ToolTeralizer{}'s test generalization
can achieve better detection-to-runtime ratios than simply increasing \ToolEvoSuite{} search budgets.
This efficiency gain stems from their complementary optimization targets:
\ToolEvoSuite{} optimizes primarily for breadth (increasing coverage)~\cite{fraser_2011_evosuite},
while \ToolTeralizer{} optimizes exclusively for depth (testing discovered paths thoroughly).
Figure~\ref{fig:teralizer-efficiency} identifies which tool configurations
provide the best detection-to-runtime trade-offs through Pareto analysis.
Configurations on the frontier represent optimal choices:
for any given runtime budget, they achieve the highest possible detection rate.
Configurations outside the frontier are suboptimal,
i.e., dominated by the Pareto-optimal points,
because the Pareto points achieve higher mutation detection rates in less time.

The results show that combining \ToolEvoSuite{}'s generation with \ToolTeralizer{}'s generalization
not only increases detection rates (Section~\ref{sec:primary-effects-eval})
but also yields several configurations with lower runtime costs
than executing \ToolEvoSuite{} with higher search budgets.
For example, for the \DatasetsEqBenchEs{} projects,
only 2 of 3 \ToolEvoSuite{} search budget settings are Pareto optimal.
\ToolEvoSuite{} generation with a 60-second per-class search budget
achieves 51.6\% detection in 55,075 seconds yet falls outside the Pareto frontier.
In the runtime dimension,
this result is most strongly dominated by 1-second \ToolEvoSuite{} generation
combined with \VariantNaiveB{} generalization
(Pareto point \#5 in Table~\ref{tab:pareto-eqbench}),
which achieves 51.7\% detection in 37,532 seconds,
i.e., 0.1 percentage points higher detection with 31.9\% lower runtime cost.
In the mutation detection dimension,
the result is most strongly dominated by 10-second \ToolEvoSuite{} generation
combined with \VariantImprovedC{} generalization (Pareto point \#9),
which produces a 53.8\% detection result in 48,269 seconds, i.e.,
a detection improvement of 2.2 percentage points achieved in 12.4\% less runtime.
Increasing the runtime beyond this comparison point achieves even higher
detection rates that further extend the Pareto frontier (Pareto points \#10--\#14).

Efficiency improvements of \ToolEvoSuite{} + \ToolTeralizer{} combinations
over \ToolEvoSuite{} search budget increases
are less pronounced for the \DatasetsCommonsEs{} projects.
Here, all three \ToolEvoSuite{} search budget settings are Pareto optimal
(Pareto points \#1, \#2, and \#4 in Table~\ref{tab:pareto-commons}).
Nevertheless, generalization via \ToolTeralizer{} contributes
8 additional points to the Pareto frontier.
Specifically, the combination of 1-second \ToolEvoSuite{} generation
and \VariantImprovedA{} generalization (Pareto point \#3)
produces a Pareto optimal result
that has higher detection rate and runtime cost than \#2
but lower detection rate and runtime cost than \#4.
The 7 remaining Pareto points \#5--\#11 again extend the Pareto frontier
toward higher detection rates at higher runtime cost,
reflecting the increased detection rates achievable via generalization
before reaching a plateau at around
1.0--1.3 percentage points above the corresponding \ToolEvoSuite{} results
(Section~\ref{sec:primary-effects-eval}.)

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figures/fig_teralizer_efficiency}
  \caption{Pareto fronts for EvoSuite and Teralizer variants across projects.}
  \Description{Two scatter plots showing Pareto efficiency analysis.
  Left panel shows eqbench project, right panel shows commons-utils project.
  X-axis shows runtime in seconds, Y-axis shows mutation detection percentage.
  Blue circles represent EvoSuite-only configurations,
  red X marks show EvoSuite + NAIVE combinations,
  green triangles show EvoSuite + IMPROVED combinations.
  Dashed black line connects Pareto-optimal configurations numbered 1-14 (eqbench) and 1-11 (commons-utils).
  For eqbench, points show detection rates from 48% to 56% with runtimes from 30000 to 90000 seconds.
  For commons-utils, points show detection rates from 56.5% to 59.5% with runtimes from 5000 to 20000 seconds.
  Pareto frontier demonstrates that combining EvoSuite with Teralizer
  can achieve better detection-to-runtime ratios than increasing EvoSuite search budgets alone.}
  \label{fig:teralizer-efficiency}
\end{figure}

\begin{table}[H]
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \input{tables/tab-pareto-eqbench}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \input{tables/tab-pareto-commons}
  \end{minipage}
\end{table}

Comparing \VariantNaive{} to \VariantImproved{},
we find that both variants produce
6 Pareto optimal results from their 9 evaluated configurations
for the \DatasetsEqBenchEs{} project
(3 \ToolEvoSuite{} search budgets, each combined with 3 \tries{} settings).
For the \DatasetsCommonsEs{} projects,
\VariantImproved{} has 7 of 9 of results on the Pareto frontier,
compared to only 1 of 9 results produced by \VariantNaive{}.
The underlying causes were previously discussed in RQ1 and RQ2
(Section~\ref{sec:primary-effects-eval} and Section~\ref{sec:ancillary-effects-eval}):
less complex constraints in the \DatasetsEqBenchEs{} project
favor \VariantNaive{}, thus enabling it to be competitive with
\VariantImproved{} despite its less sophisticated input generation approach.
In contrast, constraints are more complex for the \DatasetsCommonsEs{} projects.
This increases the mutation detection rate and runtime advantage that
\VariantImproved{} has over \VariantNaive{}
because the more sophisticated input value generation avoids
\texttt{TooManyFilterMissesException}s.

TODO: Answer to RQ4.
