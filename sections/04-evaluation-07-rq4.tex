\subsection{RQ4: How efficient is test generalization compared to test generation?}
\label{sec:runtime-eval}

We measured \ToolTeralizer{}'s runtime across the seven generalization strategies
to assess viability compared to existing automated testing tools.
Since no existing, publicly available tools perform automated test generalization from unit tests to property-based tests,
we compared efficiency against \ToolEvoSuite{}~\cite{fraser_2011_evosuite}, the established test generation tool.
\VariantShared{} processing stages (i.e., Stages 1--3) were executed only once per project by \ToolTeralizer{}
and collected specifications were then reused across all seven generalization strategies.
Processing required a total time of 3.4 hours for \DatasetCommonsDev{},
8.2--9.8 hours for the three \DatasetsCommonsEs{} variants,
and 24.8--30.9 hours for the three \DatasetsEqBenchEs{} variants (Section~\ref{sec:execution-time}).
Despite the relatively high runtime cost,
Pareto analysis (Section~\ref{sec:execution-efficiency}) shows
that combining low search budget \ToolEvoSuite{} generation with \ToolTeralizer{} generalization
achieved better detection-to-runtime ratios than simply running \ToolEvoSuite{} with higher search budgets
in several configurations.

\subsubsection{Execution Time of \ToolTeralizer{}}
\label{sec:execution-time}

Runtime distribution across processing stages reveals
that test suite reduction via mutation testing
dominates overall computational cost of \ToolTeralizer{}'s processing pipeline.
Figure~\ref{fig:teralizer-runtimes} shows
Stage~5 (test suite reduction) consuming 2,637--42,860 seconds (69.9--96.8\% of total processing time)
across projects and generalization strategies.
In contrast, Stages 1 + 2~(project analysis) require only 82--453 seconds (3.9--6.0\%),
Stage~3 (specification extraction) requires 60--2,776 seconds (4.1--47.8\%),
and Stage~4 (generalized test creation) requires 5--178 seconds (0.2--2.8\%).

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figures/fig_teralizer_runtimes}
  \caption{Teralizer runtimes per project, processing stage, and generalization strategy. Stages 1--3 are \VariantShared{} stages that are only executed once. Stages 4 and 5 are executed once per generalization strategy, i.e., \VariantBaseline{}, \VariantNaives{}, and \VariantImproveds{}.}
  \Description{Grouped bar chart showing runtime breakdown across processing stages for seven projects.
  Y-axis shows runtime in seconds (up to 40000).
  X-axis groups bars by processing stage:
  Original Validation, Specification Extraction, Initial Validation,
  Test Transformation, and Generalization Validation.
  Each group contains bars for different variants (SHARED, BASELINE, NAIVE with 10/50/200 tries,
  and IMPROVED with 10/50/200 tries) shown in different colors.
  Bar heights show runtime with values labeled above each bar.
  Generalization Validation stage shows the highest runtimes (up to 35.5k seconds),
  while Test Transformation shows minimal runtime (5-173 seconds).
  Pattern reveals validation stages dominate total runtime,
  particularly for higher tries settings.}
  \label{fig:teralizer-runtimes}
\end{figure}

\paragraph{Stage 1--4 Runtimes}

The first four processing stages complete efficiently
despite containing the core generalization logic.
Project analysis (Stage 1 + 2) executes the \VariantOriginal{} test suite (61--411s)
and performs simple analyses on the corresponding JUnit reports and test code,
both of which create only minimal overhead beyond the test suite execution (20--42s).
Specification extraction (Stage 3) uses \ToolSPF{}
to concretely execute tests using single-path symbolic analysis,
extracting path conditions and symbolic outputs
without requiring any constraint solving calls
(Section~\ref{sec:specification-extraction}).
While this introduces some overhead
because the JVM implementation of \ToolJPF / \ToolSPF{}
is less optimized than production-ready JVMs
(\ToolJPF{} itself runs inside of a host JVM process \cite{pasareanu_2013_symbolic}),
the cost is comparatively small, representing a one-time cost
of, on average, ca.\ 5$\times$ the runtime of an \VariantOriginal{} test suite execution (mean: 1020s vs. 221s).
Generalized test creation cost (Stage 4) is even smaller at a one-time runtime cost
of 5--177 seconds per variant.
The reason for this low cost is that test creation
only performs syntactic replacements, e.g.,
converting JUnit annotations to \ToolJqwik{} ones,
wrapping input constraint encodings in \texttt{TestParameters} classes,
and replacing expected values in assertions
(Section~\ref{sec:generalized-test-creation}).
%Consequently, Stages~1--4 offer comparatively little opportunity for runtime improvements.

\paragraph{Stage 5 Runtimes}

Test suite reduction costs are substantially larger
than the costs of the preceding generalization stages
due to the high runtime cost of mutation testing.
For example, on the \VariantOriginal{} test suite
without any added property-based tests,
mutation testing requires 6--10$\times$ as much runtime (618--3,469 seconds)
as execution of the \VariantOriginal{} test suite without mutation testing (mean: 1833s vs. 221s).
Mutation testing of the generalized test suites has even higher
runtime requirements (626--23,847 seconds per variant, mean: 4,351s)
than for the \VariantOriginal{} ones (mean ratio: 2.4$\times$).
This is for largely the same reasons
that individual property-based \ToolJqwik{} tests
take longer to execute than conventional JUnit tests:
\ToolJqwik{} overhead, larger number of tested inputs,
as well as filter-and-regenerate cycles
which occur more often for \VariantNaive{} variants
and in the presence of more complex input constraints (Section~\ref{sec:test-suite-execution-time}).
Thus, mutation testing represents more than 99\%
of the total runtime cost of Stage 5 across all projects and generalization variants,
and an average of 55.2\% of the full generalization pipeline.
The small remainder of the Stage 5 runtime costs falls to
collection of test coverage reports as well as processing
of coverage and mutation reports to identify and exclude non-contributing tests.

While it is possible to reduce the one-time cost of generalization by skipping test suite reduction,
this would cause considerable increases to the execution times of the generalized test suites.
After all, the primary purpose of the test suite reduction stage
is to identify and remove generalized tests that do not improve overall fault detection capability,
thus avoiding the high runtime cost
associated with property-based test execution (Section~\ref{sec:test-suite-execution-time}).
In our evaluation, this filtering reduces the number of retained generalized tests
from a total of 65,633 to 4,240 across all projects and test suite variants
(Section~\ref{sec:test-suite-test-count}),
thus demonstrating the impact
that filtering based on mutation scores has on the size and runtime of the generalized test suites.
Even though there is a non-negligible one-time cost associated with this,
that cost amortizes over time compared to a longer-running test suite
that incurs further costs with every test suite execution.
Future optimization efforts could focus on validation efficiency
through faster mutation testing approaches
or lightweight pre-filtering heuristics
that identify likely-beneficial candidates without full mutation testing.

\subsubsection{Efficiency of \ToolTeralizer{} vs.\ \ToolEvoSuite{}}
\label{sec:execution-efficiency}

Our evaluation results show that combining \ToolEvoSuite{}'s test generation with \ToolTeralizer{}'s test generalization
can achieve better detection-to-runtime ratios than simply increasing \ToolEvoSuite{} search budgets.
This efficiency gain stems from their complementary optimization targets:
\ToolEvoSuite{} optimizes primarily for breadth (increasing coverage)~\cite{fraser_2011_evosuite},
while \ToolTeralizer{} optimizes exclusively for depth (testing discovered paths thoroughly).
Figure~\ref{fig:teralizer-efficiency} identifies which tool configurations
provide the best detection-to-runtime trade-offs through Pareto analysis.
Configurations on the frontier represent optimal choices:
for any given runtime budget, they achieve the highest possible detection rate.
Configurations outside the frontier are suboptimal,
i.e., dominated by the Pareto-optimal points,
because the Pareto points achieve higher mutation detection rates in less time.

For example, for the \DatasetsEqBenchEs{} projects,
only 2 of 3 \ToolEvoSuite{} search budget settings are Pareto optimal
(Pareto points \#1 and \#2 in Table~\ref{tab:pareto-eqbench})
due to their comparatively low runtime cost.
\ToolEvoSuite{} generation with a 60 seconds per-class search budget,
on the other hand, falls outside the Pareto frontier with a 
detection rate of 51.6\% and a runtime cost of 55,075 seconds.
In the runtime dimension,
this result is dominated by 1-second \ToolEvoSuite{} generation
combined with \VariantNaiveB{} generalization
(Pareto point \#5 in Table~\ref{tab:pareto-eqbench}),
which achieves 51.7\% detection in 37,532 seconds,
i.e., 0.1 percentage points higher detection with 31.9\% lower runtime cost.
In the mutation detection dimension,
the result is dominated by 10-second \ToolEvoSuite{} generation
combined with \VariantImprovedC{} generalization (Pareto point~\#9),
which produces a 53.8\% detection result in 48,269 seconds, i.e.,
a detection improvement of 2.2 percentage points achieved in 12.4\% less runtime.
Increasing the runtime beyond this comparison point achieves even higher
detection rates in the generalized test suites
that further extend the Pareto frontier (Pareto points \#10--\#14).

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{figures/fig_teralizer_efficiency}
  \caption{Pareto fronts for EvoSuite and Teralizer variants across projects.}
  \Description{Two scatter plots showing Pareto efficiency analysis.
  Left panel shows eqbench project, right panel shows commons-utils project.
  X-axis shows runtime in seconds, Y-axis shows mutation detection percentage.
  Blue circles represent EvoSuite-only configurations,
  red X marks show EvoSuite + NAIVE combinations,
  green triangles show EvoSuite + IMPROVED combinations.
  Dashed black line connects Pareto-optimal configurations numbered 1-14 (eqbench) and 1-11 (commons-utils).
  For eqbench, points show detection rates from 48% to 56% with runtimes from 30000 to 90000 seconds.
  For commons-utils, points show detection rates from 56.5% to 59.5% with runtimes from 5000 to 20000 seconds.
  Pareto frontier demonstrates that combining EvoSuite with Teralizer
  can achieve better detection-to-runtime ratios than increasing EvoSuite search budgets alone.}
  \label{fig:teralizer-efficiency}
\end{figure}

\begin{table}
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \input{tables/tab-pareto-eqbench}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \input{tables/tab-pareto-commons}
  \end{minipage}
\end{table}

Efficiency improvements of \ToolEvoSuite{} + \ToolTeralizer{} combinations
over \ToolEvoSuite{} search budget increases
are less pronounced for the \DatasetsCommonsEs{} projects.
Here, all three \ToolEvoSuite{} search budget settings are Pareto optimal
(Pareto points \#1, \#2, and \#4 in Table~\ref{tab:pareto-commons}).
Nevertheless, generalization via \ToolTeralizer{} contributes
8 additional points to the Pareto frontier.
Specifically, the combination of 1-second \ToolEvoSuite{} generation
and \VariantImprovedA{} generalization (Pareto point \#3)
produces a Pareto optimal result
that has higher detection rate and runtime cost than \#2
but lower detection rate and runtime cost than \#4.
The 7 remaining Pareto points \#5--\#11 again extend the Pareto frontier
toward higher detection rates at higher runtime cost,
reflecting the increased detection rates achievable via generalization
before reaching a plateau at around
1.0--1.3 percentage points above the corresponding \ToolEvoSuite{} results
(Section~\ref{sec:primary-effects-eval}.)

Comparing \VariantNaive{} to \VariantImproved{},
we find that both variants produce
6 Pareto optimal results from their 9 evaluated configurations
for the \DatasetsEqBenchEs{} project
(3 \ToolEvoSuite{} search budgets, each combined with 3 \tries{} settings).
For the \DatasetsCommonsEs{} projects,
\VariantImproved{} has 7 of 9 of results on the Pareto frontier,
compared to only 1 of 9 results produced by \VariantNaive{}.
The underlying causes were previously discussed in RQ2 and RQ3
(Section~\ref{sec:constraint-complexity-eval} and Section~\ref{sec:ancillary-effects-eval}):
less complex constraints in the \DatasetsEqBenchEs{} project
favor \VariantNaive{}, thus enabling it to be competitive with
\VariantImproved{} despite its less sophisticated input generation approach.
In contrast, constraints are more complex for the \DatasetsCommonsEs{} projects.
This increases the mutation detection rate and runtime advantage that
\VariantImproved{} has over \VariantNaive{}
because the more sophisticated input value generation avoids
\texttt{TooManyFilterMissesException}s.

\rqanswerbox{4}{TODO}
