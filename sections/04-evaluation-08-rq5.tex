\subsection{RQ5: What are the causes of unsuccessful generalization attempts under controlled conditions?}
\label{sec:limitations-eval}

While RQ1--4 demonstrated that \ToolTeralizer{} can improve mutation detection rates
and operate within practical time constraints,
our evaluation also revealed that many generalization attempts do not succeed.
This research question investigates
the causes of unsuccessful generalizations
to identify potential directions for future work. 
% limitations that are internal versus external to \ToolTeralizer{},
% and which ones require engineering effort
% versus research advances to resolve.
In RQ5, we first present results for the primary
evaluation dataset, i.e., for the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects,
to establish a baseline under controlled conditions
that align with current tool capabilities
(as explained in Section~\ref{sec:experimental-framework}).
In RQ6, we then present the results under real-world conditions, i.e., for the RepoReapers projects,
which reveal additional challenges that arise
when attempting to fully automate the generalization of test suites in real-world projects.

\ToolTeralizer{} automatically identifies
and excludes tests, assertions, and generalizations (i.e., generalized, property-based tests)
that are beyond current capabilities of the tool via filtering
to focus its resources on suitable generalization candidates (Section~\ref{sec:approach}).
Test-level filtering primarily occurs
during Stage~1 of the processing pipeline,
%(Section~\ref{sec:test-assertion-analysis}),
assertion-level filtering during Stages~1 and 2,
%(Section~\ref{sec:test-assertion-analysis} and Section~\ref{sec:specification-extraction}),
and generalization-level filtering during Stage~4.
%(Section~\ref{sec:generalized-test-creation}).
Beyond exclusions from filtering,
\ToolTeralizer{} also excludes
any tests, assertions, and generalizations
for which processing fails during execution
of the pipeline stages.
Table~\ref{tab:exclusions-breakdown}
quantifies inclusion and exclusion rates
across our primary evaluation dataset
while distinguishing between
filtering-based and failure-based exclusions.
A more fine-grained overview of filtering-based exclusions
is shown in Table~\ref{tab:exclusions-filtering}.
A filtering-based exclusion occurs if at least one
filter rejects. Filters that defer do not cast a vote
because they have insufficient information to make an accept or reject decision.

\paragraph{Test-level Exclusions}

Overall, 19,306 of 23,246 \VariantOriginal{} tests (83.1\%)
across the primary evaluation dataset remain included (Table~\ref{tab:exclusions-breakdown}).
Filtering excludes a total of 3,933 tests (16.9\%) due to filter rejections.
The largest number of tests is rejected by the \texttt{NoAssertions} filter~(10.3\% of tests rejected),
followed by the \texttt{NonPassingTest} filter~(6.6\%)
and the \texttt{TestType} filter~(0.8\%).
Null pointer dereferences that occur during project analysis
exclude 7 additional tests~(0.0\%).

Of the 1,527 \texttt{NonPassingTest} rejections, 132 are tests that fail
after disabling \ToolEvoSuite{}'s isolation and reproducibility features.
These features had to be disabled due to incompatibilities
which caused \ToolPit{} crashes during mutation testing.
However, removal of these features causes \ToolEvoSuite{}-generated tests that rely on system time or specific environmental conditions to fail.
Because \ToolPit{} only supports class-level exclusion,
1,395 additional passing tests in the same classes must also be excluded,
amplifying the impact of individual test failures by over 10$\times$.

All 180 \texttt{TestType} rejections are caused by the presence of
@Parameterized\-Test annotations in the \DatasetCommonsDev{} project.
As described in Section~\ref{sec:test-assertion-analysis},
\ToolTeralizer{} currently supports only standard @Test annotations,
forcing it to exclude @ParameterizedTest, @RepeatedTest,
and other specialized test types from processing.

The \texttt{NoAssertions} filter applies after the two preceding filters,
operating on 21,532 remaining tests from the original 23,246.
From this subset, a total of 2,226 tests are rejected by the filter
because they do not directly contain any assertions in the test method.
In \ToolEvoSuite{}-generated test suites, all \texttt{NoAssertions} exclusions
are genuinely assertion-free tests that pass if no exception occurs during test execution.
In contrast, 69 of 80 \texttt{NoAssertions} exclusions (86.3\%) in the developer-written
\DatasetCommonsDev{} test suite are false positives: these tests
contain assertions in helper methods called from the test method.
However, \ToolTeralizer{}'s current static analysis
only examines the top-level test method,
which causes it to miss these delegated assertion calls
(as described in Section~\ref{sec:test-assertion-analysis}).

\input{tables/tab-exclusions-breakdown}
\input{tables/tab-exclusions-filtering}

\paragraph{Assertion-level Exclusions}

Across the 28,923 identified assertions within the primary evaluation dataset,
a total of 13,836~(47.8\%) are included and 15,087~(52.2\%) are excluded
(Table~\ref{tab:exclusions-filtering}).
Of these exclusions, 12,092 are the result of filtering rejections
whereas 2,995 stem from failures during specification extraction via \ToolSPF{}.

Causes for the 2,995 assertion-level failures include \ToolSPF{} errors, \ToolTeralizer{} errors,
and exceeded analysis limits.
%(timeout, specification size limit, depth limit, etc.).
SPF exceptions constitute 51.4\% of failures (1,540 assertions).
They occur primarily because of missing models for native methods
in the current implementation of \ToolSPF{}
and, to a lesser extent,
because of implementation bugs in \ToolSPF{}/\ToolJPF{}.
Exceeded analysis limits account for 45.3\%
of failed specification extractions (1,358 assertions).
For example, 790 (26.4\%) \ToolSPF{} runs are interrupted
after exceeding the configured maximum specification size,
and 524 (17.5\%) assertions exceed the configured maximum depth limit
(Section~\ref{sec:specification-extraction}).
Both of these aim to avoid timeouts and memory exhaustion
in the presence of complex control flows or complex constraints.
Further failures are due to timeouts (0.9\%, 28 assertions)
and out-of-memory errors (0.5\%, 16 assertions)
which evade preemptive detection via the two preceding measures.
Exceptions due to null pointer dereferences
represent the remaining 3.2\% of failures (97 assertions).

A total of 5.5\% of identified assertions (1,597) are rejected by the ExcludedTest filter
because they belong to tests already excluded at the test level.
This cascading effect ensures consistency across processing stages.

Another 2.6\% of assertions (743) are rejected by the \texttt{AssertionType} filter
because \ToolTeralizer{}
does not offer generalization support for them
(Section~\ref{sec:test-assertion-analysis}).
Specifically, excluded assertions comprise
reference equality checks (assertSame: 142, assertNotSame: 79),
null checks (assertNull: 124, assertNotNull: 86),
array comparisons (assertArrayEquals: 207),
inequality assertions (assertNotEquals: 54),
type checks (assertInstanceOf: 18),
and explicit failures (fail: 33).
These assertions largely involve data types
that are not supported by \ToolTeralizer{}'s
current specification extraction.

The \texttt{MissingValue} filter excludes 24.7\% of assertions.
A rejection occurs when \ToolTeralizer{}'s static analysis
cannot identify which method call represents the method under test (MUT)
or when the declaration of the MUT cannot be
resolved by Spoon (Section~\ref{sec:tested-method-identification}).
Missing values also cause \texttt{ParameterType} and \texttt{VoidReturnType} filters to defer.

The \texttt{ParameterType} filter rejects 15.4\% of assertions
when none of the tested method's parameters
have generalizable types (numeric or boolean).
Deferral numbers are lower than \texttt{MissingValue} rejections
because, in some cases, \ToolTeralizer{} can infer parameter types from the call site
of the MUT even if the full method declaration cannot be resolved.

Finally, the \texttt{VoidReturnType} filter rejects 3 assertions
for tested methods with void return types.
The current implementation of \ToolTeralizer{}
does not support such methods because \ToolSPF{}
does not report any symbolic outputs
(i.e., symbolic representations of return values)
for them. Consequently, no input-output specification can be inferred.

\paragraph{Generalization-level Exclusions}

All filtering-based generalization exclusions in Table~\ref{tab:exclusions-breakdown}
are due to \texttt{Non\-Passing\-Test} rejections.
\VariantBaseline{} generalizations are affected by such rejections
in 0.2\% of cases (22 of 13,836 generalized tests).
This low rejection rate indicates
that the transformation to property-based \ToolJqwik{} tests
is largely successful in producing compilable code
that matches the behavior of the original JUnit tests.
The 22 test failures that force \texttt{NonPassingTest} to reject
occur for tests in the \DatasetCommonsDev{} project
that call MUTs or assertions inside of a loop.
This is problematic because \ToolTeralizer{}'s current implementation
does not properly account for loops.
As a result, the \VariantBaseline{} generalization strategy
replaces the \texttt{expected} value of assertions inside of loops
with the concrete input value of the 
first loop iteration, which commonly causes later loop iterations
to fail with an \texttt{AssertionError}.

\VariantNaive{} generalizations
show the largest number of filtering-based exclusions:
2,233 (22.2\%) for \VariantNaiveA{},
2,938 (27.8\%) for \VariantNaiveB{},
and 3,005 (28.4\%) for \VariantNaiveC{}.
The primary failure cause is \texttt{TooManyFilterMissesExceptions}.
These exceptions are thrown by \ToolJqwik{}
when it cannot generate inputs that satisfy path constraints within its retry limit.
Prevalence of these exceptions increases with higher \tries{} values
as \VariantNaive{} input generation struggles to produce valid inputs,
especially for more complex path constraints
(Section~\ref{sec:constraint-complexity-eval}).
The remaining filtering-based exclusions occur due to 
inaccurate input/output specifications which cause exceptions
during assertion checking (\texttt{AssertionFailedError}, 729 / 803 / 819 exclusions)
or general test execution (\texttt{ArithmeticException}, 99 / 99 / 99 exclusions).
Underlying causes of inaccurate specifications include
(i) assertions in loops,
(ii) assertions in transitively called methods,
(iii) tested methods called within loops,
and (iv) implicit preconditions.
While (i)--(iii) are clear limitations of \ToolTeralizer{},
(iv) can indicate unintended behavior in \VariantOriginal{} tests
such as potential divisions by 0 or under-/overflows that evade detection
by \VariantOriginal{} tests but are identified by the generalized tests created by \ToolTeralizer{}.

\VariantImproved{} generalization strategies
reduce overall exclusion rates from filtering-based rejections
to 14.6--16.0\% through constraint-aware input value generation
(Section~\ref{sec:constraint-complexity-eval}).
Specifically, \texttt{TooManyFilterMissesExceptions} decrease
from 2,223 to 1,189 ($-$46.5\%) for \VariantNaiveA{} vs.\ \VariantImprovedA{},
from 2,938 to 1,223 ($-$58.4\%) for \VariantNaiveB{} vs.\ \VariantImprovedB{},
and from 3,005 to 1,265 ($-$57.9\%) for \VariantNaiveC{} vs.\ \VariantImprovedC{}.
This clearly demonstrates that \VariantImproved{} input generation is more effective
at producing inputs that satisfy identified input constraints.
Because more generalizations pass early input filtering,
the number of test failures due to later \texttt{AssertionFailedErrors} and \texttt{ArithmeticExceptions}
often increases, albeit to a lesser degree.
For \VariantNaiveA{} vs.\ \VariantImprovedA{}, the number changes from 828 to 827 ($-$0.1\%),
for \VariantNaiveB{} vs.\ \VariantImprovedB{} from 902 to 921 ($+$2.1\%),
and for \VariantNaiveC{} vs.\ \VariantImprovedC{} from 918 to 942 ($+$2.6\%).

Project-specific patterns show how test characteristics
and constraint complexity affect generalization success.
Developer-written tests in \DatasetCommonsDev{}
exhibit 11.5--14.2\% \texttt{TooManyFilterMissesException} rates
and 24.7--25.7\% from inaccurate specifications.
\ToolEvoSuite{}-generated tests in \DatasetsCommonsEs{} projects show 14.9--15.7\% and 12.7--15.3\%, respectively,
while \ToolEvoSuite{}-generated tests in \DatasetsEqBenchEs{} projects show 5.7--6.0\% and 1.3\%.
The 2$\times$ higher inaccurate specification failures
in developer-written \DatasetCommonsDev{} tests
compared to \ToolEvoSuite{}-generated \DatasetsCommonsEs{} tests
reflects differences in test construction:
\ToolEvoSuite{}-generated tests avoid loops and do not invoke assertions through helper methods,
while examined developer-written tests commonly use both patterns.
The higher rates in \ToolEvoSuite{}-generated \DatasetsCommonsEs{} tests
compared to \ToolEvoSuite{}-generated \DatasetsEqBenchEs{} tests --- despite identical test construction
patterns --- reflects the impact of constraint complexity (Section~\ref{sec:constraint-complexity-eval}).

Beyond filter rejections, \VariantNaive{} and \VariantImproved{} generalization strategies
fail 32 generalization attempts
to avoid Java's ``code too large'' compilation error.
This error occurs when a method's bytecode exceeds 64KB,
Java's hard limit for method size.
Generated property-based tests with large input/output specifications
can exceed this limit when specifications contain
numerous complex constraints,
necessitating preemptive exclusion of the largest specifications.

\rqanswerbox{5}{TODO}
