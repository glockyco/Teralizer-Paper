\subsection{RQ5: What are the causes of unsuccessful generalization attempts under controlled conditions?}
\label{sec:limitations-eval}

While RQ1--4 demonstrate that \ToolTeralizer{} can improve mutation detection rates
and operate within practical time constraints,
our evaluation also revealed that many generalization attempts do not succeed.
In RQ5, we first present the results for the primary
evaluation dataset, i.e., for the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects,
to establish a baseline under controlled conditions
that align with current tool capabilities
(as explained in Section~\ref{sec:experimental-framework}).
In RQ6, we then investigate the results for the RepoReapers projects
to better understand real-world applicability challenges.

As explained in Section~\ref{sec:approach},
there are two different types of causes
based on which \ToolTeralizer{} may exclude individual tests, assertions or generalizations
from further processing:
filtering and failures.
Filtering preemptively excludes cases that are beyond the current capabilities of \ToolTeralizer{}
to focus on suitable generalization candidates.
Even though a single filter rejection is enough
to exclude a given test, assertion or generalization,
\ToolTeralizer{} generally collects responses from all
applicable filters to enable a more robust analysis of filtering causes.
Despite preemptive filtering,
some processing attempts fail due to exceptions that are thrown at runtime,
prompting \ToolTeralizer{} to exclude the corresponding
test, assertion or generalization from further processing once such an exception occurs.

Table~\ref{tab:exclusions-breakdown}
quantifies inclusion and exclusion rates
of tests, assertion, and generalizations
while distinguishing between
filtering-based and failure-based exclusions.
A more fine-grained overview of filtering results
is shown in Table~\ref{tab:exclusions-filtering}.
Filters that defer do not cast a vote
because they have insufficient information to make an accept or reject decision.

\paragraph{Test-level Exclusions}

Overall, 19,306 of 23,246 \VariantOriginal{} tests (83.1\%)
across all variants of the primary evaluation dataset remain included.
Filtering excludes 3,933 tests (16.9\%) due to filter rejections.
The largest number of tests is rejected by the \texttt{NoAssertions} filter~(10.3\% of tests rejected),
followed by the \texttt{NonPassingTest} filter~(6.6\%)
and the \texttt{TestType} filter~(0.8\%).
Null pointer dereferences that occur during project analysis
exclude 7 additional tests~(0.0\%).

Of the 1,527 \texttt{NonPassingTest} rejections, 132 are tests that fail
after disabling \ToolEvoSuite{}'s isolation and reproducibility features.
These features had to be disabled due to incompatibilities
which caused \ToolPit{} crashes during mutation testing.
However, removal of these features causes \ToolEvoSuite{}-generated tests that rely on system time or specific environmental conditions to fail.
Because \ToolPit{} only supports class-level exclusion,
1,395 additional passing tests in the same classes are also excluded as a side effect,
amplifying the impact of individual test failures by over 10$\times$.

All 180 \texttt{TestType} rejections are caused by the presence of
@Parameterized\-Test annotations in the \DatasetCommonsDev{} project.
As described in Section~\ref{sec:test-assertion-analysis},
\ToolTeralizer{} currently supports only standard @Test annotations,
forcing it to exclude @ParameterizedTest, @RepeatedTest,
and other specialized test types from processing.

The \texttt{NoAssertions} filter operates on
21,532 tests that remain 
after exclusions due to test-level failures (7 tests)
and rejections by the preceeding filters (1527 + 180 tests).
From this subset, 2,226 tests are rejected
because they do not directly contain any assertions in the test method.
In \ToolEvoSuite{}-generated test suites, all \texttt{NoAssertions} exclusions
are genuinely assertion-free tests that pass if no exception occurs during test execution.
In contrast, 69 of 80 \texttt{NoAssertions} exclusions (86.3\%) in the developer-written
\DatasetCommonsDev{} test suite are false positives: these tests
contain assertions in helper methods called from the test method.
However, \ToolTeralizer{}'s current static analysis
only examines the top-level test method,
which causes it to miss these delegated assertion calls
(as described in Section~\ref{sec:test-assertion-analysis}).

\paragraph{Assertion-level Exclusions}
\label{par:assertion-level-exclusions}

Across the 28,923 identified assertions within the primary evaluation dataset,
a total of 13,836~(47.8\%) are included and 15,087~(52.2\%) are excluded.
Of these exclusions, 12,092 are the result of filtering rejections
whereas 2,995 stem from failures during specification extraction via \ToolSPF{}.

Causes for the 2,995 assertion-level failures include \ToolSPF{} errors, \ToolTeralizer{} errors,
and exceeded analysis limits.
%(timeout, specification size limit, depth limit, etc.).
SPF exceptions constitute 51.4\% of failures (1,540 assertions).
They occur primarily due to missing models for native methods
in the current implementation of \ToolSPF{}
and, to a lesser extent,
due to implementation bugs in \ToolSPF{}/\ToolJPF{}.
Exceeded analysis limits account for 45.3\%
of failures (1,358 assertions):
790 (26.4\%) \ToolSPF{} runs are interrupted
after exceeding the maximum specification size,
and 524 (17.5\%) runs exceed the maximum depth limit
(Listing~\ref{lst:jpf-config}).
Both of these aim to avoid timeouts and memory exhaustion
in the presence of complex control flows or complex constraints.
Further failures are due to timeouts (0.9\%, 28 assertions)
and out-of-memory errors (0.5\%, 16 assertions)
which evade preemptive detection via the two preceeding measures.
NullPointerExceptions represent the remaining 3.2\% of failures (97 assertions).

A total of 5.5\% of identified assertions (1,597) are rejected by the \texttt{ExcludedTest} filter
because they belong to tests that are already excluded at the test level.
This cascading effect ensures consistency across processing stages.

Another 2.6\% of assertions (743) are rejected by the \texttt{AssertionType} filter.
Excluded assertions comprise
reference equality checks (assertSame: 142, assertNotSame: 79),
null checks (assertNull: 124, assertNotNull: 86),
array comparisons (assertArrayEquals: 207),
inequality assertions (assertNotEquals: 54),
type checks (assertInstanceOf: 18),
and explicit failures (fail: 33).
These assertions largely involve data types
that are not supported by current symbolic analysis.

The \texttt{MissingValue} filter excludes 24.7\% of assertions.
A rejection occurs when \ToolTeralizer{}'s static analysis
cannot identify which method call represents the method under test (MUT)
or when the declaration of the MUT cannot be
resolved by Spoon (Section~\ref{sec:tested-method-identification}).
Missing values also cause \texttt{ParameterType} and \texttt{VoidReturnType} filters to defer.

The \texttt{ParameterType} filter rejects 15.4\% of assertions
where none of the tested method's parameters
have generalizable types (numeric or boolean).
Deferral numbers are lower than \texttt{MissingValue} rejections
because, in some cases, \ToolTeralizer{} can infer parameter types from the call site
of the MUT even if the full method declaration cannot be resolved.

Finally, the \texttt{VoidReturnType} filter rejects 3 assertions
for tested methods with void return types.
The current implementation of \ToolTeralizer{} does not support such methods
because no output specification can be inferred for them.

\input{tables/tab-exclusions-breakdown}
\input{tables/tab-exclusions-filtering}

\paragraph{Generalization-level Exclusions}
\label{par:generalization-level-exclusions}

All filtering-based generalization exclusions in Table~\ref{tab:exclusions-breakdown}
are due to \texttt{Non\-Passing\-Test} rejections.
\VariantBaseline{} generalizations are affected by such rejections
in 0.2\% of cases (22 of 13,836 generalized tests).
This low rejection rate indicates
that the transformation to \ToolJqwik{} tests is largely successful.
The 22 test failures that force \texttt{NonPassingTest} to reject
occur for tests in the \DatasetCommonsDev{} project
that call MUTs or assertions within a loop.
This is problematic because \ToolTeralizer{}'s current implementation
does not properly account for loops.
As a result, the \VariantBaseline{} generalization strategy
replaces the \texttt{expected} value of assertions within loops
with the concrete input value of the 
first loop iteration, which commonly causes later loop iterations
to fail with an \texttt{AssertionError}.

\VariantNaive{}
shows 3,061--3,923 (22.1--28.4\%) filtering-based exclusions.
The primary cause is \texttt{Too\-Many\-Filter\-Misses\-Exceptions},
which occur 2,233 times (16.1\%) for \VariantNaiveA{},
2,938 (21.2\%) for \VariantNaiveB{},
and 3,005 (21.7\%) for \VariantNaiveC{}.
Prevalence increases with higher \tries{}
as \VariantNaive{} struggles to produce enough valid inputs,
especially for more complex constraints
(Section~\ref{sec:constraint-complexity-eval}).
The remaining filtering-based exclusions are due to 
inaccurate input/output specifications which cause exceptions
during assertion checking (\texttt{AssertionFailedError}, 729 / 803 / 819 exclusions by \textsc{Naive$_{10/50/200}$})
or general test execution (\texttt{ArithmeticException}, 99 / 99 / 99 exclusions).
Underlying causes include
(i) assertions in loops,
(ii) assertions in transitively called methods,
(iii) MUT calls within loops,
and (iv) implicit preconditions.
While (i)--(iii) are limitations of \ToolTeralizer{},
(iv)~can indicate unintended behavior
such as potential divisions by 0 or under-/overflows that evade detection
by the \VariantOriginal{} tests but are identified by the generalized tests.

\VariantImproved{} generalization strategies
reduce overall exclusion rates from filtering-based rejections
to 2,016--2,207 (14.6--16.0\%) through constraint-aware input value generation
(Section~\ref{sec:constraint-complexity-eval}).
Specifically, \texttt{Too\-Many\-Filter\-Misses\-Exceptions} decrease
from 2,223 to 1,189 ($-$46.5\%) for \VariantNaiveA{} vs.\ \VariantImprovedA{},
from 2,938 to 1,223 ($-$58.4\%) for \VariantNaiveB{} vs.\ \VariantImprovedB{},
and from 3,005 to 1,265 ($-$57.9\%) for \VariantNaiveC{} vs.\ \VariantImprovedC{}.
This clearly demonstrates that \VariantImproved{} input generation is more effective
at producing inputs that satisfy identified input constraints.
Because more generalizations pass early input filtering,
the number of test failures due to later \texttt{AssertionFailedErrors} and \texttt{ArithmeticExceptions}
often increases, albeit to a lesser degree.
For \VariantNaiveA{} vs.\ \VariantImprovedA{}, the number changes from 828 to 827 ($-$0.1\%),
for \VariantNaiveB{} vs.\ \VariantImprovedB{} from 902 to 921 ($+$2.1\%),
and for \VariantNaiveC{} vs.\ \VariantImprovedC{} from 918 to 942 ($+$2.6\%).

Project-specific patterns show how test characteristics
and constraint complexity affect generalization success.
Developer-written tests in \DatasetCommonsDev{}
exhibit 11.5--14.2\% \texttt{Too\-Many\-Filter\-Misses\-Exception}s
and 24.7--25.7\% from inaccurate specifications.
\ToolEvoSuite{}-generated tests in \DatasetsCommonsEs{} show 14.9--15.7\% and 12.7--15.3\%, respectively,
while \ToolEvoSuite{}-generated tests in \DatasetsEqBenchEs{} show 5.7--6.0\% and 1.3\%.
The 2$\times$ higher inaccurate specification rate
in \DatasetCommonsDev{} compared to \DatasetsCommonsEs{}
reflects test construction differences:
\ToolEvoSuite{}-generated tests avoid loops and do not invoke assertions through helper methods,
while the developer-written tests commonly use both patterns.
The higher rates in \DatasetsCommonsEs{} tests
compared to \DatasetsEqBenchEs{} tests --- despite identical test construction
patterns --- reflects the impact of constraint complexity (Section~\ref{sec:constraint-complexity-eval}).

Beyond filter rejections, \VariantNaive{} and \VariantImproved{}
fail 32 generalizations
to avoid ``code too large'' errors.
This error occurs when a method's bytecode exceeds 64KB,
Java's hard limit for method size.
Generalized tests can exceed this limit
when specifications contain many complex constraints,
necessitating preemptive exclusion of the largest specifications.

\rqanswerbox{5}{
  Of 28,923 identified assertions,
  9,881--13,836 (34.2--47.8\%) are successfully generalized
  across the three strategies.
  Most exclusions occur at the assertion level,
  primarily due to static analysis limitations in identifying tested methods (24.7\%)
  and the presence of parameter types that cannot be accurately modeled by current symbolic analysis (15.4\%).
  \ToolSPF{} errors and exceeded analysis limits
  exclude 5.3\% and 4.7\% of assertions, respectively.
  Test-level filtering excludes 5.5\% of assertions due to non-passing \VariantOriginal{} tests.
  \VariantNaive{} generalized tests pass in 71.4--77.6\% of cases,
  with failures primarily due to \texttt{Too\-Many\-Filter\-Misses\-Exceptions} (16.1--21.7\%)
  and inaccurate specifications in the presence of
  loops and interprocedural control flow in tests (6.0--6.6\%).
  \VariantImproved{} increases pass rates to 83.8--85.2\%
  by reducing filter misses by 46.5--58.4\% through constraint-aware input generation.
}
