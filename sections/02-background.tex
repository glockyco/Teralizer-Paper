\section{Background}
\label{sec:background}

This section provides the technical foundations for semantics-based test generalization.
We first situate our work within the broader landscape of test amplification,
distinguishing generalization from other enhancement strategies.
We then introduce property-based testing as our target representation,
highlighting its ability to validate entire input partitions rather than single test cases.
Next, we describe single-path symbolic analysis,
the key technique for extracting path-exact specifications from existing test executions.
Finally, we present mutation testing as our evaluation methodology
for assessing the effectiveness of generalized tests.

\subsection{Test Amplification and Generalization}
\label{sec:test-amplification}

Test amplification exploits knowledge embedded in existing test cases (e.g.,
meaningful input data and expected outputs encoded as assertions) to
automatically enhance manually written tests to support specific engineering goals~\cite{danglot_2019_snowballing}.
Amplification techniques go beyond simply maximizing coverage,
targeting diverse objectives ranging from regression detection to fault localization.
\citeauthor{danglot_2019_snowballing}'s taxonomy~\cite{danglot_2019_snowballing} distinguishes four categories of amplification:
AMP\textsubscript{add} creates new tests from existing ones,
AMP\textsubscript{change} targets specific program modifications,
AMP\textsubscript{exec} varies execution conditions, and
AMP\textsubscript{mod} modifies test structure or assertions to generalize behavior.

Test generalization belongs to the AMP\textsubscript{mod} category,
transforming tests from validating individual input-output pairs
to validating properties across entire input partitions.
For example, a test verifying that \texttt{abs(0)} returns \texttt{0}
validates this relationship for only one input,
missing mutations that preserve behavior at that point but violate the general property.
A mutation changing \texttt{x >= 0} to \texttt{x == 0}
still passes the original test but fails for positive inputs (Figure~\ref{fig:regression-detection}).
Generalizing such tests requires extracting path-exact specifications
that capture program behavior across input partitions
while preserving the oracles developers have encoded in their assertions.

Central to all test amplification approaches is the oracle problem:
determining expected outputs for new test inputs~\cite{barr_2015_oracle}.
Existing tests provide validated oracles for their specific execution paths,
encoding developer knowledge about expected behavior.
Other execution paths lack equally trustworthy oracles,
making it difficult to distinguish intentional behavior
from incidental state changes or outputs.
This constraint fundamentally shapes how different amplification techniques
approach the generation of new test inputs and their corresponding assertions.

\subsection{Property-Based Testing as Target Representation}
\label{sec:property-based-testing}

Property-based testing (PBT), pioneered by QuickCheck for Haskell~\cite{claessen_2000_quickcheck}
and now available through frameworks like ScalaCheck for Scala~\cite{nilsson_2014_scalacheck},
Hypothesis for Python~\cite{maciver_2019_hypothesis}, and \ToolJqwik{} for Java~\cite{link_2022_jqwik},
validates specifications over entire input partitions rather than single test cases.
Where a conventional test might verify \texttt{sqrt(4)} returns \texttt{2},
a property-based test instead verifies \texttt{sqrt(x) * sqrt(x) = x}
across hundreds of randomly generated non-negative values.
PBT frameworks comprise three key components.
\emph{Generators} produce inputs according to specified constraints, such as \texttt{x >= 0}.
\emph{Properties} express invariants that must hold for all generated inputs, such as \texttt{abs(x) == x}.
\emph{Shrinking} minimizes failing inputs to simplify debugging,
such as reducing a failing input from 776837 to 1 when the same property violation occurs.

For example, the property-based test in Figure~\ref{fig:regression-detection}
uses \texttt{@Property} to indicate property-based testing,
\texttt{@Int(min=0)} to constrain input generation to non-negative integers,
and validates that \texttt{assertEquals(x, abs(x))} holds for all generated values.
When this test executes, \ToolJqwik{} generates hundreds of non-negative integers,
including edge cases like 0, 1, and \texttt{Integer.MAX\_VALUE},
and verifies the property holds for each.
If a failure occurs, the framework's shrinking algorithm automatically reduces
the failing input to its minimal form, simplifying debugging.

The combination of constrained generation and property checking
enables thorough exploration of input spaces,
revealing edge cases and boundary conditions
that developers might not explicitly consider~\cite{hughes_2016_experiences,maciver_2019_hypothesis}.
However, adoption remains limited in practice~\cite{goldstein_2024_pbt_practice}.
Creating effective property-based tests requires identifying appropriate properties,
defining input generators with suitable constraints,
and translating example-based assertions into general specifications: a conceptual shift
that creates significant adoption barriers~\cite{barr_2015_oracle,tillmann_2005_parameterized}.
Nevertheless, conventional unit tests already encode behavioral properties implicitly:
a test verifying \texttt{abs(0) = 0} reflects the property \texttt{abs(x) = x} for non-negative inputs,
but validates it at only a single point.

\subsection{Symbolic Analysis for Specification Extraction}
\label{sec:symbolic-analysis}

Automating the transformation from conventional tests into property-based specifications
requires extracting two elements:
the \emph{path condition} that characterizes inputs following the same execution path,
and the \emph{symbolic output expression} that computes expected results for those inputs.
For example, the \texttt{abs(0)} test in Figure~\ref{fig:generalization} yields
the path condition \texttt{x >= 0} and the symbolic output \texttt{x}.
These path-exact specifications enable property-based tests that validate behavior
across entire input partitions while preserving the original test's semantics.

Single-path symbolic analysis achieves this extraction by following the concrete execution path of an existing test
while maintaining symbolic representations of variables~\cite{pasareanu_2013_symbolic}.
Unlike full symbolic execution, which faces path explosion
when exploring all possible paths~\cite{baldoni_2018_survey,cadar_2013_symbolic},
single-path analysis omits backtracking and constraint solving,
recording conditions only along the executed path.
This focused approach is well suited to test generalization:
existing tests already identify the behaviors of interest
and provide validated oracles for those behaviors.
The analysis thus extracts path-exact specifications
without the computational overhead of exploring alternative branches.

The precision of extracted specifications depends strongly on the types of
values involved. Linear integer constraints (e.g., \texttt{x > 0},
\texttt{y <= 2*x}) are well supported by symbolic execution tools such as
Symbolic PathFinder (SPF) for Java~\cite{pasareanu_2013_symbolic} and KLEE for
C~\cite{cadar_2008_klee},
which can both record exact conditions and solve them efficiently. Non-linear
arithmetic and floating-point operations are more problematic: although tools can still
represent them precisely through symbolic formulas, constraint solving quickly becomes computationally intractable,
leading to timeouts~\cite{de_moura_2008_z3}. Strings,
arrays, and complex objects pose the greatest practical barrier: symbolic representations
typically lose precision or become overly abstract, limiting their usefulness
for specification extraction~\cite{baldoni_2018_survey,amadini_2021_string_survey}.

These limitations affect test generalization at two distinct stages.
First, imprecise specifications (as with complex types) prevent generalization entirely
since we cannot create meaningful property-based tests without accurate models.
Second, even with precise specifications (as with non-linear numeric constraints),
test generalization succeeds but the resulting tests may fail during execution
when PBT frameworks cannot efficiently produce inputs satisfying complex constraints.
This input generation difficulty represents a fundamental computational challenge
that frameworks cannot overcome through filtering or constraint encoding~\cite{claessen_2000_quickcheck,link_2022_jqwik}.
Thus, while test generalization can theoretically handle any accurately modeled behavior,
practical success requires both precise specifications and tractable constraints.

\subsection{Mutation Testing for Evaluation}
\label{sec:mutation-testing}

Having established how to extract specifications from existing tests,
we require a systematic way to assess whether transforming tests based on these specifications
improves effectiveness.
Because original and generalized tests execute the same paths, traditional
coverage metrics cannot reveal improvements~\cite{inozemtseva_2014_coverage}.
Statement, branch, and path coverage~\cite{zhu_1997_software}
remain identical whether a test validates one input or hundreds from the same partition.
Mutation testing, in contrast, reflects the ability of a test suite to expose
behavioral differences within those paths, making it the appropriate measure for
evaluating test generalization~\cite{jia_2011_analysis}.

Mutation testing rests on two hypotheses: the competent programmer hypothesis
(real faults are small deviations from correct programs) and the coupling effect
(tests that detect simple faults also detect more complex ones)~\cite{offutt_1992_investigations}.
These hypotheses support using small syntactic changes, i.e., mutations, as proxies for real programming errors.
Mutation operators systematically alter program statements
to create mutants, each containing a single fault.
Common mutation operators include arithmetic replacements (e.g., \texttt{+} to
\texttt{-}), relational boundary shifts (e.g., \texttt{>} to \texttt{>=}),
logical connector changes (e.g., \texttt{\&\&} to \texttt{||}), constant
modifications, and replacements of return values with defaults such as
\texttt{0}, \texttt{true}, or \texttt{null}~\cite{jia_2011_analysis}.
A test suite's mutation score, i.e., the proportion of mutants killed,
provides a quantitative measure of test effectiveness
that correlates with real fault detection capability~\cite{just_2014_mutants,papadakis_2019_mutation}.

Mutation testing also clarifies the limitations of single-input tests
when evaluating amplification techniques.
The unit test verifying \texttt{abs(0) = 0}
cannot kill a mutant changing \texttt{x >= 0} to \texttt{x == 0},
because the test's single input still satisfies the mutated condition.
A generalized property-based test
that exercises the same execution path with multiple inputs
will detect this mutant when positive values produce negative results.
The difference in mutants killed by the original and generalized suites
quantifies the improvement in fault detection.
Tools such as \ToolPit{}~\cite{coles_2016_pit} support this evaluation for Java programs.
