\section{Background}
\label{sec:background}

This section provides the technical foundations for semantics-based test generalization.
Section~\ref{sec:test-amplification} situates our work
within the test amplification landscape.
Section~\ref{sec:property-based-testing}
introduces property-based testing as our target representation,
highlighting its ability to validate entire input partitions rather than single test cases.
Section~\ref{sec:symbolic-analysis} describes single-path symbolic analysis,
the technique we use to extract path-exact specifications from existing test executions.
Finally, Section~\ref{sec:mutation-testing}
presents mutation testing as our evaluation methodology
for assessing the effectiveness of generalized tests.

\subsection{Test Amplification and Generalization}
\label{sec:test-amplification}

Test amplification uses knowledge embedded in implementations and tests of software projects
to automatically enhance the projects' test suites.
\citeauthor{danglot_2019_snowballing}'s taxonomy~\cite{danglot_2019_snowballing}
distinguishes four categories of amplification:
AMP\textsubscript{add} creates new tests from existing ones,
AMP\textsubscript{change} targets specific program modifications,
AMP\textsubscript{exec} varies execution conditions,
and AMP\textsubscript{mod} modifies test structure or assertions to generalize behavior.

Test generalization belongs to the AMP\textsubscript{mod} category. 
It transforms tests from validating individual input-output pairs
to validating properties across entire input partitions.
For example, a test which asserts that $abs(0)$ returns $0$
validates $abs$ for only a single input-output pair,
missing regressions that preserve the behavior at that point
but violate the general property $abs(x) = x$
which should hold when $x \geq 0$
(Figure~\ref{fig:regression-detection}).
Since this property is implicitly encoded in the original test,
we can automatically transform the test
into a corresponding property-based test (Figure~\ref{fig:generalization}).

A central challenge in test amplification approaches is the oracle problem:
determining expected outputs for new test inputs~\cite{barr_2015_oracle}.
Existing tests provide validated oracles for their specific execution paths,
encoding developer knowledge about expected behavior.
Other execution paths lack equally trustworthy oracles,
making it difficult to distinguish intentional behavior
from incidental state changes or outputs.

\subsection{Property-Based Testing as Target Representation}
\label{sec:property-based-testing}

Property-based testing (PBT), pioneered by QuickCheck for Haskell~\cite{claessen_2000_quickcheck}
and now available through frameworks like ScalaCheck for Scala~\cite{nilsson_2014_scalacheck},
Hypothesis for Python~\cite{maciver_2019_hypothesis}, and \ToolJqwik{} for Java~\cite{link_2022_jqwik},
validates specifications over entire input partitions rather than single test cases.
%
PBT frameworks comprise three key components.
\emph{Generators} produce inputs according to specified constraints, such as $x \geq 0$.
\emph{Properties} express invariants that must hold for all generated inputs,
such as $abs(x) = x$ for non-negative $x$.
\emph{Shrinking} minimizes failing inputs to simplify debugging,
such as reducing a failing input from 776,837 to 1.
This generative approach distinguishes PBT from parameterized testing,
which commonly relies on a predefined set of developer-supplied inputs.

For example, the property-based test in Figure~\ref{fig:regression-detection}
uses \texttt{@Property} to indicate property-based testing
and \texttt{@Int(min=0)} to constrain input generation to non-negative integers.
It then validates that \texttt{assertEquals(x, abs(x))} holds for all generated values.
When this test executes, \ToolJqwik{} generates hundreds of non-negative integers,
including edge cases like 0, 1, and \texttt{Integer.MAX\_VALUE},
and checks that the property holds for each one.
If a failure occurs, the framework's shrinking algorithm automatically reduces
the failing input to its minimal form, simplifying debugging.

The combination of constrained generation and property checking
enables thorough exploration of input spaces,
revealing edge cases
that developers might not explicitly consider~\cite{hughes_2016_experiences,maciver_2019_hypothesis}.
However, adoption of property-based testing remains limited~\cite{goldstein_2024_pbt_practice}.
Creating property-based tests requires identifying appropriate properties,
defining input generators with suitable constraints,
and translating example-based assertions into general specifications: a conceptual shift
that can be difficult for developers~\cite{barr_2015_oracle,tillmann_2005_parameterized}
even though conventional unit tests already encode behavioral properties implicitly:
an assertion $abs(0) = 0$ reflects the property $abs(x) = x$ for $x \geq 0$
but validates it only for a single input.

\subsection{Symbolic Analysis for Specification Extraction}
\label{sec:symbolic-analysis}

Automating the transformation from conventional tests into property-based specifications
requires extracting two elements:
the \emph{path condition} that characterizes inputs following the same execution path,
and the \emph{symbolic output expression} that computes expected results for those inputs.
For example, the $abs(0)$ test in Figure~\ref{fig:generalization} yields
the path condition $x \geq 0$ and the symbolic output $x$.
These path-exact specifications enable property-based tests that validate behavior
across entire input partitions while preserving the original test's semantics.

Single-path symbolic analysis achieves this extraction by following the concrete execution path of an existing test
while maintaining symbolic representations of variables~\cite{pasareanu_2013_symbolic}.
Unlike full symbolic execution, which faces path explosion
when exploring all possible paths~\cite{baldoni_2018_survey,cadar_2013_symbolic},
single-path analysis omits backtracking and constraint solving,
recording conditions only along the executed path.
This focused approach is well suited to test generalization:
existing tests identify the behaviors of interest
and provide validated oracles for those behaviors.
The analysis thus extracts path-exact specifications
without the computational overhead of exploring alternative branches.

The precision of extracted specifications depends strongly on the types of
values involved. Linear integer constraints (e.g., $x > 0$,
$y \leq 2 \cdot x$) are well supported by symbolic execution tools such as
Symbolic PathFinder (SPF) for Java~\cite{pasareanu_2013_symbolic} and KLEE for
C~\cite{cadar_2008_klee},
which can both record exact conditions and solve them efficiently. Non-linear
arithmetic and floating-point operations are more problematic: although tools can still
represent them precisely through symbolic formulas, constraint solving quickly becomes computationally intractable,
leading to timeouts~\cite{de_moura_2008_z3}. Strings,
arrays, and complex objects pose the greatest practical barrier: symbolic representations
typically lose precision or become overly abstract, limiting their usefulness
for specification extraction~\cite{baldoni_2018_survey,amadini_2021_string_survey}.

These limitations affect test generalization at two distinct stages.
First, imprecise specifications (as with complex types) prevent generalization entirely
since we cannot create meaningful property-based tests without accurate models.
Second, even with precise specifications (as with non-linear numeric constraints),
test generalization succeeds but the resulting tests may fail during execution
when PBT frameworks cannot produce inputs satisfying complex constraints.
This input generation difficulty represents a fundamental computational challenge
that frameworks cannot overcome through filtering or constraint encoding~\cite{claessen_2000_quickcheck,link_2022_jqwik}.
Thus, while test generalization can theoretically handle any accurately modeled behavior,
practical success requires both precise specifications and tractable constraints.

\subsection{Mutation Testing for Evaluation}
\label{sec:mutation-testing}

Having established how to extract specifications from existing tests,
we require a systematic way to assess whether transforming tests based on these specifications
improves fault detection capability.
Because original and generalized tests execute the same paths, traditional
coverage metrics cannot reveal improvements~\cite{inozemtseva_2014_coverage}.
Statement, branch, and path coverage~\cite{zhu_1997_software}
remain identical whether a test validates one input or hundreds from the same partition.
Mutation testing, in contrast, reflects the ability of a test suite to expose
behavioral differences within those paths, making it a suitable metric for evaluating
which generalized tests provide additional fault detection capability~\cite{jia_2011_analysis}.

Mutation testing systematically introduces small faults into program code
and measures whether test suites detect them.
The approach rests on two hypotheses: the competent programmer hypothesis
(real faults are small deviations from correct programs) and the coupling effect
(tests that detect simple faults also detect more complex ones)~\cite{offutt_1992_investigations}.
These hypotheses justify using small syntactic changes as proxies for real programming errors.
Mutation operators systematically alter program statements to create mutants,
each containing a single fault.
Common operators include arithmetic replacements (e.g., \texttt{+} to \texttt{-}),
relational boundary shifts (e.g., \texttt{>} to \texttt{>=}),
logical connector changes (e.g., \texttt{\&\&} to \texttt{||}),
constant modifications, and replacements of return values with defaults
such as \texttt{0}, \texttt{true}, or \texttt{null}~\cite{jia_2011_analysis}.
A test suite's mutation score,
which is calculated as the proportion of mutants the test suite detects ("kills"),
provides a quantitative measure that correlates with real fault detection capability~\cite{just_2014_mutants,papadakis_2019_mutation}.

The limitations of single-input tests become clear through mutation analysis.
A unit test verifying $abs(0) = 0$
cannot kill a mutant changing \texttt{x >= 0} to \texttt{x == 0},
because the test's single input still satisfies the mutated condition.
A property-based test
that exercises the same execution path with multiple inputs
will detect this mutant when positive values produce negative results.
This difference reveals both the improvement potential of test generalization
and provides a concrete criterion for selecting which generalized tests
to retain. Only those generalizations that detect mutants not caught by existing tests
contribute unique fault detection capability to the test suite,
whereas generalizations that do not kill any new mutants only increase test suite size and runtime
without any tangible benefits.
