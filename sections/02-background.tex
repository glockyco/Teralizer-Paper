\newpage{}
\section{Background}
\label{sec:background}

\subsection{Property-Based Testing}
\label{sec:property-based-testing}

Property-based testing generalizes from testing specific examples to testing properties that should hold for entire classes of inputs~\cite{claessen2000quickcheck}.
While a conventional unit test might verify that \texttt{Math.max(3, 5)} returns \texttt{5},
a property-based test would verify that \texttt{Math.max(a, b)} returns the larger value
for hundreds of automatically generated pairs $(a, b)$.
The property-based testing framework generates random inputs,
executes the test with each input,
and reports any inputs that violate the specified property.
This approach can reveal edge cases and unexpected behaviors
that developers might not think to test explicitly.

Despite its effectiveness at finding bugs,
property-based testing sees limited adoption in practice.
Creating property-based tests requires developers to
(i)~identify properties that should hold across input classes,
(ii)~define appropriate input generators that produce valid test data,
and (iii)~translate existing test assertions into property specifications.
This manual effort, combined with the conceptual shift from examples to properties,
creates a significant barrier to adoption~\cite{barr2015oracle}.
Automating the transformation from conventional unit tests to property-based tests
could lower this barrier while preserving the domain knowledge
encoded in existing test suites.

\subsection{Test Amplification}
\label{sec:test-amplification}

Test amplification encompasses techniques that automatically enhance existing test suites
to improve their effectiveness at achieving engineering goals~\cite{danglot2019snowballing}.
These techniques exploit the knowledge already encoded in tests---such as
valid input ranges, expected behaviors, and important scenarios---to
generate additional tests that explore new behaviors or
strengthen existing test oracles.
Amplification approaches include generating new test inputs~\cite{baudry2015dspot},
adding assertions to capture additional behaviors~\cite{xie2006augmenting},
and transforming test structure to improve coverage or fault detection.

Test generalization represents a specific form of amplification
that transforms tests from validating individual input-output pairs
to validating properties across input classes.
Prior work on test generalization includes JARVIS~\cite{peleg_2018_jarvis},
which infers properties from test inputs and outputs.
However, JARVIS requires developers to manually define constraint templates
and produces overapproximations that may not accurately reflect
the actual program behavior.
Fully automating test generalization requires extracting exact specifications
from the implementation itself,
ensuring that generated tests accurately reflect the program's actual behavior
rather than approximations.

\subsection{Symbolic Analysis for Testing}
\label{sec:symbolic-analysis}

Symbolic execution explores program paths by treating inputs as symbolic variables
rather than concrete values~\cite{king1976symbolic}.
As execution proceeds, the analysis builds path conditions---conjunctions
of constraints that inputs must satisfy to follow each path.
While powerful for discovering new paths and generating test inputs,
full symbolic execution faces scalability challenges due to path explosion
and constraint solving complexity~\cite{pasareanu2013symbolic}.

For test generalization, we need specifications for paths that existing tests already cover,
not discovery of new paths.
This insight enables a more efficient approach: constraint collection.
Instead of exploring all possible paths,
constraint collection follows the concrete execution path of an existing test
while maintaining symbolic representations of variables~\cite{pasareanu2013symbolic}.
At each branch, the analysis records the condition that was satisfied
to follow the test's path, building up the complete path condition
without exploring alternatives.
This linear traversal avoids the exponential complexity of full symbolic execution
while still extracting the exact constraints that characterize the test's execution path.
The resulting path conditions serve as input specifications for property-based tests,
defining precisely which inputs will follow the same execution path as the original test.