\subsection{Generalized Test Creation}
\label{sec:generalized-test-creation}

The generalized test creation stage transforms conventional JUnit tests
into property-based \ToolJqwik{} tests that validate behavior across entire input partitions
rather than single input-output pairs.
Figure~\ref{fig:generalization} provides a high-level overview of this transformation:
the original test with hardcoded values (left) becomes a property-based test (right)
where inputs are automatically generated to satisfy pre-defined constraints
and expected values are encoded as expressions that hold for all inputs from the corresponding input partitions.
Thus, the transformation preserves the developer-provided oracles encoded in assertions
while generalizing from concrete values to symbolic specifications.

To be able to systematically evaluate the costs and benefits of test generalization,
\ToolTeralizer{} creates three variants of each property-based test
which differ in their input generation strategies.
The \VariantBaseline{} variant tests only the original inputs to measure framework overhead.
The \VariantNaive{} variant adds random input generation to explore the input space,
and the \VariantImproved{} variant incorporates constraint-aware generation to favor values at the boundaries of input partitions.
Section~\ref{sec:transformation-pipeline} describes the main transformation process,
Section~\ref{sec:three-variant-design} explains differences between the three variants,
and Section~\ref{sec:constraint-encoding} provides further details on the constraint encoding strategy used by \VariantImproved{}.

\subsubsection{Transformation Process}
\label{sec:transformation-pipeline}

For each generalizable assertion with a successfully extracted input/output specification,
\ToolTeralizer{} creates a new test class containing a single property-based test
based on the corresponding conventional unit test.
Listing~\ref{lst:generalized-test} shows the result of this transformation
for our running example's \textit{good performance} case.
Compared to the original test shown in Listing~\ref{lst:original-test},
the new property-based test replaces the \texttt{@Test} annotation with \texttt{@Property}
to specify both an input supplier and the number of test executions,
adds a \texttt{TestParams} parameter
to the method signature to receive generated inputs,
substitutes hardcoded MUT arguments in \texttt{calculate(1500, 1000)}
to produce the new MUT call \texttt{calculate(\_p\_.sales, \_p\_.target)},
and replaces the concrete expected value \texttt{75} with the oracle call \texttt{calculateExpected(\_p\_)}
which uses the extracted output specification
to calculate a matching output for each generated input (see Listing~\ref{lst:java-output-encoding}).
The \texttt{TestParams} class shown in Listing~\ref{lst:test-params-class}
encapsulates the generated sales and target values,
while the supplier implementation in Listing~\ref{lst:baseline-supplier}
demonstrates how the \VariantBaseline{} variant provides input values for these parameters
by wrapping the concrete values extracted during \ToolSPF{} execution of the original test.

Several contributing factors influence the design of the created property-based test classes.
Instead of directly modifying existing test classes,
\ToolTeralizer{} creates one new test class per assertion
to isolate generalization effects.
This is relevant for mutation testing.
\ToolPit{} requires a green test suite and only supports class-level test exclusion,
so a single failing generalized assertion
could otherwise prevent all tests in the same class from being evaluated.
Furthermore, the isolation prevents unintended side-effects on developer-written code.
Input parameters are provided by supplier classes rather than individual parameter annotations
to enable encoding of constraints that reference multiple parameters.
Only the generalized assertion is preserved in the generalized test method.
Other assertions are removed because they might validate methods
that consume outputs from the generalized MUT
and fail when those outputs differ from the original test's values.
Non-generalizable parameters (strings, arrays, and other objects)
retain their original concrete input values
because current symbolic analysis techniques
cannot extract complete constraints for these types.
This enables partial generalization to be applied even if
some of the required parameters cannot be generalized.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={Generalized test for the \textit{good performance} case.}, label=lst:generalized-test]
@Property(
  supplier = BaseLineSupplier.class,
  tries = 200
)
void testCalculate(TestParams _p_) {
  BonusCalculator c = new BonusCalculator();
  // exceptional performance:
  int b1 = c. calculate(2500, 1000);
  // good performance:
  int b2 = c.calculate(_p_.sales, _p_.target);
  // bad performance:
  int b3 = c. calculate(500, 1000);
  assertEquals(calculateExpected(_p_), b2);
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Output oracle for the \textit{good performance} case.}, label=lst:java-output-encoding]
int calculateExpected(TestParams _p_) {
  return _p_.sales / 20;
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantBaseline{} supplier for \textit{good performance} inputs.
    The supplier uses the same inputs as the original test.
    }, label=lst:baseline-supplier]
class BaselineSupplier {
  Arbitrary get() {
    return Arbitraries.just(
      new TestParams(1500, 1000));
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Container class for generated input values.}, label=lst:test-params-class]
class TestParams {
  int sales;
  int target;
  TestParams(int sales, int target) {
    this.sales = sales;
    this.target = target;
  }
}
\end{lstlisting}

\end{minipage}
\end{genericfloat}
}

\subsubsection{Three-Variant Design}
\label{sec:three-variant-design}

\ToolTeralizer{} creates three variants of each property-based test
that differ only in their input generation strategies.
This design allows us to isolate
and measure distinct aspects of test generalization:
pure framework overhead (\VariantBaseline{}),
benefits from testing additional inputs (\VariantNaive{}),
and improvements through constraint-aware input generation (\VariantImproved{}).
To reduce evaluation costs of this multi-variant approach,
\ToolTeralizer{} shares common analysis stages across all three variants.
Test and assertion analysis, tested method identification, and specification extraction
execute only once per test, with their results reused for each generalization strategy.
This architecture ensures fair comparison
while avoiding redundant computation --- all three generalization strategies work from identical specifications
and analysis results, eliminating confounding factors
such as variation in analysis time or non-deterministic failures from \texttt{OutOfMemoryError}s.

The \textbf{\VariantBaseline{}} variant uses only the original test's input values
to isolate the overhead of the property-based testing framework.
As shown in Listing~\ref{lst:baseline-supplier} for the \textit{good performance} case,
the supplier returns \texttt{Arbitraries.just(new TestParams(1500, 1000))},
providing exactly the same inputs as the original test.
This enables us to quantify the infrastructure cost of property-based testing,
i.e., test orchestration, parameter injection, and \ToolJqwik{}'s execution machinery,
without the cost of additional input generation and repeated test execution.
By establishing this baseline overhead,
we can isolate the cost of input generation in \VariantNaive{} and \VariantImproved{}.
Any runtime beyond \VariantBaseline{} is attributable to generation and filtering strategies
rather than basic property-based testing infrastructure.

The \textbf{\VariantNaive{}} variant adds random input generation combined with post-generation filtering
to demonstrate the benefits of testing additional inputs beyond the original ones.
As shown in Listing~\ref{lst:naive-supplier},
\ToolTeralizer{} uses \ToolJqwik{}'s \texttt{Arbitraries} classes
to generate random integer values for \texttt{sales} and \texttt{target},
then applies the \texttt{satisfiesInputSpec} filter shown in Listing~\ref{lst:java-input-encoding}
to retain only values that match the constraints encoded in the extracted input specification
\texttt{sales / 2 < target \&\& sales >= target}
which we previously showed in Listing~\ref{lst:extracted-specs}.
To ensure that created property-based tests
always cover at least the same mutants as the corresponding original tests,
all \VariantNaive{} and \VariantImproved{} suppliers
contain additional code that always selects the original inputs
as the first set of inputs exercised by the property-based tests.
The implementation of this logic is straightforward,
but is excluded in the listings for brevity.

The \textbf{\VariantImproved{}} variant implements
a constraint-aware input generation strategy
to address the limitations of purely random input selection
in the presence of restrictive input constraints.
For example, an input specification \texttt{a == b \&\& b == c}
is highly unlikely to be satisfied by three random assignments
for \texttt{a}, \texttt{b}, and \texttt{c}, causing \ToolJqwik{}
to throw a \texttt{TooManyFilterMissesException} after too many
failed input generation attempts.
This, in turn, prompts \ToolTeralizer{}
to exclude the property-based test from the final test suite.
To address this limitation,
the \VariantImproved{} variant encodes some constraints directly in the input supplier.
For example, as Listing~\ref{lst:improved-supplier} shows for the \textit{good performance} case,
the generated supplier enforces the constraint \texttt{sales >= target}
via the call \texttt{between(target, Integer.MAX\_VALUE)}.
This partial encoding of constraints
increases the likelihood that a valid input is selected before filtering
by reducing the size of the input space from which values can be chosen.
% Section~\ref{sec:constraint-encoding} details the algorithm that determines
% which constraints are encoded during this constraint-aware input generation
% and which are left for filtering.

{
\begin{genericfloat}[tbph]
\newpage{}
\noindent
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantNaive{} supplier for \textit{good performance} inputs.
    The supplier generates random inputs and then filters them to only test values that match the input specification.
    }, label=lst:naive-supplier]
class NaiveSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      sales -> Arbitraries.integers().map(
        target -> new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\begin{lstlisting}[language=Java, caption={Input filter for the \textit{good performance} case.}, label=lst:java-input-encoding]
boolean satisfiesInputSpec(TestParams _p_) {
  return _p_.sales / 2 < _p_.target
    && _p_.sales >= _p_.target;
}
\end{lstlisting}

\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}

\begin{lstlisting}[language=Java, caption={\VariantImproved{} supplier for \textit{good performance} inputs.
    The supplier partially encodes the input specification during generation, reducing filtering failures compared to \VariantNaive{}.
    }, label=lst:improved-supplier]
class ImprovedSupplier {
  Arbitrary get() {
    return Arbitraries.integers().flatMap(
      target -> Arbitraries.integers()
        // sales >= target is encoded
        // sales / 2 < target is not encoded
        .between(target, Integer.MAX_VALUE)
        .map(sales -> 
          new TestParams(sales, target)))
    .filter(this::satisfiesInputSpec);
  }
}
\end{lstlisting}

\end{minipage}
\end{genericfloat}
}

\subsubsection{Constraint-Aware Generation}
\label{sec:constraint-encoding}

In the current implementation of \ToolTeralizer{},
the constraint-aware input generation strategy
used by the \VariantImproved{} variant
encodes simple equality and inequality constraints
such as \texttt{x == y}, \texttt{x < 10}, or \texttt{y >= x}
where both sides of the (in-)equality are either variables or constants.
More complex constraints are not encoded in the initial input value generation
but are enforced during filtering.
For example, the input constraint \texttt{sales / 2 < target}
is not represented in the input generation code of the \texttt{ImprovedSupplier}
shown in Listing~\ref{lst:improved-supplier}.
However, any generated inputs that do not satisfy this constraint
are still rejected by the \texttt{filter(this::satisfiesInputSpec)} call.
This ensures that even if input constraints can only be partially encoded,
all exercised input values are guaranteed to satisfy the complete input specification.

\begin{algorithm}[b]
\caption{Constraint-Aware Input Generation (\VariantImproved{})}
\label{alg:constraint-encoding}
\begin{algorithmic}[1]
\REQUIRE Input specification $S$, Parameters $P = \{p_1, \ldots, p_n\}$
\ENSURE Generated test inputs satisfying $S$
\STATE Assign indices: $idx(p_i) = i$ for $i \in \{1, \ldots, n\}$
\FORALL{constraints $c \in S$ of form $p_i \odot p_j$ where $\odot \in \{=, <, \leq, >, \geq\}$}
    \IF{$idx(p_i) > idx(p_j)$}
        \STATE Add constraint to $p_i$ based on $p_j$
    \ENDIF
    \IF{$idx(p_j) > idx(p_i)$}
        \STATE Rewrite constraint and add to $p_j$ based on $p_i$
        \STATE E.g., $p_i < p_j$ becomes $p_j > p_i$
    \ENDIF
\ENDFOR
\FOR{each parameter $p_i$ in index order}
    \STATE $E_i \gets$ equality constraints for $p_i$
    \STATE $L_i \gets$ lower bound constraints for $p_i$  
    \STATE $U_i \gets$ upper bound constraints for $p_i$
    \IF{$E_i \neq \emptyset$}
        \STATE Generate $p_i = $ value from first equality constraint
    \ELSIF{$L_i \neq \emptyset$ or $U_i \neq \emptyset$}
        \STATE $lower \gets \max(L_i)$ if exists, else type minimum
        \STATE $upper \gets \min(U_i)$ if exists, else type maximum
        \STATE Generate $p_i \in [lower, upper]$
    \ELSE
        \STATE Generate $p_i$ randomly within type bounds
    \ENDIF
\ENDFOR
\STATE Apply filter for non-encodable constraints
\STATE \RETURN generated inputs if filter passes
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:constraint-encoding} shows how \ToolTeralizer{} determines
which constraints to encode for each parameter.
The algorithm first assigns indices based on parameter order (line 1),
then processes each encodable constraint (lines 2-10) to handle circular dependencies:
constraints are only added to the parameter with the higher index,
with constraint directions rewritten as needed.
This ensures each parameter depends only on previously generated ones.
For example, given \texttt{a >= b \&\& b >= a} with \texttt{a} at index 0 and \texttt{b} at index 1,
the algorithm adds both \texttt{b <= a} (rewritten from \texttt{a >= b})
and \texttt{b >= a} to parameter \texttt{b}, while \texttt{a} remains unconstrained.
During generation (lines 11-22), the algorithm selects the strictest applicable bounds:
equality constraints take precedence, followed by the highest lower bound and lowest upper bound.
Thus, \texttt{a} generates freely,
while \texttt{b} generates from interval \texttt{[a, a]}, effectively encoding \texttt{a == b}.
Finally, line 23 applies filtering for non-encodable constraints,
ensuring all generated inputs satisfy the complete specification.

Even though this partial encoding of input constraints
cannot fully eliminate \texttt{TooManyFilterMissesException}s for all cases,
it reduces their prevalence by constraining
the input space from which generated inputs can be selected.
An additional benefit that the constraint-aware input generation approach provides
is that it enables \ToolJqwik{}'s \texttt{Arbitraries}
to more reliably produce inputs at the boundaries of input partitions.
For example, consider \texttt{x >= 0 \&\& x <= 1000}.
In the \VariantNaive{} variant, \ToolJqwik{} has no knowledge of the true
boundaries of this input partition. Thus,
the used \texttt{Arbitraries} produce assumed boundary values such
as \texttt{Integer.MIN\_VALUE}, \texttt{Integer.MAX\_VALUE - 1}, etc.
While these are quickly excluded by filtering, coverage of the true
boundary values such as 0, 1, 999, and 1000 is now left up to chance.
In contrast, both boundaries of this example would be exactly encoded
in \texttt{Arbitraries} calls of the \VariantImproved{} variant,
enabling them to reliably produce the true boundary values.
We deliberately avoid full constraint solving for input generation
because it would introduce significant runtime overhead
and would still require fallbacks for cases that cannot be solved,
either because they are not tractable for current solvers
or because they are inherently undecidable.
