%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen,review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%\acmBooktitle{ACM Transactions on Software Engineering and Methodology}
% \acmISBN{978-1-4503-XXXX-X/2018/06}
\acmJournal{TOSEM}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{tabularx}

\newcommand{\ToolTeralizer}{Teralizer}

\newcommand{\ToolJPFLong}{Java PathFinder}
\newcommand{\ToolJPF}{JPF}

\newcommand{\ToolSPFLong}{Symbolic PathFinder}
\newcommand{\ToolSPF}{SPF}

\newcommand{\ToolJqwik}{jqwik}
\newcommand{\ToolJacoco}{JaCoCo}
\newcommand{\ToolPit}{PIT}

\newcommand{\ToolEvoSuite}{\textsc{EvoSuite}}

\newcommand{\DatasetEqBench}{\textsc{EqBench}}
\newcommand{\DatasetEqBenchA}{eqbench-es-default-1s}
\newcommand{\DatasetEqBenchB}{eqbench-es-default-10s}
\newcommand{\DatasetEqBenchC}{eqbench-es-default-60s}
\newcommand{\DatasetsEqBenchEs}{eqbench-es-$*$}

\newcommand{\DatasetCommons}{commons-utils}
\newcommand{\DatasetCommonsA}{commons-utils-es-default-1s}
\newcommand{\DatasetCommonsB}{commons-utils-es-default-10s}
\newcommand{\DatasetCommonsC}{commons-utils-es-default-60s}
\newcommand{\DatasetsCommonsEs}{commons-utils-es-$*$}

\newcommand{\DatasetRepoReapers}{repo-reapers}

\newcommand{\VariantOriginal}{\textsc{Original}}
\newcommand{\VariantInitial}{\textsc{Initial}}
\newcommand{\VariantBaseline}{\textsc{Baseline}}

\newcommand{\VariantNaive}{\textsc{Naive}}
\newcommand{\VariantNaiveA}{\textsc{Naive\textsubscript{10}}}
\newcommand{\VariantNaiveB}{\textsc{Naive\textsubscript{50}}}
\newcommand{\VariantNaiveC}{\textsc{Naive\textsubscript{100}}}

\newcommand{\VariantImproved}{\textsc{Improved}}
\newcommand{\VariantImprovedA}{\textsc{Improved\textsubscript{10}}}
\newcommand{\VariantImprovedB}{\textsc{Improved\textsubscript{50}}}
\newcommand{\VariantImprovedC}{\textsc{Improved\textsubscript{200}}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title[short title]{full title}
\title{Teralizer: Something About Automated Test Generalization}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Johann Glock}
\email{johann.glock@aau.at}
\orcid{0000-0002-0152-8611}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

\author{Clemens Bauer}
\email{clemens.bauer@aau.at}
\orcid{0009-0000-9199-8563}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

\author{Martin Pinzger}
\email{martin.pinzger@aau.at}
\orcid{0000-0002-5536-3859}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Glock et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Abstract
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\received{n/a}
\received[revised]{n/a}
\received[accepted]{n/a}

%%
%% A "teaser figure" is an image, or set of images in one figure,
%% that are placed after all author and affiliation information,
%% and before the body of the article, spanning the page. If you
%% wish to have such a figure in your article, place the command
%% immediately before the \maketitle command:
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{figure caption}
%%   \Description{figure description}
%% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% ------------------------------------------------------------------------------
% INTRODUCTION
% ------------------------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}

% ------------------------------------------------------------------------------
% BACKGROUND
% ------------------------------------------------------------------------------

\section{Background}
\label{sec:background}

\subsection{Property-based Testing}
\label{sec:property-based-testing}

\subsection{Test Generalization}
\label{sec:test-generalization}

% ------------------------------------------------------------------------------
% APPROACH
% ------------------------------------------------------------------------------

\section{Approach}
\label{sec:approach}

\subsection{Overview}
\label{sec:approach-overview}

processing pipeline:
CLEANUP\_PROJECT(0),
%
DOWNLOAD\_PROJECT(1),
SETUP\_PROJECT(2),
%
ADD\_DEPENDENCIES(3),
BUILD\_PROJECT\_ORIGINAL(4),
%
GENERATE\_EVOSUITE\_TESTS(5),
POSTPROCESS\_EVOSUITE\_TESTS(6),
%
BUILD\_SPOON\_MODEL(7),
%
EXECUTE\_TESTS\_ORIGINAL(8),
COLLECT\_JUNIT\_REPORTS\_ORIGINAL(9),
COLLECT\_JACOCO\_DATA\_ORIGINAL(10),
FILTER\_TESTS\_ORIGINAL(11),
COLLECT\_PIT\_DATA\_ORIGINAL(12),
%
ANALYZE\_TESTS(13),
FILTER\_TESTS(14),
FILTER\_ASSERTIONS(15),
%
ADD\_JPF\_INSTRUMENTATION(16),
BUILD\_PROJECT\_INSTRUMENTED(17),
EXECUTE\_JPF(18),
ANALYZE\_JPF(19),
CLEANUP\_JPF\_INSTRUMENTATION(20),
%
BUILD\_PROJECT\_INITIAL(21),
EXECUTE\_TESTS\_INITIAL(22),
%
COLLECT\_JUNIT\_REPORTS\_INITIAL(23),
COLLECT\_JACOCO\_DATA\_INITIAL(24),
COLLECT\_PIT\_DATA\_INITIAL(25),
%
CLEANUP\_GENERALIZATION(26),
%
GENERALIZE\_TESTS(27),
BUILD\_PROJECT\_GENERALIZED(28),
%
EXECUTE\_TESTS\_GENERALIZED(29),
COLLECT\_JUNIT\_REPORTS\_GENERALIZED(30),
FILTER\_GENERALIZATIONS(31),
%
COLLECT\_JACOCO\_DATA\_GENERALIZED(32),
COLLECT\_PIT\_DATA\_GENERALIZED(33);

basically:
- instrument project
- detect test methods + assertions + tested methods
- for each assertion / tested method, collect input + output specification with SPF
- for each assertion / tested method, create generalization with 3 variants (utilizing extracted specifications): BASELINE, NAIVE, IMPROVED

at various points, apply filtering to avoid processing of tests / assertions / generalizations that are unlikely to successfully pass all processing steps (or even guaranteed to fail)

thoughout the pipeline, track (i) runtime, (ii) (intermediate) results (success? failure? causes? other task outputs / created files?), (iii) mutation / coverage / test data;

offers a cleanup task to revert any changes applied by generalization (we only add files, so cleanup is just removing files; no existing files are modified);

\subsection{Project Instrumentation}
\label{sec:project-instrumentation}

(shorten + merge this with the overview? we have so many subsections...)

accepts either a URL to Git repository or a path to local directory as target;
if target is a URL to a Git repository, the project is cloned automatically (with Git's default settings);
processing then continues the same for both types of projects;

detects JUnit 4 vs. JUnit 5 testing framework, others not supported;
detects Maven vs. Gradle (Groovy DSL) projects, others not supported;
adds required dependencies based on detected project type (update JUnit, add JUnit Vintage, add JaCoCo, add PIT, add jqwik);
creates separate pom.xml / build.gradle (with comments for additions by Teralizer) - original file is left untouched;
to verify successful instrumentation, project is built, tests executed, test / coverage / mutation results collected;

\subsection{Test / Assertion Detection and Analysis}
\label{sec:test-analysis}

execute test suite;
identify executed test methods via junit / surefire XML reports;
for each test method, identify all assertions in the method via Spoon (calls to methods in "org.junit.Assert" (JUnit 4) or "org.junit.jupiter.api.Assertions" (JUnit 5)));
for each (supported) assertion (assertEquals, assertTrue, assertFalse, assertThrows), identify one tested method call via Spoon (for assertThrows: the executed method, for assertEquals/True/False: the method that returned the "actual" value of the assertion);
we assume each assertion "tests" one method, and do not consider side effects => no support for, e.g., method sequences that modify object state (e.g.: list.add(...), assert(..., list.size()));
side effects could be modeled as inputs/outputs of the tested method;

tests and assertions are filtered if they cannot be (correctly) handled by the current implementation. for details, see Section "Test / Assertion / Generalization Filtering".

\subsection{Specification Extraction}
\label{sec:specification-extraction}

\subsubsection{Driver Generation}
\label{sec:driver-generation}

for each assertion / tested method call, create a driver program and a corresponding SPF configuration;
the driver program is a single class with a "public static void main(...)" method;

the main method:
(i) executes any setup methods of the test class (JUnit 4: "@Before", "@BeforeClass", JUnit 5: "@BeforeAll", "@BeforeEach"),
(ii) creates an instance of the test class, and then
(iii) calls the tested method.

the SPF configuration:
(i) sets the main method of the driver program as the entrypoint for SPF execution,
(ii) sets the tested method as SPF's "symbolicMethod" (using symbolic inputs for generalizable input parameters),
(ii) configures SPF to run in constraint-collection mode (=> no constraint solving, just following the concrete execution path),
(iii) registers + configures a custom listener that extracts input-output specifications (see next Section "SPF Execution"),
(iv) configures several execution limits (depth limit, execution time limit, etc.; see Section "Other Limits / Safeguards").

(note: we had to modify (fix?) SPF to disable constraint solving during constraint-collection mode execution.)
(note: we're actually also creating an instrumented version of the test class / method, but that's only there so we can more easily identify the tested method call during SPF execution if the same method is called multiple times.)

\subsubsection{SPF Execution}
\label{sec:spf-execution}

execute SPF once for each driver program;
start tracking symbolic state when entering the tested method;
when exiting the tested method:
(i) write the concrete input values to a file,
(ii) write the concrete output values to a file,
(iii) write the symbolic input values to a file (=> path condition / input specification),
(iv) write the symbolic output values to a file (=> output specification),
then immediately terminate the execution (no need to keep going - we have everything we need);

((show an example of extracted data here))

for tested methods that exit via thrown exception,
use the thrown exception (type) as the concrete output value.
no symbolic output value can be collected in this case
(because the (type of the) thrown exception is not a function of the symbolic input values,
but is instead constant for all sets of inputs in the partition).

\subsection{Test Transformation}
\label{sec:test-transformation}


using jqwik (1.8.5) for property-based testing (\url{https://github.com/jqwik-team/jqwik});
last official release in 2024 (1.9.2);
last commit 2 days ago (checked on: 2005-04-23);
590 GitHub stars;
built for junit 5!;
comprehensive user guide (\url{https://jqwik.net/docs/current/user-guide.html});

competition 1:
junit-quickcheck (\url{https://github.com/pholser/junit-quickcheck});
last official release in 2020 (1.0);
last commit 8 months ago;
590 GitHub stars;
built for junit 4, junit 5 support only via junit-vintage (\url{https://github.com/pholser/junit-quickcheck/issues/189#issuecomment-414706607});
documentation less comprehensive (\url{https://pholser.github.io/junit-quickcheck/site/1.0/index.html});

competition 2:
quicktheories (\url{https://github.com/quicktheories/QuickTheories});
last official release in 2018 (0.25);
last commit 6 years ago;
509 GitHub stars;
built for junit 4;

\subsubsection{"BASELINE" Generalization}
\label{sec:baseline-generalization}

transforms target test into a property-based test;
one test class per generalizable assertion;
(PIT only offers class-level selections / exclusions, so generating classes causes less "collateral damage" for failing generalizations);
uses only the original set of input values via custom arbitrary;

allows us to see how much runtime overhead jqwik introduces even without any generation of input values;

transformation steps:
clone the original test class (all further actions on the cloned class);
delete other test methods in the class (non-test methods need to be preserved because they might be used by the target test);
add a nested class "TestParameters" that can hold values for all generalizable parameters of the tested method (i.e., all parameters of type byte, short, int, long, float, double);
add a nested class "TestParametersSupplier" that can generate "arbitrary" (jqwik term) "TestParameters" instances;
for the BASELINE variant, only one instance of TestParameters is generated by the supplier;
this instance uses the same tested method input values as the original test;
delete all existingTest annotations from the test method (removing @Test is most important, but other annotations are removed as well because they are unlikely to be compatible with @Property);
add jqwik @Property annotation (seed = 0, ShrinkingMode.OFF, EdgeCasesMode.FIRST, tries = 10 / 50 / 200);
add parameter of type "TestParameters \_p\_" with annotation \@ForAll(supplier = TestParametersSupplier) to the test method;
replace tested method arguments with values from TestParameters instance \_p\_ (e.g., foo(a, b, c) -> foo(\_p\_.a, \_p\_.b, \_p\_.c); only for generalizable inputs, the others remain unchanged).
delete other assertions in the test method (unless they have return values that are used in the code, e.g., Exception e = assertThrows(...));
no need to modify assertions (because inputs stay the same, so expected outputs should also stay the same);

the original test method is always preserved in the current implementation;
this would not be necessary for cases where there is only a single assertion in the test method and generalization is successful;
this would also not be necessary for casses where all assertions in a test method are successfully generalized;
statistics on how common these cases are in R1 (or RQ2? or RQ4?).

because most of the test method is copied for each generalized assertion, this creates a lot of duplicate code;
the currently implementation does not optimize for this at all - statistics on test suite size increases in RQ2;
some of this could likely be avoided by putting in more engineering effort, e.g., automatically extracting setup functions that can be reused across all generalizations of a test method;
alternatively, we could add multiple TestParameters parameters (one for each generalized assertion) - but that might not be very maintainable either;

\subsubsection{"NAIVE" Generalization}
\label{sec:naive-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "naive" approach for selecting sets of input values;

basic approach:
step 1: randomly generate sets of values that match the types of input parameters;
step 2: apply filter to keep only value sets that satisfy the input specification;
repeat until desired number of input sets (we use 10, 50, 200 in the evaluation) has been generated (automatically done by jqwik, we just set how many we want);
still preserves original test inputs via a custom arbitrary => no reduction of mutation score due to "bad" random values;

problem: many TooManyFilterMissesExceptions;
reason: depending on the input specification, randomly selecting sets of input values can be (very) unlikely to produce satisfying inputs (e.g., a = b = c => 3 random ints that are equal);

example: a = b = c (all ints);
randomly generate a;
randomly generate b;
randomly generate c;
apply the a = b = c filter (likely not a match => throw away and try again; after too many non-matching attempts => TooManyFilterMissesExceptions)

\subsubsection{"IMPROVED" Generalization}
\label{sec:improved-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "improved" approach for selecting sets of input values to reduce TooManyFilterMissesExceptions;

basic approach:
step 1: generate sets of input values that already take into account "as many constraints as possible";
step 2: apply filter to keep only value sets that satisfy the full input specification;
repeat until the desired number of input sets has been generated;
like NAIVE, IMPROVED also preserves original test inputs via a custom arbitrary;

example: a = b \&\& b = c (all ints);
randomly generate a since we don't have any constraints to consider yet;
generate b such that b = a (i.e., take into account the a = b constraint);
generate c such that b = c (i.e., take into account the b = c constraint);
apply the a = b = c filter (trivial in this case => use a more interesting example);

currently only considers the following constraints:
var1 == (var2 | const);
var1 < (var2 | const);
var1 <= (var2 | const);
var1 > (var2 | const);
var1 >= (var2 | const);

supported types: byte, short, int, long, float, double;
mixed-type constraints are also supported (e.g., int-var < float-var);

more complex terms (e.g., "compound" terms (is this the correct terminology?), (trigonometric) function calls) are not taken into account (e.g., a < b - c, a == cos(b));
constraints that are not equality, upper- or lower-bound constraints are not taken into account either (e.g., inequality constraints);
=> show some statistics about used vs. unused constraints -> further details in RQ4 or the discussion;

actual value selection logic (code that implements this logic for all generalizable inputs is automatically generated):

if at least one equality constraint exists for a variable, all other constraints are ignored
if multiple equality constraints exist, we just take "the first one";
all equality constraints have the same value anyway because we select these at runtime based on whichever concrete values have already been assigned to involved variables;

if multiple upper / lower bounds exist, the strongest bound is used, i.e., the highest lower bound and the lowest upper bound.
as with equality constraints, this is determined at runtime, i.e., based on on whichever values have already been assigned to involved variables;

in practice, naive processing of constraints often leads to "dead ends" where no further assignments are possible;
for example: a >= b \&\& b >= a; here we would have a ">= b" constraint on "a" and a ">= a" constraint on "b";
to resolve such situations, we assign an index to each variable that occurs in the input specification, e.g., idx(a)=1, idx(b)=2;
constraints are then rewritten to apply only to the variable with the highest index, e.g. (i) "a >= b" -> "b <= a", and (ii) "b >= a" -> "b >= a";
thus, the used constraints for variable value selection become: a -> no constraints, b -> {"<= a", ">= a"};
since a is now unconstrained, we can simply select a random value for it; once a is assined, b can be assigned as well; 
similar transformations are applied for all suppored constraints (==, <, <=, >, >=);
(todo: find the correct terminology to describe this; perhaps constraint rewriting / simplification?
other related terms: constraint satisfaction problems, variable elimination, constraint propagation, domain reduction, and arc consistency algorithms)

in some cases, choices of early variables lead to unsatisfiable constraints later on;
for example: b > a \&\& b <= 0 -> no satisfying assignment for b if a a >= 0;
in this cases, we return an empty arbitrary for b, thus prompting jqwik to pick a new value for a;
similar situations occur for over-/underflows, e.g., b > a with a = Integer.MAX\_VALUE;
in this case, b = a + 1 = Integer.MIN\_VALUE, which will be rejected by filtering;

some current limitations could be resolved through more engineering effort (e.g., custom arbitraries);
one way to generate values that satisfy all / more constraints would be through constraint solving (add references);
however, this would have a significant runtime cost and would still suffer various limitations (add references);

\subsection{Test / Assertion / Generalization Filtering}
\label{sec:all-filtering}

describe filtering here or in RQ4?

we need to ensure that:
1: we do not generate any incompilable code;
2: we have a green test suite for mutation testing (PIT only works with green suites - otherwise it throws an error);
also, we would like to avoid spending processing effort on generalization attempts that are unlikely to be successful (=> early excludes);
to achieve this, we apply filtering at multiple stages + levels in the processing pipeline (for filtering data, see RQ4).

(note: large reduction of required filtering would be possible by putting in more engineering effort)

\subsubsection{Test-level Filtering}
\label{sec:test-filtering}

filterTestOriginal:
NonPassingTestFilter: filters failing tests (PIT requires green suite, and generalization of failing tests does not seem useful anyway);
TestTypeFilter: filters tests with unsupported test annotations (we currently only support @Test annotations);

filterTest:
NoAssertionsFilter: filters tests without assertions (with no assertions, choosing a target method is even trickier than it already is, and we have no useful output specification);

\subsubsection{Assertion-level Filtering}
\label{sec:assertion-filtering}

filterAssertion:
ExcludedTestFilter: filters assertions that are part of filtered or otherwise excluded tests (if the tests cannot be handled, assertion-level results cannot override this);
MissingValueFilter: filters assertions for which no tested method could be identified (without a tested method, we don't have specifications, so cannot perform generalization);
VoidReturnTypeFilter: filters assertions with a tested method that has a void return type (no return type -> no output specification -> no generalization);
UnsupportedAssertionFilter: filters unsupported assertions (we currently only support assertEquals, assertTrue, assertFalse, and (with some caveats?) asserThrows);
ParameterTypeFilter: filters assertions with a tested method that has no parameters of supported types (we can currently generalize parameters of types byte, short, int, long, float, double);

\subsubsection{Generalization-level Filtering}
\label{sec:generalization-filtering}

filterGeneralization:
NonPassingTestFilter: filters generalizations that fail during test execution (PIT requires green suite, can be either due to "incorrect" generalization or due to "bad" original tests);

\subsubsection{Other Limits / Safeguards}
\label{sec:other-filtering}

During SPF execution:
maximum PC size limit;
maximum depth limit;
maximum execution time;

During Test Transformation:
maximum Java specification size;

\subsection{Program Output / Collected Data}
\label{sec:collected-data}

primarily, we provide 1 property-based test for each original assertion (excluding ones that are filtered throughout the processing pipeline).
(do we need this section? or are outputs / data already explained well enough in the other section? even then, might still be useful to have a condensed overview here)

additionally, we provide (i) a PostgreSQL database with data about the processed projects and (ii) various intermediate results / log files.
(perhaps move this information to some "Data Availability" section.)

The database contains tables:
- project,
- test,
- assertion,
- generaliztion,
- filter\_result,
- (evosuite\_runtime),
- (evosuite\_report),
- junit\_test\_report,
- jacoco\_coverage\_report,
- pit\_coverage\_report,
- pit\_mutation\_report,
- task.

The intermediate results / log files contain:
- modified pom.xml / build.gradle files,
- command-data (all executed commands + output logs + error logs),
- jacoco-data (JaCoCo coverage CSV reports per project + variant),
- jpf-data (output logs, input-output values + specifications, driver + config + instrumented test files),
- junit-data (JUnit / Surefire XML reports per project + variant),
- pit-data (PIT linecoverage + mutations XML reports per project variant),

using PIT for mutation testing because;
most mutation testing tools
(i) do not provide results in a structured format
that's suitable for automated processing
and / or (ii) do not provide (official) support for Java 8,
and / or (iii) are not actively maintained anymore
(according to the PIT website:
\url{https://pitest.org/java_mutation_testing_systems/#summary-of-mutation-testing-systems})


% ------------------------------------------------------------------------------
% EVALUATION
% ------------------------------------------------------------------------------

\section{Evaluation}
\label{sec:evaluation}

To evaluate the primary effects, secondary effects, runtime requirements, and current
limitations of our generalization approach, we investigate the following research questions:

\begin{itemize}
  \item RQ1: To which degree does generalization affect the mutation score of the target test suites?
  \item RQ2: To which degree does generalization affect the size and runtime of the target test suites?
  \item RQ3: What are the runtime requirements of the generalization approach?
  \item RQ4: What are the causes of unsuccessful generalization attempts?
\end{itemize}

Throughout the remainder of this section, we first describe the target programs used for
the evaluation in Section~\ref{sec:target-programs}. In Section~\ref{sec:evaluation-setup},
we provide further information about the used evaluation setup, including the used hardware,
relevant pre-/processing steps, and used configuration settings of Teralizer.
Sections~\ref{sec:primary-effects-eval}--\ref{sec:filtering-eval-extended}
describe the results of the evaluation, answering RQ1--RQ4.


\subsection{Target Programs}
\label{sec:target-programs}

Evaluation of the test generalization approach requires a dataset consisting of projects for which
(i) the source code of the implementation and corresponding tests is available.
Additionally, as discussed throughout Section~\ref{sec:approach}, included projects
(ii) need to be processable by SPF (which does not support newer Java versions than Java 8),
(iii) need to use JUnit 4 or JUnit 5 as a testing framework,
(iv) need to use Maven or Gradle as a build tool,
(v) should use (partially) numeric inputs and numeric outputs for methods targeted by the generalization
(because Teralizer currently only supports generalization of numeric inputs and outputs,
as discussed in Section~\ref{sec:specification-extraction}).

While various publicly available benchmarks exist that fulfill requirements i--iv,
these benchmarks are often based on code from popular libraries such as Apache Commons, Google Gson, ... .
For example, ... (list + describe some benchmarks that support this claim).
Because these libraries need to be useable in different application scenarios,
and maintainable by a large number of simultaneous contributors,
they tend to make heavy use of encapsulation, inheritance, polymorphism, reflection, etc.
As a result, such projects are largely incompatible
with the basic specification extraction approach used by Teralizer
because most implementation code does not fulfill requirement (v).
Furthermore, due to their widespread use, active development communities, and long-running nature,
these libraries are often more thoroughly tested than other projects are.
Consequently, evaluating any test generalization approach on them
would see only little opportunity to improve the effectiveness of the test suite.

To achieve a thorough evaluation of our approach
that provides generalizable results for both its strengths as well as its weaknesses
in the presence of the requirements and considerations discussed above,
we built our own evaluation dataset using projects from three different sources:
the \DatasetEqBench{} Benchmark \cite{badihi_2021_eqbench},
utility classses of {TODO: number} Apache Commons projects, and
{TODO: number} open source projects from the RepoReapers dataset \cite{munaiah_2017_reporeapers}.
Further details about the included projects
are provided in the follwing Sections~\ref{sec:eqbench}--\ref{sec:additional-oss-projects}.
Descriptive statistics are available in Section~\ref{sec:dataset-statistics}.

\subsubsection{EqBench}
\label{sec:eqbench}

To represent projects that are well-suited for processing by \ToolTeralizer{},
we include the \DatasetEqBench{} benchmark as the first part of our evaluation dataset.
The \DatasetEqBench{} benchmark is a dataset built by \citeauthor{badihi_2021_eqbench}
to evaluate the effectiveness of equivalence checking tools \cite{badihi_2021_eqbench}.
The datasets contains Java (and C) implementations of non-equivalent program pairs.
Since many equivalence checking tools such as ... \cite{TODO} and ... \cite{TODO}
rely (partially) on symbolic execution for their analysis,
\DatasetEqBench{} is well suited for processing with \ToolSPFLong{}-based tools such as \ToolTeralizer{}.
The dataset achieves this by focusing primarily on programs that process numeric inputs,
and by avoiding use of language features such as recursion and reflection
which contemporary equivalence checking approaches only offer limited support for \cite{TODO}.
Because the dataset only contains implementation code, but does not contain test code,
we used \ToolEvoSuite{} to generate test suites
that can be used as input for the generalization.
We chose \ToolEvoSuite{} for test generation
because it consistently achieved top placements in recent test generation tool competitions \cite{TODO}.
For the test generation, we used three different timeout settings
for \ToolEvoSuite{} to approximate different strengths of test suites:
60 seconds per class (\ToolEvoSuite{}'s default), 10 seconds per class, and 1 second per class.
All other configuration settings were left at \ToolEvoSuite{}'s default values. We refer to
the corresponding projects consisting of \DatasetEqBench{} implementation code and
\ToolEvoSuite{} generated test code as \textit{\DatasetEqBenchA{}}, \textit{\DatasetEqBenchB{}}, and \textit{\DatasetEqBenchC{}}.
Descriptive statistics for these projects are provided in Section~\ref{sec:dataset-statistics}.

% tests generated with EvoSuite;
% came "third" in SBFT Tool Competition 2025 - Java Test Case Generation Track (https://arxiv.org/pdf/2504.09168), but differences between top 3 not statistically significant per the paper;
% can't find a summary paper for SBFT 2024;
% performed best on the SBFT Tool Competition 2023 - Java Test Case Generation Track for line and branch coverage metrics, and second-best for understandability metric;
% also the overall winner for SBST Tool Competition 2022 and SBST Tool Competition 2021 which use line + branch (via JaCoCo) and mutation coverage (via PIT) as metrics;

% 3 different test suites with search budgets: 1s, 10s, 60s (= EvoSuite default);
% no other changes to default EvoSuite configuration

\subsubsection{Apache Commons Utils}
\label{sec:apache-commons-utils}

To have some representation of large open source libraries in our evaluation dataset,
we extracted source code and corresponding test code of utility methods from \{number\} Apache Commons projects.
We identified appropriate utility methods via Sourcegraph's RegEx-based repository search\footnote{\url{https://sourcegraph.com/search}}.
More specifically, we searched for public static methods
with at least one numeric or boolean input parameter
and a numeric or boolean output value.
The exact search query we used is available in our replication package.
To construct the dataset from the Souregraph search results,
we manually created a new project and added to it:
(i) all matching utility methods,
(ii) all methods that are (transitively) called by the utility methods,
(iii) all tests that cover the utility methods, and
(iii) all dependencies that are needed to compile the included code.
Beyond the original test suite consisting of test classes extracted from Apache Commons projects,
we also generated three additional test suites via \ToolEvoSuite{}
using the same settings that we used for the \DatasetEqBench{} test generation.
Thus, four variants of the created project are included in the evaluation dataset:
\textit{\DatasetCommons{}} (which uses the original test suite)
as well as \textit{\DatasetCommonsA{}}, \textit{\DatasetCommonsB{}}, and \textit{\DatasetCommonsC{}}
(which use \ToolEvoSuite{} generated test suites).

\subsubsection{RepoReapers}
\label{sec:additional-oss-projects}

To get a more representative view
of current limitations of the approach in a real-world setting,
we additionally applied Teralizer to <number> projects
from the RepoReapers dataset\cite{munaiah_2017_reporeapers} for RQ4.
@TODO: Write 1-2 sentences about the RepoReapears dataset.
To select these projects from the full RepoReapers dataset,
we applied the following selection criteria: projects
(i)~have to be implemented in Java 1.5 to 1.8
(ii)~have to use JUnit 4 or 5 for testing,
(iii)~have to use Maven as a build tool,
(iv)~have to follow the expected folder structure (i.e., have implementation in src/main/java and tests in src/test/java) 
(v)~have to contain 5000-50000 LOC,
(vi)~have to contain 20\%-80\% of the total source code in test classes
(vii)~have to have a total repository size less than 100MB.
Selection criteria (i) is imposed by the \ToolSPF{} dependency of \ToolTeralizer{}.
Criteria (ii) to (iv) are current limitations of the implemented protopye
(as discussed in Section~\ref{sec:approach}),
but could be lifted with additional engineering effort.
The remaining selection criteria (v) to (vii) were applied
to keep the overall size of the dataset to a manageable amount
while focusing on a subset that has
a good balance between implementation code and test code.
We refer to this dataset as \textit{\DatasetRepoReapers{}} throughout the rest of this paper.

\subsubsection{Dataset Statistics}
\label{sec:dataset-statistics}

Table~\ref{tab:dataset-statistics} provides descriptive statistics
of the projects that are included in the dataset.
We separately list the number of files, classes, and source lines of code (SLOC)
for both the implementation code as well as the test code of the projects.
For the \DatasetRepoReapers{} projects,
we list the total, mean, and median values
across all 1160 included (sub-)projects.

@TODO: Describe the \DatasetsEqBenchEs{} numbers.

@TODO: Describe the \DatasetCommons{} numbers.

The developer-written tests in \DatasetCommons{}
often have several times as many lines of code
as the automatically generated \ToolEvoSuite{} tests
in \DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}.
This is because the developer-written tests often
test multiple sets of input values
and / or multiple target methods
in a single single test method.
The \ToolEvoSuite{} generated test suites, on the other hand,
use very isolated / focused test methods
that generally cover only a single target method
using a single set of input values.
For practical examples of these differences, see Figure~\ref{fig:@TODO}.

@TODO: add a figure showing test cases in \DatasetCommons{} vs. \DatasetCommonsA{} / \DatasetCommonsB{} / \DatasetCommonsC{}.

@TODO: Describe the \DatasetRepoReapers{} numbers.

descriptive statistics (evosuite runtime, number of classes, number of tests, LOC, ...)

\begin{table}[H]
  \caption{Number of files, classes, source lines of code (SLOC), and test methods per project.}
  \label{tab:dataset-statistics}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{3}{r}{Implementation} & \multicolumn{4}{r}{Test} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-8}
    Project & Files & Classes & SLOC & Files & Classes & SLOC & Methods \\
    \midrule
    eqbench-es-default-1s & 544 & 652 & 27,871 & 544 & 544 & 35,666 & 4,718 \\
    eqbench-es-default-10s & 544 & 652 & 27,871 & 543 & 543 & 36,937 & 4,875 \\
    eqbench-es-default-60s & 544 & 652 & 27,871 & 544 & 544 & 37,836 & 4,974 \\
    \midrule
    commons-utils & 106 & 247 & 19,709 & 80 & 119 & 14,389 & 725 \\
    commons-utils-es-default-1s & 106 & 247 & 19,709 & 103 & 103 & 17,524 & 2,481 \\
    commons-utils-es-default-10s & 106 & 247 & 19,709 & 103 & 103 & 19,082 & 2,738 \\
    commons-utils-es-default-60s & 106 & 247 & 19,709 & 102 & 102 & 18,839 & 2,735 \\
    \midrule
    repo-reapers (total) & 79,789 & 96,589 & 5,203,516 & 40,872 & 55,038 & 3,905,071 & 167,046 \\
    repo-reapers (mean) & 68 & 83 & 4,478 & 35 & 47 & 3,360 & 143 \\
    repo-reapers (median) & 51 & 57 & 3,241 & 22 & 26 & 2,053 & 69 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Evaluation Setup}
\label{sec:evaluation-setup}

To collect the necessary data to answer RQ1-RQ4,
we executed \ToolTeralizer{} once for every project in the evaluation dataset
on a MacBook Air with M2 processor and 24~GB of memory
(for threats to validity resulting from this setup, see Section~\ref{sec:threats-to-validity}).
The JVM settings for \texttt{InitialHeapSize} and \texttt{MaxHeapSize}
were kept at their default values, i.e., 384~MB (${1/64}$th of memory) for \texttt{InitialialHeapSize} and 6~GB (${1/4}$th of memory) for \texttt{MaxHeapSize}.
Since there are no existing approaches
that that automatically create property-based tests from existing (unit) tests,
no other tools are included in the evaluation.
Instead, we compare \ToolTeralizer{} generated test suites
to the developer written and \ToolEvoSuite{} generated test suites in the dataset.
As described in Section~\ref{sec:approach},
\ToolTeralizer{} creates several variants
of each target project / test suite
throughout its execution
and stores them for use in later processing steps
as well as to include them in our replication package~\cite{replicationpackage}.
We refer to these variants as:

\begin{itemize}
  \item \VariantOriginal{}:
    The original project without any modifications applied by \ToolTeralizer{}.
  \item \VariantInitial{}:
    The state of the project
    after project instrumentation, test analysis, and specification extraction have concluded 
    (see Sections~\ref{sec:project-instrumentation}--\ref{sec:specification-extraction})
    but before any tests have been generalized.
    Tests for which these steps were not successful
    are excluded from this project variant and,
    therefore, are not included in any further generalization steps.
    An overview of excluded tests is provided in Section~\ref{sec:filtering-eval}
    when discussing the results of RQ4.
  \item \VariantBaseline{}:
    The state of the project
    after \VariantBaseline{} generalization has been applied
    (see Section~\ref{sec:baseline-generalization}).
  \item \VariantNaiveA{}, \VariantNaiveB{}, \VariantNaiveC{}:
    The state of the project
    after \VariantNaive{} generalization has been applied.
    As described in Section~\ref{sec:naive-generalization},
    the subscript values indicate the values used for \ToolJqwik{}'s \texttt{tries} setting.
  \item \VariantImprovedA{}, \VariantImprovedB{}, \VariantImprovedC{}:
    The state of the project
    after \VariantImproved{} generalization has been applied
    (see Section~\ref{sec:improved-generalization}).
    The subscript values indicate the values used for \ToolJqwik{}'s \texttt{tries} setting.
\end{itemize}

In addition to the source code of the two pre-generalization variants
(\VariantOriginal{} and \VariantInitial)
as well as the seven post-generalization variants
(\VariantBaseline{},
\VariantNaive{} with 10 / 50 / 100 tries,
and \VariantImproved{} with 10 / 50 /100 tries),
\ToolTeralizer{} also stores (meta) data and (intermediate) processing results
at various stages throughout its execution
in a PostgreSQL database and in log files.
This data includes, for example,
(i) descriptions of identified tests and assertions
(e.g., names, involved data types, lines of code),
(ii) information about executed processing tasks
(e.g., processing status, causes of failures, execution time),
and (iii) raw tool outputs
(e.g., console output logs, JUnit / \ToolJacoco{} / \ToolPit{} reports , extracted input-/output-specifications).
For a more thorough discussion of collected data and produced outputs, see Section~\ref{sec:collected-data}.
The evaluation results presented in the following sections
are generated from the collected data via Python code in Jupyter notebooks.
To aid independent validation and replication of our results,
all of the collected data, the full Java implementation of \ToolTeralizer{},
and the Python / Jupyter scripts used for the evaluation
are publicly available in our replication package~\cite{replicationpackage}.

\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

\begin{table}[H]
  \caption{Number of total, covered, and uncovered mutants in included classes per project.}
  \label{tab:mutants-per-project}
  \begin{tabular}{lrrrrr}
    \toprule
            & Included     & Included      & \multicolumn{3}{r}{Mutants} \\
                                             \cmidrule(lr){4-6}
    Project & Test Methods & Impl. Classes & Total & Covered & Uncovered \\
    \midrule
    eqbench-es-default-1s & 3937\; (83.4 \%) & 607\; (93.1 \%) & 23905 & 21492\; (89.9 \%) & 2413\; (10.1 \%) \\
    eqbench-es-default-10s & 4049\; (83.1 \%) & 600\; (92.0 \%) & 23654 & 21657\; (91.6 \%) & 1997\; (\phantom{0}8.4 \%) \\
    eqbench-es-default-60s & 4124\; (82.9 \%) & 603\; (92.5 \%) & 23663 & 22010\; (93.0 \%) & 1653\; (\phantom{0}7.0 \%) \\
    \midrule
    commons-utils & 461\; (63.6 \%) & 90\; (36.4 \%) & 8096 & 5215\; (64.4 \%) & 2881\; (35.6 \%) \\
    commons-utils-es-default-1s & 2079\; (83.8 \%) & 111\; (44.9 \%) & 8581 & 7536\; (87.8 \%) & 1045\; (12.2 \%) \\
    commons-utils-es-default-10s & 2330\; (85.1 \%) & 112\; (45.3 \%) & 8391 & 7939\; (94.6 \%) & 452\; (\phantom{0}5.4 \%) \\
    commons-utils-es-default-60s & 2326\; (85.0 \%) & 112\; (45.3 \%) & 8354 & 8109\; (97.1 \%) & 245\; (\phantom{0}2.9 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

Table \ref{tab:mutants-per-project} shows the number of total, covered, and uncovered mutants per project for the INITIAL variant.
Around 8000 mutants across for the apache-commons-utils projects. Around 24000 mutants for the eqbench projects.
Total mutants for the ORIGINAL variant (i.e., before SPF execution) are around 5\% higher (not shown in this table). For filtering reasons, see Section~\ref{sec:filtering-eval}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

% Overall observations:
Figure~\ref{fig:mutation-detection-results} shows the percentage of detected mutants per project and generalization variant.
Results are relative to detected mutants, because our generalization approach --- by design --- does not affect coverage.
Detected mutants includes killed (X\% of overall detections), timed-out (A\%), memory error (B\%), and run error (C\%).

Generalization improves detected mutants for all evaluated projects and generalization variants (comparison with increasing EvoSuite runtime in Section~\ref{sec:runtime-eval}).
NAIVE and IMPROVED achieve similar improvements, with NAIVE performing slightly better on the eqbench projects, and IMPROVED performing slightly better on the commons-utils projects.
Using a higher \texttt{tries} setting for jqwik also increases the percentage of detected mutants. However, there seem to be diminishing returns: increasing tries from 10 to 50 achieves a larger improvement in mutation scores than increasing tries from 50 to 200.

In absolute numbers, generalization detects 3 new mutants for commons-utils, X--Y new mutants for commons-utils-es-1s to 60s, and X--Y new mutants for eqbench-es-1s to 60s. ...

@TODO: Mention that running EvoSuite + Teralizer generally achieves better results than running EvoSuite with a larger search budget (see \ref{sec:runtime-eval}).

% Observations on commons-utils with original tests:
(@TODO: Observations on commons-utils with original tests)
The lowest improvements are seen for the commons-utils project with 

% Observations on commons-utils with EvoSuite tests:
(@TODO: Observations on commons-utils with EvoSuite tests)

% Observations on eqbench with EvoSuite tests:
(@TODO: Observations on eqbench with EvoSuite tests)

\begin{table}[H]
  \caption{Number of mutants and percentage of detections per mutator.}
  \label{tab:detections-per-mutator}
  \begin{tabular}{lrrrrcrrrr}
    \toprule
    & & & & & \multicolumn{5}{c}{Detected \%} \\
    \cmidrule{6-10}
    Mutator & Total & Total \% & Min \% & Max \% & INITIAL & \multicolumn{2}{c}{NAIVE$_{200}$} & \multicolumn{2}{c}{IMPROVED$_{200}$} \\
    \midrule
    Math & 61841 & 59.10 & 52.34 & 62.16 & 50.99 & 54.98 & (+3.99) & 54.36 & (+3.37) \\
    RemoveConditionalOrderElse & 11501 & 10.99 & 8.50 & 11.94 & 61.08 & 62.29 & (+1.21) & 62.47 & (+1.39) \\
    ConditionalsBoundary & 11501 & 10.99 & 8.50 & 11.94 & 27.68 & 28.89 & (+1.21) & 30.23 & (+2.55) \\
    PrimitiveReturns & 7731 & 7.39 & 6.15 & 10.09 & 89.42 & 89.63 & (+0.20) & 89.90 & (+0.47) \\
    RemoveConditionalEqualElse & 5536 & 5.29 & 3.21 & 10.40 & 58.80 & 60.87 & (+2.07) & 61.00 & (+2.20) \\
    InvertNegs & 3122 & 2.98 & 2.94 & 3.12 & 58.91 & 60.61 & (+1.70) & 60.99 & (+2.08) \\
    VoidMethodCall & 973 & 0.93 & 0.58 & 1.35 & 24.96 & 24.96 & -- & 25.49 & (+0.53) \\
    NullReturnVals & 933 & 0.89 & 2.13 & 3.38 & 98.77 & 98.77 & -- & 98.77 & -- \\
    BooleanTrueReturnVals & 569 & 0.54 & 0.17 & 1.44 & 98.55 & 98.55 & -- & 98.55 & -- \\
    Increments & 546 & 0.52 & 0.50 & 0.54 & 72.81 & 73.38 & (+0.57) & 73.50 & (+0.69) \\
    BooleanFalseReturnVals & 250 & 0.24 & 0.09 & 0.63 & 87.87 & 87.87 & -- & 87.87 & -- \\
    EmptyObjectReturnVals & 141 & 0.13 & 0.41 & 0.43 & 90.30 & 90.30 & -- & 90.30 & -- \\
    \bottomrule
  \end{tabular}
\end{table}

Table \ref{tab:detections-per-mutator} shows the total number of mutants across all projects for each of the 12 mutators.
(Add explanations of the different mutators?)
Percentages are similar across projects, with the most common mutants coming from \texttt{MathMutator} (X--Y\% of total mutants across projects), \texttt{ConditionalsBoundaryMutator} (X--Y\%), and \texttt{RemoveConditionalMutator\_\-ORDER\_\-ELSE} (X--Y\%).

number of newly killed mutants  per project + variant

number / percentage of killing generalizations

differences between killed / unkilled mutants (here or in RQ4?)

\begin{table}[H]
  \caption{Model properties of mutants that are (not) detected by the IMPROVED$_{200}$ variant.}
  \label{tab:mutation-detection-comparison}
  \begin{tabular}{lcrrrrr}
    \toprule
            &          &         & Model Size & Operations & Constraints & Constr. Used \% \\
    Project & Detected & Mutants & (Median) & (Median) & (Median) & (Mean / Median) \\
    \midrule
    eqbench-es-default-1s & yes & 11145 & 182 & 9 & 2 & 47 \% / \phantom{0}80 \% \\
    eqbench-es-default-1s & no & 10347 & 207 & 16 & 5 & 24 \% / \phantom{0}50 \% \\
    \midrule
    eqbench-es-default-10s & yes & 11658 & 183 & 9 & 2 & 62 \% / 100 \% \\
    eqbench-es-default-10s & no & 9999 & 200 & 15 & 2 & 58 \% / 100 \% \\
    \midrule
    eqbench-es-default-60s & yes & 12052 & 182 & 9 & 2 & 70 \% / 100 \% \\
    eqbench-es-default-60s & no & 9958 & 171 & 11 & 2 & 68 \% / 100 \% \\
    \midrule
    commons-utils & yes & 4193 & 124 & 11 & 4 & 26 \% / \phantom{0}75 \% \\
    commons-utils & no & 1022 & 100 & 10 & 4 & 20 \% / \phantom{0}75 \% \\
    \midrule
    commons-utils-es-default-1s & yes & 4390 & 201 & 15 & 5 & 44 \% / \phantom{0}84 \% \\
    commons-utils-es-default-1s & no & 3183 & 550 & 45 & 6 & 12 \% / \phantom{0}50 \% \\
    \midrule
    commons-utils-es-default-10s & yes & 4660 & 267 & 23 & 5 & 47 \% / \phantom{0}85 \% \\
    commons-utils-es-default-10s & no & 3309 & 507 & 46 & 6 & 10 \% / \phantom{0}56 \% \\
    \midrule
    commons-utils-es-default-60s & yes & 4821 & 246 & 20 & 5 & 47 \% / \phantom{0}85 \% \\
    commons-utils-es-default-60s & no & 3288 & 417 & 41 & 6 & 11 \% / \phantom{0}54 \% \\
    \bottomrule
  \end{tabular}
\end{table}

@TODO: Check why model size / operations / constraints differ so much across the commons-utils projects. After all, the underlying program code is the same.

Detected mutants generally have smaller models with fewer operations as well as fewer and less complex constraints.
As a result, fewer of the constraints are used by the generalization (see explanation in \ref{sec:improved-generalization}).

Improvements in detected mutants are larger for the eqbench projects (which use more constraints)
than for the commons-utils projects (which use fewer constraints) (see Table~\ref{tab:mutants-per-mutator}).

The number of used constraints could be improved in future work.

\subsection{RQ2: Effects on Test Suite Size and Runtime}
\label{sec:ancillary-effects-eval}

effects on the number of tests in the test suite

\begin{table}[H]
\caption{Number of tests before and after generalization, with changes, per project.}
\label{tab:tests-per-project}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{6}{c}{Tests} \\
\cmidrule(lr){2-7}
Project & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
eqbench-es-default-1s & 4718 & 206 & 206 & 4718 & -- & -- \\
eqbench-es-default-10s & 4875 & 211 & 210 & 4876 & +1 & +0.02 \% \\
eqbench-es-default-60s & 4974 & 210 & 210 & 4974 & -- & -- \\
\midrule
commons-utils & 725 & 3 & 0 & 728 & +3 & +0.41 \% \\
commons-utils-es-default-1s & 2481 & 69 & 68 & 2482 & +1 & +0.04 \% \\
commons-utils-es-default-10s & 2738 & 70 & 69 & 2739 & +1 & +0.04 \% \\
commons-utils-es-default-60s & 2735 & 75 & 74 & 2736 & +1 & +0.04 \% \\
\bottomrule
\end{tabular}
\end{table}

effects on the number of lines of code in the test suite

\begin{table}[H]
\caption{Number of test lines before and after generalization, with changes, per project.}
\label{tab:lines-per-project}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{6}{c}{Lines} \\
\cmidrule(lr){2-7}
Project & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
eqbench-es-default-1s & 30989 & 19019 & 1302 & 48706 & +17717 & +57.2 \% \\
eqbench-es-default-10s & 32503 & 20353 & 1284 & 51572 & +19069 & +58.7 \% \\
eqbench-es-default-60s & 33510 & 20288 & 1285 & 52513 & +19003 & +56.7 \% \\
\midrule
commons-utils & 8561 & 457 & 0 & 9018 & +457 & +5.3 \% \\
commons-utils-es-default-1s & 16563 & 5261 & 413 & 21411 & +4848 & +29.3 \% \\
commons-utils-es-default-10s & 18124 & 5423 & 421 & 23126 & +5002 & +27.6 \% \\
commons-utils-es-default-60s & 17886 & 5801 & 452 & 23235 & +5349 & +29.9 \% \\
\bottomrule
\end{tabular}
\end{table}

effects on the total execution time of the test suite

\begin{table}[H]
\caption{Test suite runtime before and after generalization, with changes, per project.}
\label{tab:runtime-per-project}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{6}{c}{Runtime (in seconds)} \\
\cmidrule(lr){2-7}
Project & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
eqbench-es-default-1s & 17.44 & 101.62 & 0.68 & 118.38 & +100.94 & +578.7 \% \\
eqbench-es-default-10s & 16.70 & 139.64 & 0.56 & 155.77 & +139.08 & +832.9 \% \\
eqbench-es-default-60s & 18.21 & 124.68 & 0.69 & 142.20 & +123.99 & +681.0 \% \\
\midrule
commons-utils & 7.95 & 0.74 & 0.00 & 8.69 & +0.74 & +9.4 \% \\
commons-utils-es-default-1s & 4.31 & 29.49 & 0.14 & 33.66 & +29.35 & +680.5 \% \\
commons-utils-es-default-10s & 7.40 & 71.50 & 0.21 & 78.70 & +71.30 & +963.3 \% \\
commons-utils-es-default-60s & 6.30 & 28.07 & 0.08 & 34.29 & +27.99 & +444.1 \% \\
\bottomrule
\end{tabular}
\end{table}

effects on the execution time of individual tests

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_test_runtime_differences}
  \caption{Runtime Comparison: Test vs. Generalization.}
  %\Description{@TODO}
  \label{fig:test-runtime-differences}
\end{figure}

@TODO: Split Figure~\ref{fig:test-runtime-differences} into two (sub-)figures to make each one individually referenceable.

\subsection{RQ3: Runtime Requirements}
\label{sec:runtime-eval}

only one run for each configuration (due to high runtime requirements)
investigation for a subset of configurations confirmed that the trends hold

total runtime (+ runtime per project?)
@TOOD: Probably can remove Table~\ref{tab:teralizer-runtimes} and just mention the total runtime somewhere. We have a more detailed overview in Figure~\ref{fig:teralizer-runtimes} anyway.

\begin{table}[H]
  \caption{Total runtimes of Teralizer for all evaluated projects.}
  \label{tab:teralizer-runtimes}
  \begin{tabular}{lr}
    \toprule
    Project & Runtime \\ 
    \midrule
    eqbench-es-default-1s & 24h 46min 27s \\ 
    eqbench-es-default-10s & 28h 16min 32s \\ 
    eqbench-es-default-60s & 30h 53min 22s \\ 
    \midrule 
    commons-utils & 3h 23min 22s \\ 
    commons-utils-es-default-1s & 8h 13min 55s \\ 
    commons-utils-es-default-10s & 9h 46min 48s \\ 
    commons-utils-es-default-60s & 9h 04min 32s \\ 
    \bottomrule 
  \end{tabular} 
\end{table}

runtime per project + processing stage + variant

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetCommons{} results in Figure~\ref{fig:teralizer-runtimes}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_teralizer_runtimes}
  \caption{Teralizer runtimes per project, processing stage, and generalization variant.}
  %\Description{@TODO}
  \label{fig:teralizer-runtimes}
\end{figure}

efficiency relative to higher EvoSuite search budgets

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetCommons{} results in Figure~\ref{fig:teralizer-efficiency}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{fig_teralizer_efficiency.pdf}
  \caption{Pareto fronts for EvoSuite and Teralizer variants across projects.}
  \label{fig:teralizer-efficiency}
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project: commons-utils.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & EvoSuite & Teralizer & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 56.8 & 4648.7 \\
          2 & 10s & - & 57.3 & 5597.3 \\
          3 & 1s & IMPROVED$_{10}$ & 57.9 & 7294.5 \\
          4 & 60s & - & 58.1 & 10239.8 \\
          5 & 10s & NAIVE$_{10}$ & 58.1 & 10445.1 \\
          6 & 10s & IMPROVED$_{50}$ & 58.4 & 10603.4 \\
          7 & 10s & IMPROVED$_{10}$ & 58.4 & 11081.5 \\
          8 & 10s & IMPROVED$_{200}$ & 58.5 & 13270.4 \\
          9 & 60s & IMPROVED$_{10}$ & 59.3 & 13938.7 \\
          10 & 60s & IMPROVED$_{50}$ & 59.4 & 14727.5 \\
          11 & 60s & IMPROVED$_{200}$ & 59.5 & 15735.7 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project: eqbench.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & EvoSuite & Teralizer & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 48.1 & 26479.3 \\
          2 & 10s & - & 50.6 & 29861.1 \\
          3 & 1s & NAIVE$_{10}$ & 50.7 & 36727.9 \\
          4 & 1s & IMPROVED$_{50}$ & 51.4 & 37457.4 \\
          5 & 1s & NAIVE$_{50}$ & 51.7 & 37531.9 \\
          6 & 10s & IMPROVED$_{10}$ & 51.9 & 41525.3 \\
          7 & 10s & IMPROVED$_{50}$ & 53.6 & 42256.2 \\
          8 & 10s & NAIVE$_{50}$ & 53.8 & 45398.0 \\
          9 & 10s & IMPROVED$_{200}$ & 53.8 & 48268.6 \\
          10 & 10s & NAIVE$_{200}$ & 54.1 & 62938.2 \\
          11 & 60s & IMPROVED$_{50}$ & 54.5 & 68092.9 \\
          12 & 60s & NAIVE$_{50}$ & 54.7 & 68782.2 \\
          13 & 60s & IMPROVED$_{200}$ & 54.8 & 75081.4 \\
          14 & 60s & NAIVE$_{200}$ & 55.0 & 93017.1 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
\end{figure}

large potential for runtime improvements, see examples in the discussion

\subsection{RQ4 (Part 1 of 2): Causes of Unsuccessful Generalizations in the Evaluation Dataset}
\label{sec:filtering-eval}

In this section, we describe causes of unsuccessful generalizations in the main
evaluation dataset (commons-utils + evosuite-variants, eqbench + evosuite variants).
To get more generalizable results that enable better informed decisions about future
research directions, we also evaluated generalization success vs. failure in <number>
additional open source projects beyond the main evaluation dataset. Results of the
extended evaluation are discussed in Section~\ref{sec:filtering-eval-extended}.

overall test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Included and excluded counts by variant and level.}
  \label{tab:exclusions-summary}
  \begin{tabular}{llrrr}
    \toprule
    Variant & Type & Total & \multicolumn{1}{c}{Included} & \multicolumn{1}{c}{Excluded} \\
    \midrule
    SHARED & Test & 23246 & 19306\; (83.1\%) & 3940\; (16.9\%) \\
    SHARED & Assertion & 28923 & 13836\; (47.8\%) & 15087\; (52.2\%) \\
    BASELINE & Generalization & 13836 & 13814\; (99.8\%) & 22\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & 13836 & 10743\; (77.6\%) & 3093\; (22.4\%) \\
    NAIVE$_{50}$ & Generalization & 13836 & 9964\; (72.0\%) & 3872\; (28.0\%) \\
    NAIVE$_{200}$ & Generalization & 13836 & 9881\; (71.4\%) & 3955\; (28.6\%) \\
    IMPROVED$_{10}$ & Generalization & 13836 & 11788\; (85.2\%) & 2048\; (14.8\%) \\
    IMPROVED$_{50}$ & Generalization & 13836 & 11660\; (84.3\%) & 2176\; (15.7\%) \\
    IMPROVED$_{200}$ & Generalization & 13836 & 11597\; (83.8\%) & 2239\; (16.2\%) \\
    \bottomrule
  \end{tabular}
\end{table}

filtering-based test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Filtering results for tests, assertions, and generalizations by filter and (generalization) variant.}
  \label{tab:exclusions-filtering}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 23246 & 21719\; (93.4\%) & 1527.0\; (\phantom{0}6.6\%) \\
    SHARED & Test & TestType & 23246 & 23066\; (99.2\%) & 180.0\; (\phantom{0}0.8\%) \\
    SHARED & Test & NoAssertions & 21532 & 19306\; (89.7\%) & 2226.0\; (10.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 28923 & 27326\; (94.5\%) & 1597.0\; (\phantom{0}5.5\%) \\
    SHARED & Assertion & MissingValue & 28923 & 21766\; (75.3\%) & 7157.0\; (24.7\%) \\
    SHARED & Assertion & ParameterType & 28923 & 17835\; (61.7\%) & 11088.0\; (38.3\%) \\
    SHARED & Assertion & UnsupportedAssertion & 28923 & 28180\; (97.4\%) & 743.0\; (\phantom{0}2.6\%) \\
    SHARED & Assertion & VoidReturnType & 28923 & 21763\; (75.2\%) & 7160.0\; (24.8\%) \\
    \midrule
    BASELINE & Generalization & NonPassingTest & 13836 & 13814\; (99.8\%) & 22.0\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & NonPassingTest & 13804 & 10743\; (77.8\%) & 3061.0\; (22.2\%) \\
    NAIVE$_{50}$ & Generalization & NonPassingTest & 13804 & 9964\; (72.2\%) & 3840.0\; (27.8\%) \\
    NAIVE$_{200}$ & Generalization & NonPassingTest & 13804 & 9881\; (71.6\%) & 3923.0\; (28.4\%) \\
    IMPROVED$_{10}$ & Generalization & NonPassingTest & 13804 & 11788\; (85.4\%) & 2016.0\; (14.6\%) \\
    IMPROVED$_{50}$ & Generalization & NonPassingTest & 13804 & 11660\; (84.5\%) & 2144.0\; (15.5\%) \\
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 13804 & 11597\; (84.0\%) & 2207.0\; (16.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

failing tests

\begin{table}[H]
  \caption{Number of test execution failures by exception type and (generalization) variant.}
  \label{tab:exclusions-test-fails}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Variant & ORIGINAL & BASELINE & \multicolumn{3}{c}{NAIVE} & \multicolumn{3}{c}{IMPROVED} \\
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
    Tries & - & - & 10 & 50 & 200 & 10 & 50 & 200 \\
    \midrule
    ArithmeticException & 0 & 0 & 99 & 99 & 99 & 57 & 58 & 58 \\
    AssertionFailedError & 132 & 22 & 729 & 803 & 819 & 752 & 845 & 866 \\
    NumberFormatException & 0 & 0 & 0 & 0 & 0 & 18 & 18 & 18 \\
    TooManyFilterMissesException & 0 & 0 & 2233 & 2938 & 3005 & 1189 & 1223 & 1265 \\
    \bottomrule
  \end{tabular}
\end{table}

@TODO: Remove Table~\ref{tab:exclusions-spf}. Describe SPF execution failures in text only.

\begin{table}[H]
  \centering
  \caption{Number of SPF execution failures by error type.}
  \label{tab:exclusions-spf}
  \begin{tabular}{lrr}
    \toprule
    Error Type & Total & Percent \\
    \midrule
    SPF exception & 1540 & 51.42 \\
    PC size limit exceeded & 790 & 26.38 \\
    Depth limit exceeded & 524 & 17.50 \\
    Teralizer exception & 97 & 3.24 \\
    Execution timeout & 28 & 0.93 \\
    OutOfMemoryError & 16 & 0.53 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ4 (Part 2 of 2): Causes of Unsuccessful Generalizations in Other Open Source Projects}
\label{sec:filtering-eval-extended}

This evaluation only uses the IMPROVED$_{200}$
generalization variant, but results also apply to all other variants.

\begin{table}[H]
  \caption{Number of processing failures and remaining projects per processing stage.}
  \label{tab:processing-failures-per-stage}
  \begin{tabular}{l r r}
    \toprule
    Processing Stage & Failures & Remaining Projects \\
    \midrule
    Total projects & - & 1160\; (\phantom{.}100 \%) \\
    \cmidrule(lr){1-3}
    SETUP\_PROJECT & 355 & 805\; (69.4 \%) \\
    BUILD\_PROJECT\_ORIGINAL & 189 & 616\; (53.1 \%) \\
    BUILD\_SPOON\_MODEL & 8 & 608\; (52.4 \%) \\
    EXECUTE\_TESTS\_ORIGINAL & 61 & 547\; (47.2 \%) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & 31 & 516\; (44.5 \%) \\
    BUILD\_PROJECT\_INSTRUMENTED & 1 & 515\; (44.4 \%) \\
    EXECUTE\_TESTS\_INITIAL & 130 & 385\; (33.2 \%) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & 41 & 344\; (29.7 \%) \\
    COLLECT\_PIT\_DATA\_INITIAL & 64 & 280\; (24.1 \%) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & 270 & 10\; (\phantom{0}0.9 \%) \\
    \cmidrule(lr){1-3}
    Successfully processed & - & 10\; (\phantom{0}0.9 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \caption{Causes of processing failures per processing stage.}
  \label{tab:processing-failure-causes}
  \begin{tabularx}{\textwidth}{l X}
    \toprule
    Processing Stage & Causes of Processing Failures \\
    \midrule
    SETUP\_PROJECT & dependency resolution error (329), sources / tests not found (26) \\
    BUILD\_PROJECT\_ORIGINAL & compilation error (171), compilation outputs not found (18) \\
    BUILD\_SPOON\_MODEL & Spoon execution error (8) \\
    EXECUTE\_TESTS\_ORIGINAL & JUnit execution error (13), timeout exceeded (48) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & JUnit outputs not found (31) \\
    BUILD\_PROJECT\_INSTRUMENTED & compilation error (1) \\
    EXECUTE\_TESTS\_INITIAL & all tests excluded (129), timeout exceeded (1) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & JaCoCo execution error (1), JaCoCo outputs not found (40) \\
    COLLECT\_PIT\_DATA\_INITIAL & PIT execution error (16), PIT outputs not found (4), all classes excluded (3), failed to map PIT data to a test (1), timeout exceeded (40) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & all generalizations excluded (269), failed to map PIT data to a generalization (1) \\
    \bottomrule
  \end{tabularx}
\end{table}

Broadly speaking, there are three main ways to increase the number of projects
that can be successfully processed: (i)~reducing exclusions caused by filtering
(129 + 3 + 269 = 401 projects), (ii)~adding support for more varied project structures
(26 + 18 + 31 + 40 + 4 = 119 projects), and (iii)~increasing timeouts (48 + 1 + 40 = 89 projects).
Depedency resolution and compilation errors in the original project code (329 + 171 = 500 projects)
as well as external tool errors (8 + 13 + 1 + 16 = 38 projects) are less actionable.
Furthermore, Teralizer errors only occur in a small number of cases (1 + 1 + 1 = 3 projects),
so also offer comparatively little opportunity for improvements.

\begin{table}[H]
  \caption{Filtering results of the extended dataset for tests, assertions, and generalizations.}
  \label{tab:exclusions-filtering-extended}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 74332 & 65579\; (88.2\%) & 8753.0\; (11.8\%) \\
    SHARED & Test & TestType & 74332 & 65052\; (87.5\%) & 9280.0\; (12.5\%) \\
    SHARED & Test & NoAssertions & 56853 & 33390\; (58.7\%) & 23463.0\; (41.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 122166 & 101519\; (83.1\%) & 20647.0\; (16.9\%) \\
    SHARED & Assertion & MissingValue & 122166 & 51430\; (42.1\%) & 70736.0\; (57.9\%) \\
    SHARED & Assertion & ParameterType & 122166 & 5393\; (\phantom{0}4.4\%) & 116773.0\; (95.6\%) \\
    SHARED & Assertion & ReturnType & 122166 & 11650\; (\phantom{0}9.5\%) & 110516.0\; (90.5\%) \\
    SHARED & Assertion & UnsupportedAssertion & 122166 & 92996\; (76.1\%) & 29170.0\; (23.9\%) \\
    \midrule
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 229 & 206\; (90.0\%) & 23.0\; (10.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

The most common filtering reasons are unsupported parameter and return types.

@TODO: Better explain these filtering results by prooviding information about:
(i) which types are identified in the tested method signatures, and
(ii) which assertions are used in the tests.

Successful generalizations did not improve mutation scores in any of the successfully
processed projects. However, some generalizations indicate that there weak
preconditions are used in the source / test code of the original implementation.

@TODO: Check which generalizations fail the "NonPassingTest" filter due to (i) bugs in
our generalization implemenation vs. (ii) weak preconditions.

% Performance improvements of Teralizer itself can NOT be used as a substitute for
% (i) because the timeouts occur for tasks that are performed by external tools.

% ------------------------------------------------------------------------------
% DISCUSSION
% ------------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

\subsection{Benefits of the Approach}
\label{sec:benefits-of-the-approach}

\subsection{Potential for Future Improvements}
\label{sec:potential-for-improvements}

\subsubsection{Improving the Mutation Score of Generalized Tests}
\label{sec:improving-the-mutation-score}

\subsubsection{Improving the Size of Generalized Tests}
\label{sec:improving-the-test-size}

\subsubsection{Improving the Runtime of Test Generalization}
\label{sec:improving-the-runtime}

primary runtime costs are from external tools (i.e., jqwik + PIT)

mutation testing runtimes could likely be improved
through the use of the pitest accelerator plugin (\url{https://docs.arcmutate.com/docs/accelerator.html}),
thus, further improving results of Teralizer relative to EvoSuite
(website claims "analysis time for Commons Lang is reduced from over 55 minutes to under 18");
however, the plugin is not available without a license

also, parallelization would be possible rather easily for many Teralizer tasks
(and also for PIT, which only runs single-threaded by default)
(supposedly, jqwik will get parallelization support in major version 2, source (2024-05-14): \url{https://github.com/jqwik-team/jqwik/issues/45#issuecomment-2109949764})

for practical application scenarios, could add configuration to target only specific tests, rather than the full test suite

\subsubsection{Reducing the Number of Unsuccessful Generalizations}
\label{sec:reducing-unsuccessful-generalizations}

\subsubsection{Using Test Generalization for Test Suite Reduction}
\label{sec:test-suite-reduction}

\subsubsection{Dealing with Overfitting}
\label{sec:overfitting}

We do not generate assertions, so overfitting is less pronounced than if we did.
However, we still rely on the current implementation as the source of truth,
encoding the exact behavior of it in the generalized tests (at least as far as
the behavior is tested by assertions in the original, non-generalized test suite).

\subsection{Threats to Validity}
\label{sec:threats-to-validity}

\subsubsection{Construct Validity}
\label{sec:construct-validity}

\subsubsection{Internal Validity}
\label{sec:internal-validity}

\subsubsection{External Validity}
\label{sec:external-validity}

% ------------------------------------------------------------------------------
% RELATED WORK
% ------------------------------------------------------------------------------

\section{Related Work}
\label{sec:related-work}

% ------------------------------------------------------------------------------
% CONCLUSIONS
% ------------------------------------------------------------------------------

\section{Conclusions}
\label{sec:conclusions}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This research was funded in whole or in part by the Austrian Science Fund (FWF) 10.55776/P36698. For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
