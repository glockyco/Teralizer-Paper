%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[manuscript,screen,review]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%\acmBooktitle{ACM Transactions on Software Engineering and Methodology}
% \acmISBN{978-1-4503-XXXX-X/2018/06}
\acmJournal{TOSEM}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

\usepackage{tabularx}

\newcommand{\ToolTeralizer}{\textsc{Teralizer}}

\newcommand{\ToolJPFLong}{Java PathFinder}
\newcommand{\ToolJPF}{JPF}

\newcommand{\ToolSPFLong}{Symbolic PathFinder}
\newcommand{\ToolSPF}{SPF}

\newcommand{\ToolJqwik}{jqwik}
\newcommand{\ToolJacoco}{JaCoCo}
\newcommand{\ToolPit}{PIT}

\newcommand{\ToolEvoSuite}{\textsc{EvoSuite}}

\newcommand{\DatasetEqBench}{\textsc{EqBench}}
\newcommand{\DatasetEqBenchA}{eqbench-es-1s}
\newcommand{\DatasetEqBenchB}{eqbench-es-10s}
\newcommand{\DatasetEqBenchC}{eqbench-es-60s}
\newcommand{\DatasetsEqBenchEs}{eqbench-es-$*$}

\newcommand{\DatasetCommonsDev}{commons-utils-dev}
\newcommand{\DatasetCommonsA}{commons-utils-es-1s}
\newcommand{\DatasetCommonsB}{commons-utils-es-10s}
\newcommand{\DatasetCommonsC}{commons-utils-es-60s}
\newcommand{\DatasetsCommons}{commons-utils-$*$}
\newcommand{\DatasetsCommonsEs}{commons-utils-es-$*$}

\newcommand{\DatasetRepoReapers}{repo-reapers}

\newcommand{\VariantOriginal}{\textsc{Original}}
\newcommand{\VariantInitial}{\textsc{Initial}}
\newcommand{\VariantBaseline}{\textsc{Baseline}}

\newcommand{\VariantNaive}{\textsc{Naive}}
\newcommand{\VariantNaiveA}{\textsc{Naive\textsubscript{10}}}
\newcommand{\VariantNaiveB}{\textsc{Naive\textsubscript{50}}}
\newcommand{\VariantNaiveC}{\textsc{Naive\textsubscript{200}}}

\newcommand{\VariantImproved}{\textsc{Improved}}
\newcommand{\VariantImprovedA}{\textsc{Improved\textsubscript{10}}}
\newcommand{\VariantImprovedB}{\textsc{Improved\textsubscript{50}}}
\newcommand{\VariantImprovedC}{\textsc{Improved\textsubscript{200}}}

\newcommand{\tries}{\texttt{tries}}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
% \title[short title]{full title}
\title{Teralizer: Something About Automated Test Generalization}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Johann Glock}
\email{johann.glock@aau.at}
\orcid{0000-0002-0152-8611}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

\author{Clemens Bauer}
\email{clemens.bauer@aau.at}
\orcid{0009-0000-9199-8563}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

\author{Martin Pinzger}
\email{martin.pinzger@aau.at}
\orcid{0000-0002-5536-3859}
\affiliation{%
  \institution{University of Klagenfurt}
  \city{Klagenfurt}
  \country{Austria}
}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Glock et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  Abstract
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
\ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\received{n/a}
\received[revised]{n/a}
\received[accepted]{n/a}

%%
%% A "teaser figure" is an image, or set of images in one figure,
%% that are placed after all author and affiliation information,
%% and before the body of the article, spanning the page. If you
%% wish to have such a figure in your article, place the command
%% immediately before the \maketitle command:
%% \begin{teaserfigure}
%%   \includegraphics[width=\textwidth]{sampleteaser}
%%   \caption{figure caption}
%%   \Description{figure description}
%% \end{teaserfigure}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

% ------------------------------------------------------------------------------
% INTRODUCTION
% ------------------------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}

% ------------------------------------------------------------------------------
% BACKGROUND
% ------------------------------------------------------------------------------

\section{Background}
\label{sec:background}

\subsection{Property-based Testing}
\label{sec:property-based-testing}

\subsection{Test Generalization}
\label{sec:test-generalization}

% ------------------------------------------------------------------------------
% APPROACH
% ------------------------------------------------------------------------------

\section{Approach}
\label{sec:approach}

\subsection{Overview}
\label{sec:approach-overview}

processing pipeline:
CLEANUP\_PROJECT(0),
%
DOWNLOAD\_PROJECT(1),
SETUP\_PROJECT(2),
%
ADD\_DEPENDENCIES(3),
BUILD\_PROJECT\_ORIGINAL(4),
%
GENERATE\_EVOSUITE\_TESTS(5),
POSTPROCESS\_EVOSUITE\_TESTS(6),
%
BUILD\_SPOON\_MODEL(7),
%
EXECUTE\_TESTS\_ORIGINAL(8),
COLLECT\_JUNIT\_REPORTS\_ORIGINAL(9),
COLLECT\_JACOCO\_DATA\_ORIGINAL(10),
FILTER\_TESTS\_ORIGINAL(11),
COLLECT\_PIT\_DATA\_ORIGINAL(12),
%
ANALYZE\_TESTS(13),
FILTER\_TESTS(14),
FILTER\_ASSERTIONS(15),
%
ADD\_JPF\_INSTRUMENTATION(16),
BUILD\_PROJECT\_INSTRUMENTED(17),
EXECUTE\_JPF(18),
ANALYZE\_JPF(19),
CLEANUP\_JPF\_INSTRUMENTATION(20),
%
BUILD\_PROJECT\_INITIAL(21),
EXECUTE\_TESTS\_INITIAL(22),
%
COLLECT\_JUNIT\_REPORTS\_INITIAL(23),
COLLECT\_JACOCO\_DATA\_INITIAL(24),
COLLECT\_PIT\_DATA\_INITIAL(25),
%
CLEANUP\_GENERALIZATION(26),
%
GENERALIZE\_TESTS(27),
BUILD\_PROJECT\_GENERALIZED(28),
%
EXECUTE\_TESTS\_GENERALIZED(29),
COLLECT\_JUNIT\_REPORTS\_GENERALIZED(30),
FILTER\_GENERALIZATIONS(31),
%
COLLECT\_JACOCO\_DATA\_GENERALIZED(32),
COLLECT\_PIT\_DATA\_GENERALIZED(33);

basically:
- instrument project
- detect test methods + assertions + tested methods
- for each assertion / tested method, collect input + output specification with SPF
- for each assertion / tested method, create generalization with 3 variants (utilizing extracted specifications): BASELINE, NAIVE, IMPROVED

at various points, apply filtering to avoid processing of tests / assertions / generalizations that are unlikely to successfully pass all processing steps (or even guaranteed to fail)

thoughout the pipeline, track (i) runtime, (ii) (intermediate) results (success? failure? causes? other task outputs / created files?), (iii) mutation / coverage / test data;

offers a cleanup task to revert any changes applied by generalization (we only add files, so cleanup is just removing files; no existing files are modified);

\subsection{Project Instrumentation}
\label{sec:project-instrumentation}

(shorten + merge this with the overview? we have so many subsections...)

accepts either a URL to Git repository or a path to local directory as target;
if target is a URL to a Git repository, the project is cloned automatically (with Git's default settings);
processing then continues the same for both types of projects;

detects JUnit 4 vs. JUnit 5 testing framework, others not supported;
detects Maven vs. Gradle (Groovy DSL) projects, others not supported;
adds required dependencies based on detected project type (update JUnit, add JUnit Vintage, add JaCoCo, add PIT, add jqwik);
creates separate pom.xml / build.gradle (with comments for additions by Teralizer) - original file is left untouched;
to verify successful instrumentation, project is built, tests executed, test / coverage / mutation results collected;

\subsection{Test / Assertion Detection and Analysis}
\label{sec:test-analysis}

execute test suite;
identify executed test methods via junit / surefire XML reports;
for each test method, identify all assertions in the method via Spoon (calls to methods in "org.junit.Assert" (JUnit 4) or "org.junit.jupiter.api.Assertions" (JUnit 5)));
for each (supported) assertion (assertEquals, assertTrue, assertFalse, assertThrows), identify one tested method call via Spoon (for assertThrows: the executed method, for assertEquals/True/False: the method that returned the "actual" value of the assertion);
we assume each assertion "tests" one method, and do not consider side effects => no support for, e.g., method sequences that modify object state (e.g.: list.add(...), assert(..., list.size()));
side effects could be modeled as inputs/outputs of the tested method;

tests and assertions are filtered if they cannot be (correctly) handled by the current implementation. for details, see Section "Test / Assertion / Generalization Filtering".

\subsection{Specification Extraction}
\label{sec:specification-extraction}

\subsubsection{Driver Generation}
\label{sec:driver-generation}

for each assertion / tested method call, create a driver program and a corresponding SPF configuration;
the driver program is a single class with a "public static void main(...)" method;

the main method:
(i) executes any setup methods of the test class (JUnit 4: "@Before", "@BeforeClass", JUnit 5: "@BeforeAll", "@BeforeEach"),
(ii) creates an instance of the test class, and then
(iii) calls the tested method.

the SPF configuration:
(i) sets the main method of the driver program as the entrypoint for SPF execution,
(ii) sets the tested method as SPF's "symbolicMethod" (using symbolic inputs for generalizable input parameters),
(ii) configures SPF to run in constraint-collection mode (=> no constraint solving, just following the concrete execution path),
(iii) registers + configures a custom listener that extracts input-output specifications (see next Section "SPF Execution"),
(iv) configures several execution limits (depth limit, execution time limit, etc.; see Section "Other Limits / Safeguards").

(note: we had to modify (fix?) SPF to disable constraint solving during constraint-collection mode execution.)
(note: we're actually also creating an instrumented version of the test class / method, but that's only there so we can more easily identify the tested method call during SPF execution if the same method is called multiple times.)

\subsubsection{SPF Execution}
\label{sec:spf-execution}

execute SPF once for each driver program;
start tracking symbolic state when entering the tested method;
when exiting the tested method:
(i) write the concrete input values to a file,
(ii) write the concrete output values to a file,
(iii) write the symbolic input values to a file (=> path condition / input specification),
(iv) write the symbolic output values to a file (=> output specification),
then immediately terminate the execution (no need to keep going - we have everything we need);

((show an example of extracted data here))

for tested methods that exit via thrown exception,
use the thrown exception (type) as the concrete output value.
no symbolic output value can be collected in this case
(because the (type of the) thrown exception is not a function of the symbolic input values,
but is instead constant for all sets of inputs in the partition).

\subsection{Test Transformation}
\label{sec:test-transformation}


using jqwik (1.8.5) for property-based testing (\url{https://github.com/jqwik-team/jqwik});
last official release in 2024 (1.9.2);
last commit 2 days ago (checked on: 2005-04-23);
590 GitHub stars;
built for junit 5!;
comprehensive user guide (\url{https://jqwik.net/docs/current/user-guide.html});

competition 1:
junit-quickcheck (\url{https://github.com/pholser/junit-quickcheck});
last official release in 2020 (1.0);
last commit 8 months ago;
590 GitHub stars;
built for junit 4, junit 5 support only via junit-vintage (\url{https://github.com/pholser/junit-quickcheck/issues/189#issuecomment-414706607});
documentation less comprehensive (\url{https://pholser.github.io/junit-quickcheck/site/1.0/index.html});

competition 2:
quicktheories (\url{https://github.com/quicktheories/QuickTheories});
last official release in 2018 (0.25);
last commit 6 years ago;
509 GitHub stars;
built for junit 4;

\subsubsection{"BASELINE" Generalization}
\label{sec:baseline-generalization}

transforms target test into a property-based test;
one test class per generalizable assertion;
(PIT only offers class-level selections / exclusions, so generating classes causes less "collateral damage" for failing generalizations);
uses only the original set of input values via custom arbitrary;

allows us to see how much runtime overhead jqwik introduces even without any generation of input values;

transformation steps:
clone the original test class (all further actions on the cloned class);
delete other test methods in the class (non-test methods need to be preserved because they might be used by the target test);
add a nested class "TestParameters" that can hold values for all generalizable parameters of the tested method (i.e., all parameters of type byte, short, int, long, float, double);
add a nested class "TestParametersSupplier" that can generate "arbitrary" (jqwik term) "TestParameters" instances;
for the BASELINE variant, only one instance of TestParameters is generated by the supplier;
this instance uses the same tested method input values as the original test;
delete all existingTest annotations from the test method (removing @Test is most important, but other annotations are removed as well because they are unlikely to be compatible with @Property);
add jqwik @Property annotation (seed = 0, ShrinkingMode.OFF, EdgeCasesMode.FIRST, tries = 10 / 50 / 200);
add parameter of type "TestParameters \_p\_" with annotation \@ForAll(supplier = TestParametersSupplier) to the test method;
replace tested method arguments with values from TestParameters instance \_p\_ (e.g., foo(a, b, c) -> foo(\_p\_.a, \_p\_.b, \_p\_.c); only for generalizable inputs, the others remain unchanged).
delete other assertions in the test method (unless they have return values that are used in the code, e.g., Exception e = assertThrows(...));
no need to modify assertions (because inputs stay the same, so expected outputs should also stay the same);

the original test method is always preserved in the current implementation;
this would not be necessary for cases where there is only a single assertion in the test method and generalization is successful;
this would also not be necessary for casses where all assertions in a test method are successfully generalized;
statistics on how common these cases are in R1 (or RQ2? or RQ4?).

because most of the test method is copied for each generalized assertion, this creates a lot of duplicate code;
the currently implementation does not optimize for this at all - statistics on test suite size increases in RQ2;
some of this could likely be avoided by putting in more engineering effort, e.g., automatically extracting setup functions that can be reused across all generalizations of a test method;
alternatively, we could add multiple TestParameters parameters (one for each generalized assertion) - but that might not be very maintainable either;

\subsubsection{"NAIVE" Generalization}
\label{sec:naive-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "naive" approach for selecting sets of input values;

basic approach:
step 1: randomly generate sets of values that match the types of input parameters;
step 2: apply filter to keep only value sets that satisfy the input specification;
repeat until desired number of input sets (we use 10, 50, 200 in the evaluation) has been generated (automatically done by jqwik, we just set how many we want);
still preserves original test inputs via a custom arbitrary => no reduction of mutation score due to "bad" random values;
beware that generalization does NOT change coverage - we only test additional inputs of already covered input partitions;

problem: many TooManyFilterMissesExceptions;
reason: depending on the input specification, randomly selecting sets of input values can be (very) unlikely to produce satisfying inputs (e.g., a = b = c => 3 random ints that are equal);

example: a = b = c (all ints);
randomly generate a;
randomly generate b;
randomly generate c;
apply the a = b = c filter (likely not a match => throw away and try again; after too many non-matching attempts => TooManyFilterMissesExceptions)

\subsubsection{"IMPROVED" Generalization}
\label{sec:improved-generalization}

same basic processing flow as BASELINE variant (clone class, add TestParameters + Supplier, remove other tests + assertions);
uses "improved" approach for selecting sets of input values to reduce TooManyFilterMissesExceptions;

basic approach:
step 1: generate sets of input values that already take into account "as many constraints as possible";
step 2: apply filter to keep only value sets that satisfy the full input specification;
repeat until the desired number of input sets has been generated;
like NAIVE, IMPROVED also preserves original test inputs via a custom arbitrary;
like NAIVE, IMPROVED also does NOT cover any previously uncovered input partitions;

example: a = b \&\& b = c (all ints);
randomly generate a since we don't have any constraints to consider yet;
generate b such that b = a (i.e., take into account the a = b constraint);
generate c such that b = c (i.e., take into account the b = c constraint);
apply the a = b = c filter (trivial in this case => use a more interesting example);

currently only considers the following constraints:
var1 == (var2 | const);
var1 < (var2 | const);
var1 <= (var2 | const);
var1 > (var2 | const);
var1 >= (var2 | const);

supported types: byte, short, int, long, float, double;
mixed-type constraints are also supported (e.g., int-var < float-var);

more complex terms (e.g., "compound" terms (is this the correct terminology?), (trigonometric) function calls) are not taken into account (e.g., a < b - c, a == cos(b));
constraints that are not equality, upper- or lower-bound constraints are not taken into account either (e.g., inequality constraints);
=> show some statistics about used vs. unused constraints -> further details in RQ4 or the discussion;

actual value selection logic (code that implements this logic for all generalizable inputs is automatically generated):

if at least one equality constraint exists for a variable, all other constraints are ignored
if multiple equality constraints exist, we just take "the first one";
all equality constraints have the same value anyway because we select these at runtime based on whichever concrete values have already been assigned to involved variables;

if multiple upper / lower bounds exist, the strongest bound is used, i.e., the highest lower bound and the lowest upper bound.
as with equality constraints, this is determined at runtime, i.e., based on on whichever values have already been assigned to involved variables;

in practice, naive processing of constraints often leads to "dead ends" where no further assignments are possible;
for example: a >= b \&\& b >= a; here we would have a ">= b" constraint on "a" and a ">= a" constraint on "b";
to resolve such situations, we assign an index to each variable that occurs in the input specification, e.g., idx(a)=1, idx(b)=2;
constraints are then rewritten to apply only to the variable with the highest index, e.g. (i) "a >= b" -> "b <= a", and (ii) "b >= a" -> "b >= a";
thus, the used constraints for variable value selection become: a -> no constraints, b -> {"<= a", ">= a"};
since a is now unconstrained, we can simply select a random value for it; once a is assined, b can be assigned as well; 
similar transformations are applied for all suppored constraints (==, <, <=, >, >=);
(todo: find the correct terminology to describe this; perhaps constraint rewriting / simplification?
other related terms: constraint satisfaction problems, variable elimination, constraint propagation, domain reduction, and arc consistency algorithms)

in some cases, choices of early variables lead to unsatisfiable constraints later on;
for example: b > a \&\& b <= 0 -> no satisfying assignment for b if a a >= 0;
in this cases, we return an empty arbitrary for b, thus prompting jqwik to pick a new value for a;
similar situations occur for over-/underflows, e.g., b > a with a = Integer.MAX\_VALUE;
in this case, b = a + 1 = Integer.MIN\_VALUE, which will be rejected by filtering;

some current limitations could be resolved through more engineering effort (e.g., custom arbitraries);
one way to generate values that satisfy all / more constraints would be through constraint solving (add references);
however, this would have a significant runtime cost and would still suffer various limitations (add references);

\subsection{Test / Assertion / Generalization Filtering}
\label{sec:all-filtering}

describe filtering here or in RQ4?

we need to ensure that:
1: we do not generate any incompilable code;
2: we have a green test suite for mutation testing (PIT only works with green suites - otherwise it throws an error);
also, we would like to avoid spending processing effort on generalization attempts that are unlikely to be successful (=> early excludes);
to achieve this, we apply filtering at multiple stages + levels in the processing pipeline (for filtering data, see RQ4).

(note: large reduction of required filtering would be possible by putting in more engineering effort)

\subsubsection{Test-level Filtering}
\label{sec:test-filtering}

filterTestOriginal:
NonPassingTestFilter: filters failing tests (PIT requires green suite, and generalization of failing tests does not seem useful anyway);
TestTypeFilter: filters tests with unsupported test annotations (we currently only support @Test annotations);

filterTest:
NoAssertionsFilter: filters tests without assertions (with no assertions, choosing a target method is even trickier than it already is, and we have no useful output specification);

\subsubsection{Assertion-level Filtering}
\label{sec:assertion-filtering}

filterAssertion:
ExcludedTestFilter: filters assertions that are part of filtered or otherwise excluded tests (if the tests cannot be handled, assertion-level results cannot override this);
MissingValueFilter: filters assertions for which no tested method could be identified (without a tested method, we don't have specifications, so cannot perform generalization);
VoidReturnTypeFilter: filters assertions with a tested method that has a void return type (no return type -> no output specification -> no generalization);
UnsupportedAssertionFilter: filters unsupported assertions (we currently only support assertEquals, assertTrue, assertFalse, and (with some caveats?) asserThrows);
ParameterTypeFilter: filters assertions with a tested method that has no parameters of supported types (we can currently generalize parameters of types byte, short, int, long, float, double);

\subsubsection{Generalization-level Filtering}
\label{sec:generalization-filtering}

filterGeneralization:
NonPassingTestFilter: filters generalizations that fail during test execution (PIT requires green suite, can be either due to "incorrect" generalization or due to "bad" original tests);

\subsubsection{Other Limits / Safeguards}
\label{sec:other-filtering}

During SPF execution:
maximum PC size limit;
maximum depth limit;
maximum execution time;

During Test Transformation:
maximum Java specification size;

\subsection{Program Output / Collected Data}
\label{sec:collected-data}

primarily, we provide 1 property-based test for each original assertion (excluding ones that are filtered throughout the processing pipeline).
(do we need this section? or are outputs / data already explained well enough in the other section? even then, might still be useful to have a condensed overview here)

additionally, we provide (i) a PostgreSQL database with data about the processed projects and (ii) various intermediate results / log files.
(perhaps move this information to some "Data Availability" section.)

The database contains tables:
- project,
- test,
- assertion,
- generaliztion,
- filter\_result,
- (evosuite\_runtime),
- (evosuite\_report),
- junit\_test\_report,
- jacoco\_coverage\_report,
- pit\_coverage\_report,
- pit\_mutation\_report,
- task.

The intermediate results / log files contain:
- modified pom.xml / build.gradle files,
- command-data (all executed commands + output logs + error logs),
- jacoco-data (JaCoCo coverage CSV reports per project + variant),
- jpf-data (output logs, input-output values + specifications, driver + config + instrumented test files),
- junit-data (JUnit / Surefire XML reports per project + variant),
- pit-data (PIT linecoverage + mutations XML reports per project variant),

using PIT for mutation testing because;
most mutation testing tools
(i) do not provide results in a structured format
that's suitable for automated processing
and / or (ii) do not provide (official) support for Java 8,
and / or (iii) are not actively maintained anymore
(according to the PIT website:
\url{https://pitest.org/java_mutation_testing_systems/#summary-of-mutation-testing-systems})
using DEFAULTS set of mutators (see Table~\ref{tab:pit-mutators})
for further details about mutators, see the PIT website \footnote{\url{https://pitest.org/quickstart/mutators/}}

\begin{table}[H]
  \caption{Mutators in \ToolPit{}'s DEFAULTS group of mutators.}
  \label{tab:pit-mutators}
  \begin{tabular}{l l l l}
    \toprule
    &&\multicolumn{2}{l}{Example} \\
    \cmidrule{3-4}
    Mutator & Description  & Before & After\\
    \midrule
    Math                       & Replaces arithmetic operations            & \texttt{x + y}       & \texttt{x - y} \\
    Increments                 & Replaces increment/decrement              & \texttt{i++}         & \texttt{i{-}{-}} \\
    InvertNegs                 & Inverts negation of variables             & \texttt{return -x}   & \texttt{return x} \\
    \midrule
    BooleanTrueReturnVals      & Returns \texttt{true} for booleans        & \texttt{return b}    & \texttt{return true} \\
    BooleanFalseReturnVals     & Returns \texttt{false} for booleans       & \texttt{return b}    & \texttt{return false} \\
    PrimitiveReturns           & Returns \texttt{0} for numeric primitives & \texttt{return a}    & \texttt{return 0} \\
    EmptyObjectReturnVals      & Returns empty for strings                 & \texttt{return s}    & \texttt{return ""} \\
    NullReturnVals             & Returns \texttt{null} for objects         & \texttt{return o}    & \texttt{return null} \\
    \midrule
    RemoveConditionalEqualElse & Forces else for equality checks           & \texttt{if (a == b)} & \texttt{if (false)} \\
    RemoveConditionalOrderElse & Forces else for inequality checks         & \texttt{if (a < b)}  & \texttt{if (false)} \\
    ConditionalsBoundary       & Changes boundary of inequalities          & \texttt{if (a < b)}  & \texttt{if (a <= b)} \\
    \midrule
    VoidMethodCall             & Removes void method calls                 & \texttt{foo(...)}    & \texttt{/* removed */} \\
    \bottomrule
  \end{tabular}
\end{table}


% ------------------------------------------------------------------------------
% EVALUATION
% ------------------------------------------------------------------------------

\section{Evaluation}
\label{sec:evaluation}

To evaluate the primary effects, secondary effects, runtime requirements, and current
limitations of our generalization approach, we investigate the following research questions:

\begin{itemize}
  \item RQ1: To which degree does generalization affect the mutation score of the target test suites?
  \item RQ2: To which degree does generalization affect the size and runtime of the target test suites?
  \item RQ3: What are the runtime requirements of the generalization approach?
  \item RQ4: What are the causes of unsuccessful generalization attempts?
\end{itemize}

Throughout the remainder of this section, we first describe the target programs used for
the evaluation in Section~\ref{sec:target-programs}. In Section~\ref{sec:evaluation-setup},
we provide further information about the used evaluation setup, including the used hardware,
relevant pre-/processing steps, and used configuration settings of Teralizer.
Sections~\ref{sec:primary-effects-eval}--\ref{sec:filtering-eval-extended}
describe the results of the evaluation, answering RQ1--RQ4.


\subsection{Target Programs}
\label{sec:target-programs}

Evaluation of the test generalization approach requires a dataset consisting of projects for which
(i) the source code of the implementation and corresponding tests is available.
Additionally, as discussed throughout Section~\ref{sec:approach}, included projects
(ii) need to be processable by SPF (which does not support newer Java versions than Java 8),
(iii) need to use JUnit 4 or JUnit 5 as a testing framework,
(iv) need to use Maven or Gradle as a build tool,
(v) should use (partially) numeric inputs and numeric outputs for methods targeted by the generalization
(because Teralizer currently only supports generalization of numeric inputs and outputs,
as discussed in Section~\ref{sec:specification-extraction}).

While various publicly available benchmarks exist that fulfill requirements i--iv,
these benchmarks are often based on code from popular libraries such as Apache Commons, Google Gson, ... .
For example, ... (list + describe some benchmarks that support this claim).
Because these libraries need to be useable in different application scenarios,
and maintainable by a large number of simultaneous contributors,
they tend to make heavy use of encapsulation, inheritance, polymorphism, reflection, etc.
As a result, such projects are largely incompatible
with the basic specification extraction approach used by Teralizer
because most implementation code does not fulfill requirement (v).
Furthermore, due to their widespread use, active development communities, and long-running nature,
these libraries are often more thoroughly tested than other projects are.
Consequently, evaluating any test generalization approach on them
would see only little opportunity to improve the effectiveness of the test suite.

To achieve a thorough evaluation of our approach
that provides generalizable results for both its strengths as well as its weaknesses
in the presence of the requirements and considerations discussed above,
we built our own evaluation dataset using projects from three different sources:
the \DatasetEqBench{} Benchmark \cite{badihi_2021_eqbench},
utility classses of {TODO: number} Apache Commons projects, and
{TODO: number} open source projects from the RepoReapers dataset \cite{munaiah_2017_reporeapers}.
Further details about the included projects
are provided in the follwing Sections~\ref{sec:eqbench}--\ref{sec:additional-oss-projects}.
Descriptive statistics are available in Section~\ref{sec:dataset-statistics}.

\subsubsection{EqBench}
\label{sec:eqbench}

To represent projects that are well-suited for processing by \ToolTeralizer{},
we include the \DatasetEqBench{} benchmark as the first part of our evaluation dataset.
The \DatasetEqBench{} benchmark is a dataset built by \citeauthor{badihi_2021_eqbench}
to evaluate the effectiveness of equivalence checking tools \cite{badihi_2021_eqbench}.
The datasets contains Java (and C) implementations of non-equivalent program pairs.
Since many equivalence checking tools such as ... \cite{TODO} and ... \cite{TODO}
rely (partially) on symbolic execution for their analysis,
\DatasetEqBench{} is well suited for processing with \ToolSPFLong{}-based tools such as \ToolTeralizer{}.
The dataset achieves this by focusing primarily on programs that process numeric inputs,
and by avoiding use of language features such as recursion and reflection
which contemporary equivalence checking approaches only offer limited support for \cite{TODO}.
Because the dataset only contains implementation code, but does not contain test code,
we used \ToolEvoSuite{} to generate test suites
that can be used as input for the generalization.
We chose \ToolEvoSuite{} for test generation
because it consistently achieved top placements in recent test generation tool competitions \cite{TODO}.
For the test generation, we used three different timeout settings
for \ToolEvoSuite{} to approximate different strengths of test suites:
60 seconds per class (\ToolEvoSuite{}'s default), 10 seconds per class, and 1 second per class.
All other configuration settings were left at \ToolEvoSuite{}'s default values. We refer to
the corresponding projects consisting of \DatasetEqBench{} implementation code and
\ToolEvoSuite{} generated test code as \textit{\DatasetEqBenchA{}}, \textit{\DatasetEqBenchB{}}, and \textit{\DatasetEqBenchC{}}.
Descriptive statistics for these projects are provided in Section~\ref{sec:dataset-statistics}.

% tests generated with EvoSuite;
% came "third" in SBFT Tool Competition 2025 - Java Test Case Generation Track (https://arxiv.org/pdf/2504.09168), but differences between top 3 not statistically significant per the paper;
% can't find a summary paper for SBFT 2024;
% performed best on the SBFT Tool Competition 2023 - Java Test Case Generation Track for line and branch coverage metrics, and second-best for understandability metric;
% also the overall winner for SBST Tool Competition 2022 and SBST Tool Competition 2021 which use line + branch (via JaCoCo) and mutation coverage (via PIT) as metrics;

% 3 different test suites with search budgets: 1s, 10s, 60s (= EvoSuite default);
% no other changes to default EvoSuite configuration

\subsubsection{Apache Commons Utils}
\label{sec:apache-commons-utils}

To have some representation of large open source libraries in our evaluation dataset,
we extracted source code and corresponding test code of utility methods from \{number\} Apache Commons projects.
We identified appropriate utility methods via Sourcegraph's RegEx-based repository search\footnote{\url{https://sourcegraph.com/search}}.
More specifically, we searched for public static methods
with at least one numeric or boolean input parameter
and a numeric or boolean output value.
The exact search query we used is available in our replication package~\cite{replicationpackage}.
To construct the dataset from the Souregraph search results,
we manually created a new project and added to it:
(i) all matching utility methods,
(ii) all methods that are (transitively) called by the utility methods,
(iii) all tests that cover the utility methods, and
(iii) all dependencies that are needed to compile the included code.
Beyond the original test suite consisting of test classes extracted from Apache Commons projects,
we also generated three additional test suites via \ToolEvoSuite{}
using the same settings that we used for the \DatasetEqBench{} test generation.
Thus, four variants of the created project are included in the evaluation dataset:
\textit{\DatasetCommonsDev{}} (which uses the original developer written test suite)
as well as \textit{\DatasetCommonsA{}}, \textit{\DatasetCommonsB{}}, and \textit{\DatasetCommonsC{}}
(which use \ToolEvoSuite{} generated test suites).

\subsubsection{RepoReapers}
\label{sec:additional-oss-projects}

To get a more representative view
of current limitations of the approach in a real-world setting,
we additionally applied Teralizer to <number> projects
from the RepoReapers dataset\cite{munaiah_2017_reporeapers}.
@TODO: Write 1-2 sentences about the RepoReapears dataset.
To select these projects from the full RepoReapers dataset,
we applied the following selection criteria: projects
(i)~have to be implemented in Java 1.5 to 1.8
(ii)~have to use JUnit 4 or 5 for testing,
(iii)~have to use Maven as a build tool,
(iv)~have to follow the expected folder structure (i.e., have implementation in src/main/java and tests in src/test/java) 
(v)~have to contain 5000-50000 LOC,
(vi)~have to contain 20\%-80\% of the total source code in test classes
(vii)~have to have a total repository size less than 100MB.
Selection criteria (i) is imposed by the \ToolSPF{} dependency of \ToolTeralizer{}.
Criteria (ii) to (iv) are current limitations of the implemented protopye
(as discussed in Section~\ref{sec:approach}),
but could be lifted with additional engineering effort.
The remaining selection criteria (v) to (vii) were applied
to keep the overall size of the dataset to a manageable amount
while focusing on a subset that has
a good balance between implementation code and test code.
We refer to this dataset as \textit{\DatasetRepoReapers{}} throughout the rest of this paper.
Note, however, that \DatasetRepoReapers{} is only discussed in detail in RQ4,
which covers current limitations of the prototype
(see Section~\ref{sec:filtering-eval-extended}),
but not throughout RQ1--RQ3.
This is because the current implementation of \ToolTeralizer{}
is largely unsuccessful at generalizing tests from these projects.

\subsubsection{Dataset Statistics}
\label{sec:dataset-statistics}

Table~\ref{tab:dataset-statistics} provides descriptive statistics
of the projects that are included in the dataset.
We separately list the number of files, classes, and source lines of code (SLOC)
for both the implementation code as well as the test code of the projects.
For the \DatasetRepoReapers{} projects,
we list the total, mean, and median values
across all 1160 included (sub-)projects.

@TODO: Describe the \DatasetsEqBenchEs{} numbers.

@TODO: Describe the \DatasetsCommons{} numbers.

The developer-written tests in \DatasetCommonsDev{}
often have several times as many lines of code
as the automatically generated \ToolEvoSuite{} tests
in \DatasetCommonsA{}, \DatasetCommonsB{}, and \DatasetCommonsC{}.
This is because the developer-written tests often
test multiple sets of input values
and / or multiple target methods
in a single single test method.
The \ToolEvoSuite{} generated test suites, on the other hand,
use very isolated / focused test methods
that generally cover only a single target method
using a single set of input values.
For practical examples of these differences, see Figure~\ref{fig:@TODO}.

@TODO: add a figure showing test cases in \DatasetCommonsDev{} vs. \DatasetCommonsA{} / \DatasetCommonsB{} / \DatasetCommonsC{}.

@TODO: Describe the \DatasetRepoReapers{} numbers.

descriptive statistics (evosuite runtime, number of classes, number of tests, LOC, ...)

\begin{table}[H]
  \caption{Number of files, classes, source lines of code (SLOC), and test methods per project.}
  \label{tab:dataset-statistics}
  \begin{tabular}{lrrrrrrr}
    \toprule
    & \multicolumn{3}{c}{Implementation} & \multicolumn{4}{c}{Test} \\
    \cmidrule(lr){2-4} \cmidrule(lr){5-8}
    Project & Files & Classes & SLOC & Files & Classes & SLOC & Methods \\
    \midrule
    \DatasetEqBenchA{} & 544 & 652 & 27,871 & 544 & 544 & 35,666 & 4,718 \\
    \DatasetEqBenchB{} & 544 & 652 & 27,871 & 543 & 543 & 36,937 & 4,875 \\
    \DatasetEqBenchC{} & 544 & 652 & 27,871 & 544 & 544 & 37,836 & 4,974 \\
    \midrule
    \DatasetCommonsA{} & 106 & 247 & 19,709 & 103 & 103 & 17,524 & 2,481 \\
    \DatasetCommonsB{} & 106 & 247 & 19,709 & 103 & 103 & 19,082 & 2,738 \\
    \DatasetCommonsC{} & 106 & 247 & 19,709 & 102 & 102 & 18,839 & 2,735 \\
    \midrule
    \DatasetCommonsDev{} & 106 & 247 & 19,709 & 80 & 119 & 14,389 & 725 \\
    \midrule
    \DatasetRepoReapers{} (total) & 79,789 & 96,589 & 5,203,516 & 40,872 & 55,038 & 3,905,071 & 167,046 \\
    \DatasetRepoReapers{} (mean) & 68 & 83 & 4,478 & 35 & 47 & 3,360 & 143 \\
    \DatasetRepoReapers{} (median) & 51 & 57 & 3,241 & 22 & 26 & 2,053 & 69 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Evaluation Setup}
\label{sec:evaluation-setup}

To collect the necessary data to answer RQ1-RQ4,
we executed \ToolTeralizer{} once for every project in the evaluation dataset
on a MacBook Air with M2 processor and 24~GB of memory
(for threats to validity resulting from this setup, see Section~\ref{sec:threats-to-validity}).
The JVM settings for \texttt{InitialHeapSize} and \texttt{MaxHeapSize}
were kept at their default values, i.e., 384~MB (${1/64}$th of memory) for \texttt{InitialialHeapSize} and 6~GB (${1/4}$th of memory) for \texttt{MaxHeapSize}.
Further details about the used execution environments and settings are provided
in the \textit{Setup} sections of the the individual research questions.
Since there are no existing approaches
that automatically create property-based tests from existing (unit) tests,
no other tools are included in the evaluation.
Instead, we compare the original,
human written and \ToolEvoSuite{} generated test suites in the evaluation dataset
to the augmented / generalized test suites created by \ToolTeralizer{}.
As described in Section~\ref{sec:approach},
\ToolTeralizer{} creates a total of nine variants
of each target project / test suite
throughout its execution
to analyze how different settings or combinations thereof
affect the generalization results.
We refer to these variants as:

\begin{itemize}
  \item \VariantOriginal{}:
    The original project without any modifications applied by \ToolTeralizer{}.
  \item \VariantInitial{}:
    The state of the project
    after project instrumentation, test analysis, and specification extraction have concluded 
    (see Sections~\ref{sec:project-instrumentation}--\ref{sec:specification-extraction})
    but before any tests have been generalized.
    Tests for which these steps were not successful
    are excluded from this project variant and,
    therefore, are not included in any further generalization steps.
    An overview of excluded tests is provided in Section~\ref{sec:filtering-eval}
    when discussing the results of RQ4.
  \item \VariantBaseline{}:
    The state of the project
    after \VariantBaseline{} generalization has been applied
    (see Section~\ref{sec:baseline-generalization}).
  \item \VariantNaiveA{}, \VariantNaiveB{}, \VariantNaiveC{}:
    The state of the project
    after \VariantNaive{} generalization has been applied.
    As described in Section~\ref{sec:naive-generalization},
    the subscript values indicate the values used for \ToolJqwik{}'s \tries{} setting.
  \item \VariantImprovedA{}, \VariantImprovedB{}, \VariantImprovedC{}:
    The state of the project
    after \VariantImproved{} generalization has been applied
    (see Section~\ref{sec:improved-generalization}).
    The subscript values indicate the values used for \ToolJqwik{}'s \tries{} setting.
\end{itemize}

In addition to the source code of the two pre-generalization variants
(\VariantOriginal{} and \VariantInitial)
as well as the seven post-generalization variants
(\VariantBaseline{},
\VariantNaive{} with 10 / 50 / 200 \tries{},
and \VariantImproved{} with 10 / 50 / 200 \tries{}),
\ToolTeralizer{} also creates and stores (meta) data and (intermediate) processing results
at various stages throughout its execution
in a PostgreSQL database and in log files.
This data includes, for example,
(i) descriptions of identified tests and assertions
(e.g., names, involved data types, lines of code),
(ii) information about executed processing tasks
(e.g., processing status, causes of failures, execution time),
and (iii) raw tool outputs
(e.g., console output logs, JUnit / \ToolJacoco{} / \ToolPit{} reports , extracted input-/output-specifications).
For a more detailed overview of the collected data, see Section~\ref{sec:collected-data}.
The evaluation results presented in the following sections
are generated from the collected data via Jupyter notebooks.
To aid independent validation and replication of our results,
all of the collected data, the full Java implementation of \ToolTeralizer{},
and the Jupyter notebooks used for the evaluation
are publicly available in our replication package~\cite{replicationpackage}.

\subsection{RQ1: Effects on Mutation Score}
\label{sec:primary-effects-eval}

In the following, we first describe
how many tests, implementation classes, and corresponding mutants
from the evaluation dataset
are included in the evaluation
in Section~\ref{sec:included-mutants}.
Section~\ref{sec:overall-detection-rates}
focuses on the overall detection rates
achieved by the implemented generalization variants.
In Section~\ref{sec:detection-rates-per-mutator},
we provide the detection rate results for each individual mutator.
Section~\ref{sec:boundary-detection-effectiveness}
concludes the results for RQ1
by investigating the effectiveness of the partition boundary detection
used by \VariantImproved{} variants to guide input value selection.

\subsubsection{Included Mutants}
\label{sec:included-mutants}
To evaluate the effects that generalization via \ToolTeralizer{} has on mutation scores,
we compare the mutation scores reported by \ToolPit{}
for the \VariantInitial{} variants of the the target projects
to the mutation scores reported for
the \VariantNaive{} and \VariantImproved{} variants
using \ToolPit{}'s \texttt{DEFAULTS} group of mutation operators.

The first three columns of Table~\ref{tab:mutants-per-project} show
how many of the test methods and implementation classes
of the \VariantOriginal{} projects are included in the \VariantInitial{} project variants.
Tests are counted as included
if they successfully pass test analysis (see Section~\ref{sec:test-analysis}) 
and specification extraction (see Section~\ref{sec:specification-extraction}).
Implementation classes are counted as included
if at least one line of the corresponding class
is covered by an included test
according to the \ToolJacoco{} coverage reports
of the \VariantInitial{} project variant.
For the projects with \ToolEvoSuite{} generated test suites
(i.e., \DatasetsEqBenchEs{} and \DatasetsCommonsEs{}), 
83--85 \% of test methods from \VariantOriginal{} are included in \VariantInitial{}.
For \DatasetCommonsDev{}, 63.6 \% are included.
Exclusions are primarily caused by 
(i) tests without assertions (... \%),
(ii) tests for which no tested method can be identified by \ToolTeralizer{},
and (iii) tests that exercise a tested method that has no generalizable input parameters.
Further details about the causes of test exclusions
are provided when discussing RQ4 in Section~\ref{sec:filtering-eval}.

The last three columns for Table~\ref{tab:mutants-per-project} show the number of total, covered, and uncovered mutants per project.
Total mutants are all mutants that are created by \ToolPit{} in the included classes.
Covered mutants are the subset of total mutants that are executed by at least one included test.
Uncovered mutants are the subset of total mutants that are not executed by any included tests.
@TODO: Describe actual values in the table.
@TODO: Reference the table with the per-mutant prevalence data.
%Around 8000 mutants for the apache-commons-utils projects. Around 24000 mutants for the eqbench projects.
%Total mutants for the ORIGINAL variant (i.e., before SPF execution) are around 5\% higher (not shown in this table). For filtering reasons, see Section~\ref{sec:filtering-eval}.

\begin{table}[H]
  \caption{Number of total, covered, and uncovered mutants in included classes per project.}
  \label{tab:mutants-per-project}
  \begin{tabular}{lrrrrr}
    \toprule
            & Included     & Included      & \multicolumn{3}{r}{Mutants} \\
                                             \cmidrule(lr){4-6}
    Project & Test Methods & Impl. Classes & Total & Covered & Uncovered \\
    \midrule
    \DatasetEqBenchA{} & 3937\; (83.4 \%) & 607\; (93.1 \%) & 23905 & 21492\; (89.9 \%) & 2413\; (10.1 \%) \\
    \DatasetEqBenchB{} & 4049\; (83.1 \%) & 600\; (92.0 \%) & 23654 & 21657\; (91.6 \%) & 1997\; (\phantom{0}8.4 \%) \\
    \DatasetEqBenchC{} & 4124\; (82.9 \%) & 603\; (92.5 \%) & 23663 & 22010\; (93.0 \%) & 1653\; (\phantom{0}7.0 \%) \\
    \midrule
    \DatasetCommonsA{} & 2079\; (83.8 \%) & 111\; (44.9 \%) & 8581 & 7536\; (87.8 \%) & 1045\; (12.2 \%) \\
    \DatasetCommonsB{} & 2330\; (85.1 \%) & 112\; (45.3 \%) & 8391 & 7939\; (94.6 \%) & 452\; (\phantom{0}5.4 \%) \\
    \DatasetCommonsC{} & 2326\; (85.0 \%) & 112\; (45.3 \%) & 8354 & 8109\; (97.1 \%) & 245\; (\phantom{0}2.9 \%) \\
    \midrule
    \DatasetCommonsDev{} & 461\; (63.6 \%) & 90\; (36.4 \%) & 8096 & 5215\; (64.4 \%) & 2881\; (35.6 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_mutation_detection_comparison}
  \caption{Percentage of detected mutants and improvement over INITIAL per project and generalization variant.}
  %\Description{@TODO}
  \label{fig:mutation-detection-results}
\end{figure}

% Overall observations:
\subsubsection{Overall Mutation Detection Rates}
\label{sec:overall-detection-rates}
Figure~\ref{fig:mutation-detection-results} shows
the percentage of detected mutants
and improvement over \VariantInitial{}
per project and generalization variant.
Detected mutants includes
killed (X\% of overall detections), timed-out (A\%), memory error (B\%), and run error (C\%),
which matches the classification used by \ToolPit{}.
Since test generalization via \ToolTeralizer{} does not affect coverage
(as described in Sections~\ref{sec:naive-generalization} and \ref{sec:improved-generalization}),
all results are relative to covered mutants
i.e., uncovered mutants are not considered in the comparison
because \ToolTeralizer{} --- by design --- cannot achieve any coverage improvements
but also does not decrease coverage.

We are using \VariantInitial{} as the baseline for mutation score comparisons
with generalized project variants,
which ensures that the only difference
across compared projects is the absence or presence of generalized test cases.
\VariantOriginal{} is not used for comparison
because it contains additional non-generalized tests
that are excluded in \VariantInitial{} as well as generalized project variants.
\VariantBaseline{} is not included in the comparison
because it always uses the same test inputs as \VariantInitial{}
(as described in Section~\ref{sec:baseline-generalization}).
Therefore, \VariantInitial{} and \VariantBaseline{} always achieve the same detection rates.

\paragraph{\VariantNaive{} Detection Rates}

\VariantNaive{} generalization improves detection rates
for \emph{all} projects included in the evaluation dataset.
Improvements are largest for the \DatasetsEqBenchEs{} projects.
Here, detection rates increase from 48.10--51.64 \% to 50.67--54.98 \%
across the different project and generalization variants.
Thus, \VariantNaive{} detects 2.33--3.93 \% more of the total mutants
than the corresponding \VariantInitial{} test suite,
which is a relative increase of 4.51--8.17 \%.
As a result, generating test suites with \ToolEvoSuite{}
and then generalizing them with \ToolTeralizer{}
often achieves higher detection rates than
increasing \ToolEvoSuite{}'s test generation timeout
from 1 second per class to 10 s or even 60 s
(for total runtime costs of test generation and generalization,
see Section~\ref{sec:runtime-eval}).
Furthermore, higher \tries{} settings generally achieve
better detection rates than lower \tries{} settings.
However, there seem to be diminishing returns:
increasing \tries{} from 10 to 50
achieves a larger improvement in detection rates
than increasing \tries{} from 50 to 200.

Results for \DatasetsCommonsEs{} projects follow similar trends,
albeit with overall smaller improvements
of 0.82--1.23 percentage points
(i.e., a relative increases of 1.43--2.17 \%)
compared to the \VariantInitial{} test suites.
For the \DatasetCommonsDev{} project,
improvements are even smaller,
increasing detection rates
by only 0.05--0.07 percentage points.
However, overall detection rates for \DatasetCommonsDev{}
are much higher than for the other projects
with an \VariantInitial{} detection rate of 80.35 \%.
Therefore, it is expected
that achievable improvements will be correspondingly smaller.
Deviations from the described trends
are primarily due to random variance
in \ToolEvoSuite{}'s test generation
and, to a lesser degree,
due to occasional random errors
(e.g., \texttt{OutOfMemoryError})
during test execution
which cause corresponding tests to be excluded from further processing
(see Sections~\ref{sec:filtering-eval} and \ref{sec:filtering-eval-extended}).

\paragraph{\VariantImproved{} Detection Rates}

The \VariantImprovedC{} generalization variant
achieves similar results as \VariantNaiveC{}
for all seven evaluated projects.
More specifically, it reaches relative improvements
of 6.04--7.82 \% for the \DatasetsEqBenchEs{} projects,
2.02--2.29 \% for \DatasetsCommonsEs{},
and 0.06 \% for \DatasetCommonsDev{}.
\VariantImprovedA{} and \VariantImprovedB{}
consistently achieve larger improvements
than the corresponding \VariantNaive{} variants
for the \DatasetsCommonsEs{} projects
(1.80--2.19 \% vs.\ 1.43--1.83 \%),
but smaller improvements
for the \DatasetsEqBenchEs{} projects
(2.36--6.88 \% vs. 4.51--8.17 \%).
However, these differences are not uniformly distributed
across different mutation operators.
As shown in Table~\ref{tab:detections-per-mutator}
and discussed in more detail in Section~\ref{sec:detection-rates-per-mutator},
\VariantNaive{} variants only outperform \VariantImproved{} ones
for the detection of \texttt{Math} mutants,
whereas all other mutants are either tied in terms of detection rates
or favor \VariantImproved{} variants.
Furthermore, \VariantImprovedA{} performs noticeably worse
than all other project-variant combinations
on the \DatasetsEqBenchEs{} projects.
This is likely because \VariantImproved{} variants
very effectively identify input partition boundaries
in the \DatasetsEqBenchEs{} projects.
As a result, most or even all \tries{} of the \VariantImprovedA{} variant
are spent on boundary testing, leaving little opportunity to detect mutants
introduced via non-boundary mutations.
For a more in-depth discussion of this, see Section~\ref{sec:boundary-detection-effectiveness}.

\subsubsection{Mutation Detection Rates per Mutator}
\label{sec:detection-rates-per-mutator}
To evaluate how capable generalized tests are
at detecting mutants created by different mutation operators,
we compare the detection rates
of \VariantNaiveC{} and \VariantImprovedC{}
to the detection rates achieved by the \VariantInitial{} test suite.
Table \ref{tab:detections-per-mutator}
shows the results of this evaluation,
listing for each mutant:
the total number of occurrences
across the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects;
the mean, minimum, and maximum prevalence per project
relative to the total number of mutants;
the detection rates of the \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} test suites,
as well as how much the two generalized variants improve detection rates relative to \VariantInitial{}.
Results for \VariantNaive{} and \VariantImproved{} generalizations with lower \tries{}
follow the same trends as for \VariantNaiveC{} and \VariantImprovedC{} but are omitted for brevity.
The full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Prevalence of Mutants}
The three most common mutants,
which represent~81.08 \% of total mutants, are
\texttt{Math}~(59.10~\% of mutants),
\texttt{Remove\-Conditional\-OrderElse}~(10.99~\%),
and \texttt{Conditionals\-Boundary}~(10.99~\%) mutants
(prevalence of \texttt{Remove\-Conditional\-Order\-Else}
and \texttt{Conditionals\-Boundary}
is the same because both mutators modify inequality checks).
On the other hand, the three least common mutants are
\texttt{Increments}~(0.52~\%),
\texttt{Boolean\-False\-Return\-Vals}~(0.24~\%),
and \texttt{Empty\-Object\-Return\-Vals}~(0.13~\%) mutants.
Due to these large differences in mutant prevalences,
overall mutation scores are much more strongly affected
by changes in, e.g., \texttt{Math} detection rates
than by changes in, e.g., \texttt{Increments} detection rates.
This holds not only on an overall, aggregate level
(i.e., across all evaluated projects),
but also for each individual project.
After all, prevalence of individual mutants
shows only comparatively small differences across projects,
as highlighted by the minimum and maximum prevalence results.
In other words,
mutants that are common in one project
are also common in the others,
whereas mutants that are uncommon in one project
are also uncommon in the others.
For example, \texttt{Math} mutants represent 52.34~\% of mutants
in the project where they are least common,
but \texttt{Increments} mutants
represent only 0.54~\% of mutants
in the project where they are most common.

\begin{table}[H]
  \caption{Number of mutants and percentage of detections per mutator.}
  \label{tab:detections-per-mutator}
  \begin{tabular}{lrrrrcrrrr}
    \toprule
    & & & & & \multicolumn{5}{c}{Detected \%} \\
    \cmidrule{6-10}
    Mutator & Total & Total \% & Min \% & Max \% & INITIAL & \multicolumn{2}{c}{NAIVE$_{200}$} & \multicolumn{2}{c}{IMPROVED$_{200}$} \\
    \midrule
    Math & 61841 & 59.10 & 52.34 & 62.16 & 50.99 & 54.98 & (+3.99) & 54.36 & (+3.37) \\
    RemoveConditionalOrderElse & 11501 & 10.99 & 8.50 & 11.94 & 61.08 & 62.29 & (+1.21) & 62.47 & (+1.39) \\
    ConditionalsBoundary & 11501 & 10.99 & 8.50 & 11.94 & 27.68 & 28.89 & (+1.21) & 30.23 & (+2.55) \\
    PrimitiveReturns & 7731 & 7.39 & 6.15 & 10.09 & 89.42 & 89.63 & (+0.20) & 89.90 & (+0.47) \\
    RemoveConditionalEqualElse & 5536 & 5.29 & 3.21 & 10.40 & 58.80 & 60.87 & (+2.07) & 61.00 & (+2.20) \\
    InvertNegs & 3122 & 2.98 & 2.94 & 3.12 & 58.91 & 60.61 & (+1.70) & 60.99 & (+2.08) \\
    VoidMethodCall & 973 & 0.93 & 0.58 & 1.35 & 24.96 & 24.96 & -- & 25.49 & (+0.53) \\
    NullReturnVals & 933 & 0.89 & 2.13 & 3.38 & 98.77 & 98.77 & -- & 98.77 & -- \\
    BooleanTrueReturnVals & 569 & 0.54 & 0.17 & 1.44 & 98.55 & 98.55 & -- & 98.55 & -- \\
    Increments & 546 & 0.52 & 0.50 & 0.54 & 72.81 & 73.38 & (+0.57) & 73.50 & (+0.69) \\
    BooleanFalseReturnVals & 250 & 0.24 & 0.09 & 0.63 & 87.87 & 87.87 & -- & 87.87 & -- \\
    EmptyObjectReturnVals & 141 & 0.13 & 0.41 & 0.43 & 90.30 & 90.30 & -- & 90.30 & -- \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Detection Rate Results}
Detection rates for \VariantInitial{}, \VariantNaiveC{}, and \VariantImprovedC{} as well as
detection rate improvements compared to \VariantInitial{}
are shown in the last three columns of the table.
Detection rates exhibit large differences across different mutators.
The highest detection rates of 90\%--99\% are achieved
for mutators that directly modify return values.
The lowest detection rates are observed
for the \texttt{VoidMethodCall} mutator (... \% detection rate), 
which removes calls to methods that do not produce a return value,
and the \texttt{ConditionalsBoundary} mutator (... \% detection rate),
which modifies the boundaries of conditionals by replacing
\texttt{<} with \texttt{<=} and \texttt{>} with \texttt{>=} (or vice-versa).

\VariantNaive{} achieves the largest improvements over \VariantInitial{}
for the ... (...), ... (...), and ... (...) mutations.
However, \VariantNaive{} does not improve detection rates
for mutants created by the ... mutators.
(@TODO: Possible explanations: (i) high initial detection rates, (ii) new assertions needed.)
\VariantImproved{} achieves the largest improvements
for the ... (...), ... (...), and ... (...) mutations.
It does not achieve any improvements
for mutants created by the ... mutators.

Results for the 10 / 50 \tries{} variants of \VariantNaive{} and \VariantImproved{}
generally follow the same trends as those for the listed variants with 200 \tries{},
albeit with correspondingly smaller detection rate improvements over \VariantInitial{}.
(@TODO: Check the results for \VariantImprovedA{}. Those might be much worse for mutants that don't relate to conditionals.)
Full results for all variants are available in our replication package~\cite{replicationpackage}.

\paragraph{Comparison of \VariantNaiveC{} and \VariantImprovedC{}}
\VariantImprovedC{} outperforms \VariantNaiveC{} in terms of detection rate
for 7 of 12 mutators,
is on par for 4 of 12 mutators (all of which achieve no improvement for either variant),
and underperforms for 1 of 12 mutators.
The largest benefit of \VariantImprovedC{} over \VariantNaiveC{} is observed
for the detection of mutants created by the \texttt{ConditionalsBound} mutator.
Here, \VariantImprovedC{} achieves an increase of ... percentage points over \VariantInitial{},
whereas \VariantNaiveC{} only increases detection rates by ... percentage points.
This suggests that the improvements implemented in \VariantImproved{} variants
to favor boundary values during input generation achieve their intended benefits.

However, \VariantImprovedC{}'s benefits for the detection rate of \texttt{ConditionalsBoundary} mutants
appear to come at the cost of decreased detection rates for \texttt{Math} mutants.
Since the only difference between the two variants is the way in which test inputs are selected
--- \VariantNaive{} variants select inputs randomly from the given input partition; 
\VariantImproved{} variants start with inputs at the edges of the partition  ---
this suggests that focusing too much on boundary values
can have detrimental effects for the detection of mutants
that are not related to input boundaries.
Due to the high prevalence of \texttt{Math} mutants,
this can also result in an \emph{overall} decrease in detection rates.

To counteract the detrimental effects on
detection rates of \texttt{Math} mutants
incurred by the input selection procedure used by \VariantImproved{} variants,
the number of \tries{} could, for example,
be increased for the \VariantImproved{} variants
to adjust for the additional \tries{} spent on boundary testing
while keeping non-boundary testing at \VariantNaive{} levels.
More sophisticated input selection approaches
that aim to better balance boundary and non-boundary testing
without large increases to the number of executed \tries{}
could perhaps further increase detection rates
while keeping the runtime impact
resulting from a higher number of \tries{} to a minimum.
For a more in-depth evaluation of runtime requirements
across different variants and numbers of \tries{}
see Section~\ref{sec:runtime-eval}.

\subsubsection{Effectiveness of \VariantImproved{} Boundary Detection}
\label{sec:boundary-detection-effectiveness}

@TODO: Add intro. Basically: Offer more thorough explanations
why \VariantImproved{} (sometimes?) performs better than \VariantNaive{}
on the \DatasetsEqBenchEs{} projects, but not on the \DatasetsCommonsEs{} ones.

\paragraph{Evaluation Metrics}
Table~\ref{tab:mutation-detection-comparison} shows 
the model properties of mutants that are (not) detected
by \ToolTeralizer{}'s \VariantImprovedC{} generalization variant.
The term \emph{model}, in this case, refers to an 
input specification extracted by \ToolTeralizer{}.
Therefore, the (input) model(s) of a mutant
are all input specifications
that describe an input partition
which contains at least one set of inputs
that cause the mutant to be exercised during test execution.
In other words, the model(s) of a mutant
are all input specifications
that were extracted by \ToolTeralizer{}'s
specification extraction step (see Section~\ref{sec:specification-extraction})
during execution of a test that covers the corresponding mutant.

Included model properties are
the median number of model operations
and the median number of model constraints.
%The number of (nested) operations is given by
%the total number of operators in the model.
%The number of constraints is determined
%by counting the number of boolean conditions or variables
%connected by \texttt{\&\&} operators.
%There can never be \texttt{||} operators in a model
%because the two operands on both sides of the \texttt{||}
%represent different input partitions and, therefore,
%describe different models.
%
In addition to the model properties,
the last column of Table~\ref{tab:mutation-detection-comparison}
lists the mean and median percentage of constraints
that are used by the created property-based tests
when generating inputs for test execution.
As described in more detail in Section~\ref{sec:improved-generalization}, 
the current implementation of \ToolTeralizer{}'s \VariantImproved{} variants
only considers simple in-/equalities on numeric and boolean variables or constants.
More complex constraints are not encoded
in the initial input value generation of the \VariantImproved{} property-based tests
but are still enforced during \ToolPit{}'s default value filtering step.

For example, take the following (Java represented) model:
\texttt{(((a < 0) \&\& (a == (b + 1))) \&\& c)}.
This model contains 5 operators
(i.e., \texttt{<}, \texttt{\&\&}, \texttt{==}, \texttt{+}, and \texttt{\&\&}),
so has a total of 5 (nested) operations.
Furthermore, the model's contains 3 constraints:
(i) \texttt{a < 0}, (ii) \texttt{a == (b + 1)}, and (iii) \texttt{c}.
Constraints (i) and (iii) are used by the \VariantImproved{} variants.
Constraint (ii) is not used
because it contains the compound term \texttt{b + 1}.

\begin{table}[H]
  \caption{Model properties of mutants that are (not) detected by the \VariantImprovedC{} variant.}
  \label{tab:mutation-detection-comparison}
  \begin{tabular}{lcrrrrrrr}
    \toprule
            &          &         & \multicolumn{2}{c}{Operations} & \multicolumn{2}{c}{Constraints} & \multicolumn{2}{r}{Constraints Used} \\
    \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    Project & Detected & Mutants & Mean & Median & Mean & Median & Mean & Median \\
    \midrule
    \DatasetEqBenchA{} & yes & 11145 & 147 & \phantom{0}9 & \phantom{0}6 & 2 & \phantom{0}47 \% & \phantom{0}80 \% \\
    \DatasetEqBenchA{} & no & 10347 & 224 & 16 & 11 & 5 & \phantom{0}23 \% & \phantom{0}50 \% \\
    \midrule
    \DatasetEqBenchB{} & yes & 11658 & 139 & \phantom{0}9 & \phantom{0}6 & 2 & \phantom{0}62 \% & 100 \% \\
    \DatasetEqBenchB{} & no & 9999 & 231 & 15 & \phantom{0}8 & 2 & \phantom{0}57 \% & 100 \% \\
    \midrule
    \DatasetEqBenchC{} & yes & 12052 & 137 & \phantom{0}9 & \phantom{0}5 & 2 & \phantom{0}69 \% & 100 \% \\
    \DatasetEqBenchC{} & no & 9958 & 218 & 11 & \phantom{0}6 & 2 & \phantom{0}67 \% & 100 \% \\
    \midrule
    \DatasetCommonsA{} & yes & 4390 & 290 & 15 & \phantom{0}7 & 5 & \phantom{0}43 \% & \phantom{0}84 \% \\
    \DatasetCommonsA{} & no & 3183 & 389 & 45 & 12 & 6 & \phantom{0}11 \% & \phantom{0}50 \% \\
    \midrule
    \DatasetCommonsB{} & yes & 4660 & 467 & 23 & \phantom{0}6 & 5 & \phantom{0}46 \% & \phantom{0}85 \% \\
    \DatasetCommonsB{} & no & 3309 & 507 & 46 & \phantom{0}8 & 6 & \phantom{0}10 \% & \phantom{0}56 \% \\
    \midrule
    \DatasetCommonsC{} & yes & 4821 & 374 & 20 & \phantom{0}6 & 5 & \phantom{0}47 \% & \phantom{0}85 \% \\
    \DatasetCommonsC{} & no & 3288 & 423 & 41 & 10 & 6 & \phantom{0}11 \% & \phantom{0}54 \% \\
    \midrule
    \DatasetCommonsDev{} & yes & 4193 & 107 & 11 & \phantom{0}4 & 4 & \phantom{0}25 \% & \phantom{0}75 \% \\
    \DatasetCommonsDev{} & no & 1022 & 173 & 10 & \phantom{0}4 & 4 & \phantom{0}19 \% & \phantom{0}75 \% \\
    \bottomrule
  \end{tabular}
\end{table}

\paragraph{Model Properties of Detected vs.\ Undetected Mutants}
As shown in Table~\ref{tab:mutation-detection-comparison},
undetected mutants often have more complex models
%(i.e., are easier to reach during testing)
than detected ones.
For example, in the \DatasetsEqBenchEs{} projects,
mean and median operation counts
are around 1.2--1.7 times larger
for undetected mutants than for detected ones
(e.g., mean of 231 vs.\ 139 and median of 15 vs.\ 9 for \DatasetEqBenchB{}).
Operation counts in the \DatasetsCommonsEs{} projects
show even more pronounced differences,
with values of undetected mutants being 1.3--3 times larger
than those of detected ones
(e.g., mean of 45 vs.\ 15 and median of 12 vs.\ 7 for \DatasetCommonsB{}).
The only project where models of undetected mutants
have lower median operation counts than detected ones (10 vs.\ 11)
is \DatasetCommonsDev{} with its developer written test suite.
However, mean operation counts are still larger for undetected mutants
at 173 vs.\ 107 for detected ones.
Constraint counts show a similar trend to operation counts,
with mean values being 1.0--1.7 times as large
and median values being 1.0--2.5 times as large
for undetected mutants as for detected ones.
Values for \DatasetCommonsDev{} are very different from
the other \DatasetsCommonsEs{} projects because it 
covers a much smaller subset of mutants,
as previously discussed in \ref{sec:included-mutants}.

\paragraph{Constraints Used by \VariantImproved{} Generalization}
The mean (median) number of used constraints across all projects
ranges from 25--69 \% (75--100 \%) for detected mutants
and 10--67 \% (50--100 \%) for undetected ones,
with usage rates being consistently higher
for detected mutants than for undetected ones.
Because the current implementation of \ToolTeralizer{} focuses on simple constraints,
this provides further evidence
that models of undetected mutants are not only larger than those of detected ones
(i.e., contain more operations and constraints)
but also more complex
(e.g., contain more compound terms,
show more common use of non-numeric data types,
and use more math functions that are not fully modeled by SPF).
This suggests that improving \ToolTeralizer{}'s support
for more complex constraints
holds further potential for additional increases in mutation detection rates.

Furthermore, the high constraint usage rates
as well as the comparatively small operation and constraint counts
of the \DatasetsEqBenchEs{} projects suggest that it is relatively easy
to find input values that match the models.
This could explain, at least in part,
why \VariantImproved{} generalization variants are more effective
relative to the corresponding \VariantNaive{} variants
when used in the \DatasetsCommons{} projects
than in the \DatasetsEqBenchEs{} projects
(as seen in Figure~\ref{fig:mutation-detection-results}).
After all, if mutated input partition boundaries are easy to identify,
there is a higher likelihood that they will be covered
even without the more sophisticated value selection
employed by \VariantImproved{} variants.
Thus, taking into account the complexity of the underlying models
during generalization might offer further opportunities
to improve the effectiveness and/or efficiency
of test generalization approaches such as \ToolTeralizer{}.

\subsection{RQ2: Effects on Test Suite Size and Runtime}
\label{sec:ancillary-effects-eval}

In this section, we describe the secondary effects caused by the generalization
in terms of changes in test suite size and test suite runtime.
To describe these changes, we employ the following four metrics: 
\begin{enumerate}
  \item the number of tests in the test suite (Section~\ref{sec:test-suite-test-count}),
  \item the number of lines of code in the test suite (Section~\ref{sec:test-suite-line-count}),
  \item the execution time of the test suite (Section~\ref{sec:test-suite-execution-time}).
\end{enumerate}

We focus primarily on the results
of the \VariantNaiveC{} and \VariantImprovedC{} generalization variants.
The results for the remaining variants follow similar trends
but are omitted for brevity.
The full results for all generalization variants
are available in our replication package~\cite{replicationpackage}.

\subsubsection{Number of Tests in the Test Suite}
\label{sec:test-suite-test-count}

Table~\ref{tab:tests-per-project} shows how many tests are added
by the \VariantNaiveC{} and \VariantImprovedC{} generalization variants,
how many can be removed after generalization,
and how this changes the total number of tests
relative to the \VariantOriginal{} test suites
across the \DatasetsEqBenchEs{} and \DatasetsCommons{} projects.
The number of added tests is between 174--211
(3.5--4.3~\% of \VariantOriginal{} test suite size)
for the \DatasetsEqBenchEs{} projects,
between 60--75 (2.2--2.9\%)
for the \DatasetsCommonsEs{} projects,
and 3 (0.4~\%) for the \DatasetCommonsDev{} project.
\VariantImprovedC{} generally adds a larger number of tests than \VariantNaiveC{} 
because more of the tests that are created  by \VariantNaiveC{}
are excluded due to \texttt{Too\-Many\-Filter\-Misses\-Exception}s
(as described in Section~\ref{sec:naive-generalization}
and evaluated in Section~\ref{sec:filtering-eval}).
Furthermore, the number of added tests is much smaller
than the total number of generalization that are created and evaluated by \ToolTeralizer{}
because only tests that measurably increase the mutation score of the test suite are retained.

Added tests are largely compensated by removed tests
in the \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects
which use \ToolEvoSuite{} generated test suites.
As a result, total test suite size only increases by 0--1 test cases
(0--0.04\% of \VariantOriginal{} test suite size)
for these projects.
For the \DatasetCommonsDev{} project,
none of the tests for which generalizations are added can be removed.
This is because an \VariantOriginal{} test can only be removed
if generalized tests are successfully created for all assertions in the test
(as described in Section~\ref{sec:baseline-generalization}).
In the projects with \ToolEvoSuite{} generated tests,
this requirement is generally satisfied because most tests only contain a single assertion.
However, this requirement is much more difficult to satisfy for \DatasetCommonsDev{} 
because the developer written tests often contain multiple assertions.
More generally, increases in the number of tests are
expected to be smaller for projects with more focused tests
that each contain only a small number of assertions
than for less focused ones with a large number of assertions per test.

In future work, we also plan to explore opportunities
for generalization to decrease overall test suite size.
This can be achieved
by replacing multiple \VariantOriginal{} tests
that all cover the same input partition
with a single property-based test that covers the same partition instead.
For further discussion and related work about this, see Section~\ref{sec:test-suite-reduction}.

\begin{table}[H]
\caption{Number of tests before and after generalization, with changes, per project.}
\label{tab:tests-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Tests} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 4718 & 177 & 177 & 4718 & -- & -- \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 4875 & 174 & 173 & 4876 & +1 & +0.02 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 4974 & 174 & 174 & 4974 & -- & -- \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 4718 & 206 & 206 & 4718 & -- & -- \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 4875 & 211 & 210 & 4876 & +1 & +0.02 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 4974 & 210 & 210 & 4974 & -- & -- \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 2481 & 60 & 59 & 2482 & +1 & +0.04 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 2738 & 63 & 62 & 2739 & +1 & +0.04 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 2735 & 60 & 59 & 2736 & +1 & +0.04 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 2481 & 69 & 68 & 2482 & +1 & +0.04 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 2738 & 70 & 69 & 2739 & +1 & +0.04 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 2735 & 75 & 74 & 2736 & +1 & +0.04 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 725 & 3 & 0 & 728 & +3 & +0.41 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 725 & 3 & 0 & 728 & +3 & +0.41 \% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Lines of Code in the Test Suite}
\label{sec:test-suite-line-count}

While generalization has only minor effects
on the number of tests in the evaluated test suites,
the lines of code in the test suites are more strongly affected.
As shown in Table~\ref{tab:lines-per-project},
overall lines of code (LOC) in the test suites increase
by 31.5--58.7~\% for the \DatasetsEqBenchEs{} projects,
by 18.8--29.9~\% for the \DatasetsEqBenchEs{} projects,
and by 4.9--5.3~\% for the \DatasetCommonsDev{} project.

Increases in test suite size are strongly correlated with
the number of tests added to the test suite (see Table~\ref{tab:tests-per-project}),
even if added tests are compensated for by test removals.
For example, \VariantNaive{} adds 177 tests to \DatasetEqBenchA{}
and allows the same number of \VariantOriginal{} tests to be removed.
Despite this, 11780 LOC are added
but only 1127 LOC are removed
(as shown in Table~\ref{tab:lines-per-project}).
This is expected for two reasons:
(i) generalized test classes
contain all of the code that is needed
to set valid input ranges for \ToolPit{}'s input value generators
(as described in Section~\ref{sec:test-generalization}), and
(ii) when creating the classes
that contain the generalized test methods,
\ToolTeralizer{} copies all shared code
(i.e., setup/teardown methods, non-test methods, class fields, ...)
from the \VariantOriginal{} test classes
into each generalized test class
(as described in Section~\ref{sec:test-generalization})
which results in additional LOC increases due to duplicated code.

The effects of (i) are more pronounced
for \VariantImproved{} generalization variants
than for \VariantNaive{} ones
because of the more precise (but also more complex)
identification of input ranges used by \VariantImproved{} variants.
For example, the generalized test suite created by 
\VariantImprovedC{} for \DatasetCommonsDev{}
has 36 additional LOC compared to \VariantNaiveC{}
(9018 vs.\ 8982~LOC, see Table~\ref{tab:lines-per-project})
even though the same three tests are added as a result of generalization
(as shown in Table~\ref{tab:tests-per-project}).
This difference in LOC of individual tests generalized
via \VariantImproved{} vs.\ \VariantNaive{} variants
is generally larger for tested methods
with a larger number of input parameters.
The reason for this is that each input parameter
needs to be separately restricted
to match the constraints that involve that parameter
in the input specification.

Both of the causes mentioned above
could be resolved with additional engineering effort.
For example, generalized test methods created by \ToolTeralizer{}
could be added to the \VariantOriginal{} test classes
instead of creating a new classes for each one, thus resolving cause (ii).
The primary reason we avoid this in the current implementation
is to keep \ToolTeralizer{}'s changes as unintrusive and isolated
from the \VariantOriginal{} test suite (and other generalized tests) as possible
to avoid unintended adverse side-effects on developer written code
as well as unintended interactions between generalized tests.
However, there is no technical reason
that strictly requires generalized test methods
to be added to newly created classes.
Similarly, the code to set input value ranges
that satisfy the extracted input specifications
could be extracted to a library
that abstracts away the corresponding implementation details.
In the test code of the target projects,
only minor changes would remain then
(i.e., modified test annotations, inputs, and expected outputs),
thus resolving cause (i).

\begin{table}[H]
\caption{Number of test lines before and after generalization, with changes, per project.}
\label{tab:lines-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Lines} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 30989 & 11780 & 1127 & 41642 & +10653 & +34.4 \% \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 32503 & 11520 & 1061 & 42962 & +10459 & +32.2 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 33510 & 11623 & 1069 & 44064 & +10554 & +31.5 \% \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 30989 & 19019 & 1302 & 48706 & +17717 & +57.2 \% \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 32503 & 20353 & 1284 & 51572 & +19069 & +58.7 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 33510 & 20288 & 1285 & 52513 & +19003 & +56.7 \% \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 16563 & 3733 & 359 & 19937 & +3374 & +20.4 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 18124 & 3942 & 379 & 21687 & +3563 & +19.7 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 17886 & 3723 & 361 & 21248 & +3362 & +18.8 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 16563 & 5261 & 413 & 21411 & +4848 & +29.3 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 18124 & 5423 & 421 & 23126 & +5002 & +27.6 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 17886 & 5801 & 452 & 23235 & +5349 & +29.9 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 8561 & 421 & 0 & 8982 & +421 & +4.9 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 8561 & 457 & 0 & 9018 & +457 & +5.3 \% \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Execution Time of the Test Suite}
\label{sec:test-suite-execution-time}

As shown in Table~\ref{tab:runtime-per-project},
generalization with \VariantNaiveC{} and \VariantImprovedC{}
increases overall test suite runtimes
for all \DatasetsEqBenchEs{} and \DatasetsCommonsEs{} projects.
More specifically, test suite runtimes
show increases of 574.5--1210.0~\% for the \DatasetsEqBenchEs{} projects,
444.1--2651.5~\% for the \DatasetsCommonsEs{} projects,
and 9.4--57.1~\% for the \DatasetCommonsDev{} project.
Runtime increases are primarily affected by the following factors:

\begin{enumerate}
  \item the number of tests added by the generalization, even if they are compensated for by removed tests,
  \item the number of \tries{} used during property-based testing,
  \item the used generalization approach, i.e., \VariantNaive{} vs. \VariantImproved{} generalization,
  \item the complexity of input partition constraints.
\end{enumerate}

In the following paragraphs,
we discuss how each of the above factors
influences overall test suite execution times
and how these factors interact with each other.

\paragraph{Added Tests}

Execution of conventional JUnit tests has a lower runtime cost
than execution of corresponding property-based jqwik tests.
More specifically, individual property-based tests
created with the \VariantBaseline{} generalization variant 
take, on average, 149.56 milliseconds (ms) longer to execute
than the corresponding \VariantOriginal{} tests
(as shown in the left plot of Figure~\ref{fig:test-runtime-differences})
which have a mean execution time of only 3.6~ms. % 0.0035570506476290337
% Exact number = 0.0035570506476290337 via:
% SELECT AVG(runtime)
% FROM junit_test_report jtr
% JOIN generalization g ON g.test_id = jtr.test_id
% WHERE stage = 'COLLECT_JUNIT_REPORTS_ORIGINAL'
% AND g.is_included;
Therefore, overall test suite execution time increases,
on average, by at least 149.56~ms per jqwik test
that is added during the generalization process,
even if no new test inputs are exercised
and the added tests are compensated for by removed JUnit tests.
Since these runtime increases are inherent to the use of jqwik,
they are orthogonal to our specific generalization approach.
That is, any manual or automated transformation of JUnit tests
to jqwik tests incurs the same runtime overhead,
and this overhead can only be reduced
if fewer jqwik tests are created
(e.g., through test suite reduction, as discussed in Section~\ref{sec:test-suite-reduction})
or if the runtime performance of jqwik is improved.

\begin{table}[H]
\caption{Test suite runtime before and after generalization, with changes, per project.}
\label{tab:runtime-per-project}
\begin{tabular}{llrrrrrr}
\toprule
 & & \multicolumn{6}{c}{Runtime (in seconds)} \\
\cmidrule(lr){3-8}
Project & Variant & Before & Added & Removed & After & Delta & Delta \% \\
\midrule
\DatasetEqBenchA{} & \VariantNaiveC{} & 17.44 & 100.94 & 0.74 & 117.65 & +100.20 & +574.5 \% \\
\DatasetEqBenchB{} & \VariantNaiveC{} & 16.70 & 106.19 & 0.66 & 122.23 & +105.53 & +632.0 \% \\
\DatasetEqBenchC{} & \VariantNaiveC{} & 18.21 & 221.07 & 0.76 & 238.52 & +220.31 & +1210.0 \% \\
\midrule
\DatasetEqBenchA{} & \VariantImprovedC{} & 17.44 & 101.62 & 0.68 & 118.38 & +100.94 & +578.7 \% \\
\DatasetEqBenchB{} & \VariantImprovedC{} & 16.70 & 139.64 & 0.56 & 155.77 & +139.08 & +832.9 \% \\
\DatasetEqBenchC{} & \VariantImprovedC{} & 18.21 & 124.68 & 0.69 & 142.20 & +123.99 & +681.0 \% \\
\midrule
\DatasetCommonsA{} & \VariantNaiveC{} & 4.31 & 114.42 & 0.09 & 118.64 & +114.33 & +2651.5 \% \\
\DatasetCommonsB{} & \VariantNaiveC{} & 7.40 & 148.54 & 0.14 & 155.80 & +148.40 & +2005.2 \% \\
\DatasetCommonsC{} & \VariantNaiveC{} & 6.30 & 122.11 & 0.07 & 128.34 & +122.04 & +1936.2 \% \\
\midrule
\DatasetCommonsA{} & \VariantImprovedC{} & 4.31 & 29.49 & 0.14 & 33.66 & +29.35 & +680.5 \% \\
\DatasetCommonsB{} & \VariantImprovedC{} & 7.40 & 71.50 & 0.21 & 78.70 & +71.30 & +963.3 \% \\
\DatasetCommonsC{} & \VariantImprovedC{} & 6.30 & 28.07 & 0.08 & 34.29 & +27.99 & +444.1 \% \\
\midrule
\DatasetCommonsDev{} & \VariantNaiveC{} & 7.95 & 4.54 & 0.00 & 12.48 & +4.54 & +57.1 \% \\
\midrule
\DatasetCommonsDev{} & \VariantImprovedC{} & 7.95 & 0.74 & 0.00 & 8.69 & +0.74 & +9.4 \% \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_test_runtime_differences}
  \caption{Runtime Comparison: Test vs. Generalization.}
  %\Description{@TODO}
  \label{fig:test-runtime-differences}
\end{figure}

@TODO: Split Figure~\ref{fig:test-runtime-differences} into two (sub-)figures to make each one individually referenceable.

\subsection{RQ3: Runtime Requirements}
\label{sec:runtime-eval}

only one run for each configuration (due to high runtime requirements)
investigation for a subset of configurations confirmed that the trends hold

total runtime (+ runtime per project?)
@TOOD: Probably can remove Table~\ref{tab:teralizer-runtimes} and just mention the total runtime somewhere. We have a more detailed overview in Figure~\ref{fig:teralizer-runtimes} anyway.

\begin{table}[H]
  \caption{Total runtimes of Teralizer for all evaluated projects.}
  \label{tab:teralizer-runtimes}
  \begin{tabular}{lr}
    \toprule
    Project & Runtime \\ 
    \midrule
    \DatasetEqBenchA{} & 24h 46min 27s \\ 
    \DatasetEqBenchB{} & 28h 16min 32s \\ 
    \DatasetEqBenchC{} & 30h 53min 22s \\ 
    \midrule
    \DatasetCommonsA{} & 8h 13min 55s \\ 
    \DatasetCommonsB{} & 9h 46min 48s \\ 
    \DatasetCommonsC{} & 9h 04min 32s \\ 
    \midrule
    \DatasetCommonsDev{} & 3h 23min 22s \\ 
    \bottomrule 
  \end{tabular} 
\end{table}

runtime per project + processing stage + variant

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetsCommons{} results in Figure~\ref{fig:teralizer-runtimes}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{fig_teralizer_runtimes}
  \caption{Teralizer runtimes per project, processing stage, and generalization variant.}
  %\Description{@TODO}
  \label{fig:teralizer-runtimes}
\end{figure}

efficiency relative to higher EvoSuite search budgets

@TODO: Show \DatasetsEqBenchEs{} results before \DatasetsCommons{} results in Figure~\ref{fig:teralizer-efficiency}.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{fig_teralizer_efficiency.pdf}
  \caption{Pareto fronts for EvoSuite and Teralizer variants across projects.}
  \label{fig:teralizer-efficiency}
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project commons-utils.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & \ToolEvoSuite{} & \ToolTeralizer{} & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 56.8 & 4648.7 \\
          2 & 10s & - & 57.3 & 5597.3 \\
          3 & 1s & IMPROVED$_{10}$ & 57.9 & 7294.5 \\
          4 & 60s & - & 58.1 & 10239.8 \\
          5 & 10s & NAIVE$_{10}$ & 58.1 & 10445.1 \\
          6 & 10s & IMPROVED$_{50}$ & 58.4 & 10603.4 \\
          7 & 10s & IMPROVED$_{10}$ & 58.4 & 11081.5 \\
          8 & 10s & IMPROVED$_{200}$ & 58.5 & 13270.4 \\
          9 & 60s & IMPROVED$_{10}$ & 59.3 & 13938.7 \\
          10 & 60s & IMPROVED$_{50}$ & 59.4 & 14727.5 \\
          11 & 60s & IMPROVED$_{200}$ & 59.5 & 15735.7 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
  \hfill
  \begin{minipage}[t]{0.48\textwidth}
      \centering
      \captionof{table}{Pareto points for project \DatasetEqBench{}.}
      \setlength{\tabcolsep}{4pt}
      \begin{tabular}{rrlrr}
          \toprule
          Pt. & \ToolEvoSuite{} & \ToolTeralizer{} & Det. \% & Runtime (s) \\
          \midrule
          1 & 1s & - & 48.1 & 26479.3 \\
          2 & 10s & - & 50.6 & 29861.1 \\
          3 & 1s & NAIVE$_{10}$ & 50.7 & 36727.9 \\
          4 & 1s & IMPROVED$_{50}$ & 51.4 & 37457.4 \\
          5 & 1s & NAIVE$_{50}$ & 51.7 & 37531.9 \\
          6 & 10s & IMPROVED$_{10}$ & 51.9 & 41525.3 \\
          7 & 10s & IMPROVED$_{50}$ & 53.6 & 42256.2 \\
          8 & 10s & NAIVE$_{50}$ & 53.8 & 45398.0 \\
          9 & 10s & IMPROVED$_{200}$ & 53.8 & 48268.6 \\
          10 & 10s & NAIVE$_{200}$ & 54.1 & 62938.2 \\
          11 & 60s & IMPROVED$_{50}$ & 54.5 & 68092.9 \\
          12 & 60s & NAIVE$_{50}$ & 54.7 & 68782.2 \\
          13 & 60s & IMPROVED$_{200}$ & 54.8 & 75081.4 \\
          14 & 60s & NAIVE$_{200}$ & 55.0 & 93017.1 \\
          \bottomrule
      \end{tabular}
  \end{minipage}
\end{figure}

large potential for runtime improvements, see examples in the discussion

\subsection{RQ4 (Part 1 of 2): Causes of Unsuccessful Generalizations in the Evaluation Dataset}
\label{sec:filtering-eval}

In this section, we describe causes of unsuccessful generalizations in the main
evaluation dataset (commons-utils + evosuite-variants, eqbench + evosuite variants).
To get more generalizable results that enable better informed decisions about future
research directions, we also evaluated generalization success vs. failure in <number>
additional open source projects beyond the main evaluation dataset. Results of the
extended evaluation are discussed in Section~\ref{sec:filtering-eval-extended}.

overall test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Included and excluded counts by variant and level.}
  \label{tab:exclusions-summary}
  \begin{tabular}{llrrr}
    \toprule
    Variant & Type & Total & \multicolumn{1}{c}{Included} & \multicolumn{1}{c}{Excluded} \\
    \midrule
    SHARED & Test & 23246 & 19306\; (83.1\%) & 3940\; (16.9\%) \\
    SHARED & Assertion & 28923 & 13836\; (47.8\%) & 15087\; (52.2\%) \\
    BASELINE & Generalization & 13836 & 13814\; (99.8\%) & 22\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & 13836 & 10743\; (77.6\%) & 3093\; (22.4\%) \\
    NAIVE$_{50}$ & Generalization & 13836 & 9964\; (72.0\%) & 3872\; (28.0\%) \\
    NAIVE$_{200}$ & Generalization & 13836 & 9881\; (71.4\%) & 3955\; (28.6\%) \\
    IMPROVED$_{10}$ & Generalization & 13836 & 11788\; (85.2\%) & 2048\; (14.8\%) \\
    IMPROVED$_{50}$ & Generalization & 13836 & 11660\; (84.3\%) & 2176\; (15.7\%) \\
    IMPROVED$_{200}$ & Generalization & 13836 & 11597\; (83.8\%) & 2239\; (16.2\%) \\
    \bottomrule
  \end{tabular}
\end{table}

filtering-based test / assertion / generalization exclusions

\begin{table}[H]
  \caption{Filtering results for tests, assertions, and generalizations by filter and (generalization) variant.}
  \label{tab:exclusions-filtering}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 23246 & 21719\; (93.4\%) & 1527.0\; (\phantom{0}6.6\%) \\
    SHARED & Test & TestType & 23246 & 23066\; (99.2\%) & 180.0\; (\phantom{0}0.8\%) \\
    SHARED & Test & NoAssertions & 21532 & 19306\; (89.7\%) & 2226.0\; (10.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 28923 & 27326\; (94.5\%) & 1597.0\; (\phantom{0}5.5\%) \\
    SHARED & Assertion & MissingValue & 28923 & 21766\; (75.3\%) & 7157.0\; (24.7\%) \\
    SHARED & Assertion & ParameterType & 28923 & 17835\; (61.7\%) & 11088.0\; (38.3\%) \\
    SHARED & Assertion & UnsupportedAssertion & 28923 & 28180\; (97.4\%) & 743.0\; (\phantom{0}2.6\%) \\
    SHARED & Assertion & VoidReturnType & 28923 & 21763\; (75.2\%) & 7160.0\; (24.8\%) \\
    \midrule
    BASELINE & Generalization & NonPassingTest & 13836 & 13814\; (99.8\%) & 22.0\; (\phantom{0}0.2\%) \\
    NAIVE$_{10}$ & Generalization & NonPassingTest & 13804 & 10743\; (77.8\%) & 3061.0\; (22.2\%) \\
    NAIVE$_{50}$ & Generalization & NonPassingTest & 13804 & 9964\; (72.2\%) & 3840.0\; (27.8\%) \\
    NAIVE$_{200}$ & Generalization & NonPassingTest & 13804 & 9881\; (71.6\%) & 3923.0\; (28.4\%) \\
    IMPROVED$_{10}$ & Generalization & NonPassingTest & 13804 & 11788\; (85.4\%) & 2016.0\; (14.6\%) \\
    IMPROVED$_{50}$ & Generalization & NonPassingTest & 13804 & 11660\; (84.5\%) & 2144.0\; (15.5\%) \\
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 13804 & 11597\; (84.0\%) & 2207.0\; (16.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

failing tests

\begin{table}[H]
  \caption{Number of test execution failures by exception type and (generalization) variant.}
  \label{tab:exclusions-test-fails}
  \begin{tabular}{lrrrrrrrr}
    \toprule
    Variant & ORIGINAL & BASELINE & \multicolumn{3}{c}{NAIVE} & \multicolumn{3}{c}{IMPROVED} \\
    \cmidrule(lr){4-6}
    \cmidrule(lr){7-9}
    Tries & - & - & 10 & 50 & 200 & 10 & 50 & 200 \\
    \midrule
    ArithmeticException & 0 & 0 & 99 & 99 & 99 & 57 & 58 & 58 \\
    AssertionFailedError & 132 & 22 & 729 & 803 & 819 & 752 & 845 & 866 \\
    NumberFormatException & 0 & 0 & 0 & 0 & 0 & 18 & 18 & 18 \\
    TooManyFilterMissesException & 0 & 0 & 2233 & 2938 & 3005 & 1189 & 1223 & 1265 \\
    \bottomrule
  \end{tabular}
\end{table}

@TODO: Remove Table~\ref{tab:exclusions-spf}. Describe SPF execution failures in text only.

\begin{table}[H]
  \centering
  \caption{Number of SPF execution failures by error type.}
  \label{tab:exclusions-spf}
  \begin{tabular}{lrr}
    \toprule
    Error Type & Total & Percent \\
    \midrule
    SPF exception & 1540 & 51.42 \\
    PC size limit exceeded & 790 & 26.38 \\
    Depth limit exceeded & 524 & 17.50 \\
    Teralizer exception & 97 & 3.24 \\
    Execution timeout & 28 & 0.93 \\
    OutOfMemoryError & 16 & 0.53 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{RQ4 (Part 2 of 2): Causes of Unsuccessful Generalizations in Other Open Source Projects}
\label{sec:filtering-eval-extended}

This evaluation only uses the IMPROVED$_{200}$
generalization variant, but results also apply to all other variants.

\begin{table}[H]
  \caption{Number of processing failures and remaining projects per processing stage.}
  \label{tab:processing-failures-per-stage}
  \begin{tabular}{l r r}
    \toprule
    Processing Stage & Failures & Remaining Projects \\
    \midrule
    Total projects & - & 1160\; (\phantom{.}100 \%) \\
    \cmidrule(lr){1-3}
    SETUP\_PROJECT & 355 & 805\; (69.4 \%) \\
    BUILD\_PROJECT\_ORIGINAL & 189 & 616\; (53.1 \%) \\
    BUILD\_SPOON\_MODEL & 8 & 608\; (52.4 \%) \\
    EXECUTE\_TESTS\_ORIGINAL & 61 & 547\; (47.2 \%) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & 31 & 516\; (44.5 \%) \\
    BUILD\_PROJECT\_INSTRUMENTED & 1 & 515\; (44.4 \%) \\
    EXECUTE\_TESTS\_INITIAL & 130 & 385\; (33.2 \%) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & 41 & 344\; (29.7 \%) \\
    COLLECT\_PIT\_DATA\_INITIAL & 64 & 280\; (24.1 \%) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & 270 & 10\; (\phantom{0}0.9 \%) \\
    \cmidrule(lr){1-3}
    Successfully processed & - & 10\; (\phantom{0}0.9 \%) \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{table}[H]
  \caption{Causes of processing failures per processing stage.}
  \label{tab:processing-failure-causes}
  \begin{tabularx}{\textwidth}{l X}
    \toprule
    Processing Stage & Causes of Processing Failures \\
    \midrule
    SETUP\_PROJECT & dependency resolution error (329), sources / tests not found (26) \\
    BUILD\_PROJECT\_ORIGINAL & compilation error (171), compilation outputs not found (18) \\
    BUILD\_SPOON\_MODEL & Spoon execution error (8) \\
    EXECUTE\_TESTS\_ORIGINAL & JUnit execution error (13), timeout exceeded (48) \\
    COLLECT\_JUNIT\_REPORTS\_ORIGINAL & JUnit outputs not found (31) \\
    BUILD\_PROJECT\_INSTRUMENTED & compilation error (1) \\
    EXECUTE\_TESTS\_INITIAL & all tests excluded (129), timeout exceeded (1) \\
    COLLECT\_JACOCO\_DATA\_INITIAL & JaCoCo execution error (1), JaCoCo outputs not found (40) \\
    COLLECT\_PIT\_DATA\_INITIAL & PIT execution error (16), PIT outputs not found (4), all classes excluded (3), failed to map PIT data to a test (1), timeout exceeded (40) \\
    COLLECT\_PIT\_DATA\_GENERALIZED & all generalizations excluded (269), failed to map PIT data to a generalization (1) \\
    \bottomrule
  \end{tabularx}
\end{table}

Broadly speaking, there are three main ways to increase the number of projects
that can be successfully processed: (i)~reducing exclusions caused by filtering
(129 + 3 + 269 = 401 projects), (ii)~adding support for more varied project structures
(26 + 18 + 31 + 40 + 4 = 119 projects), and (iii)~increasing timeouts (48 + 1 + 40 = 89 projects).
Depedency resolution and compilation errors in the original project code (329 + 171 = 500 projects)
as well as external tool errors (8 + 13 + 1 + 16 = 38 projects) are less actionable.
Furthermore, Teralizer errors only occur in a small number of cases (1 + 1 + 1 = 3 projects),
so also offer comparatively little opportunity for improvements.

\begin{table}[H]
  \caption{Filtering results of the extended dataset for tests, assertions, and generalizations.}
  \label{tab:exclusions-filtering-extended}
  \begin{tabular}{lllrrr}
    \toprule
    Variant & Type & Filter Name & Total & \multicolumn{1}{c}{Accept} & \multicolumn{1}{c}{Reject} \\
    \midrule
    SHARED & Test & NonPassingTest & 74332 & 65579\; (88.2\%) & 8753.0\; (11.8\%) \\
    SHARED & Test & TestType & 74332 & 65052\; (87.5\%) & 9280.0\; (12.5\%) \\
    SHARED & Test & NoAssertions & 56853 & 33390\; (58.7\%) & 23463.0\; (41.3\%) \\
    \midrule
    SHARED & Assertion & ExcludedTest & 122166 & 101519\; (83.1\%) & 20647.0\; (16.9\%) \\
    SHARED & Assertion & MissingValue & 122166 & 51430\; (42.1\%) & 70736.0\; (57.9\%) \\
    SHARED & Assertion & ParameterType & 122166 & 5393\; (\phantom{0}4.4\%) & 116773.0\; (95.6\%) \\
    SHARED & Assertion & ReturnType & 122166 & 11650\; (\phantom{0}9.5\%) & 110516.0\; (90.5\%) \\
    SHARED & Assertion & UnsupportedAssertion & 122166 & 92996\; (76.1\%) & 29170.0\; (23.9\%) \\
    \midrule
    IMPROVED$_{200}$ & Generalization & NonPassingTest & 229 & 206\; (90.0\%) & 23.0\; (10.0\%) \\
    \bottomrule
  \end{tabular}
\end{table}

The most common filtering reasons are unsupported parameter and return types.

@TODO: Better explain these filtering results by prooviding information about:
(i) which types are identified in the tested method signatures, and
(ii) which assertions are used in the tests.

Successful generalizations did not improve mutation scores in any of the successfully
processed projects. However, some generalizations indicate that there weak
preconditions are used in the source / test code of the original implementation.

@TODO: Check which generalizations fail the "NonPassingTest" filter due to (i) bugs in
our generalization implemenation vs. (ii) weak preconditions.

% Performance improvements of Teralizer itself can NOT be used as a substitute for
% (i) because the timeouts occur for tasks that are performed by external tools.

% ------------------------------------------------------------------------------
% DISCUSSION
% ------------------------------------------------------------------------------

\section{Discussion}
\label{sec:discussion}

\subsection{Benefits of the Approach}
\label{sec:benefits-of-the-approach}

\subsection{Potential for Future Improvements}
\label{sec:potential-for-improvements}

\subsubsection{Improving the Mutation Score of Generalized Tests}
\label{sec:improving-the-mutation-score}

\subsubsection{Improving the Size of Generalized Tests}
\label{sec:improving-the-test-size}

\subsubsection{Improving the Runtime of Test Generalization}
\label{sec:improving-the-runtime}

primary runtime costs are from external tools (i.e., jqwik + PIT)

mutation testing runtimes could likely be improved
through the use of the pitest accelerator plugin (\url{https://docs.arcmutate.com/docs/accelerator.html}),
thus, further improving results of Teralizer relative to EvoSuite
(website claims "analysis time for Commons Lang is reduced from over 55 minutes to under 18");
however, the plugin is not available without a license

also, parallelization would be possible rather easily for many Teralizer tasks
(and also for PIT, which only runs single-threaded by default)
(supposedly, jqwik will get parallelization support in major version 2, source (2024-05-14): \url{https://github.com/jqwik-team/jqwik/issues/45#issuecomment-2109949764})

for practical application scenarios, could add configuration to target only specific tests, rather than the full test suite

\subsubsection{Reducing the Number of Unsuccessful Generalizations}
\label{sec:reducing-unsuccessful-generalizations}

\subsubsection{Using Test Generalization for Test Suite Reduction}
\label{sec:test-suite-reduction}

\subsubsection{Dealing with Overfitting}
\label{sec:overfitting}

We do not generate assertions, so overfitting is less pronounced than if we did.
However, we still rely on the current implementation as the source of truth,
encoding the exact behavior of it in the generalized tests (at least as far as
the behavior is tested by assertions in the original, non-generalized test suite).

\subsection{Threats to Validity}
\label{sec:threats-to-validity}

\subsubsection{Construct Validity}
\label{sec:construct-validity}

\subsubsection{Internal Validity}
\label{sec:internal-validity}

\subsubsection{External Validity}
\label{sec:external-validity}

% ------------------------------------------------------------------------------
% RELATED WORK
% ------------------------------------------------------------------------------

\section{Related Work}
\label{sec:related-work}

% ------------------------------------------------------------------------------
% CONCLUSIONS
% ------------------------------------------------------------------------------

\section{Conclusions}
\label{sec:conclusions}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
This research was funded in whole or in part by the Austrian Science Fund (FWF) 10.55776/P36698. For open access purposes, the author has applied a CC BY public copyright license to any author accepted manuscript version arising from this submission.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
